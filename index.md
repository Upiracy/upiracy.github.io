

# 	图形学笔记

[TOC]

<div STYLE="page-break-after:always;"></div>

## 知识框架

### 图形学综述

#### 图形学定位

##### 图形学要研究什么

> 在计算机图形学领域，一个东西只要看起来是对的，那么他就是对的。

**计算机图形学(Computer Graphics, CG)**的核心目标是三个基本任务：**表示、交互、绘制**。表示是将主、客观世界放入计算机，通过数字对二维、三维对象进行建模与储存。绘制是将计算机中的对象通过直观的人眼易读的图形图像方式表现出来。交互是通过直观的图形图像手段，改善真实用户通过计算机录入、修改、获取数据的体验。

计算机图形学的主要研究对象是点、线、面、体、场的数学构造方法和图形显示，以及其随时间变化的情况，它需要研究以下几方面的内容： 

+ 描述复杂物体图形的方法与数学算法，包括曲面、曲线的造型技术，实体造型技术，纹理、云彩、波浪等自然景物的造型和模拟，三维场景的显示如光栅图形成生成算法和线框图、真实感图形的理论和算法。
+ 物体图形描述数据的输入。
+ 几何和图形数据的储存、压缩和解压。
+ 物体图形数据的运算，如基于图形和图像的混合绘制、自然景物仿真、图形用户接口、虚拟现实、动画技术和可视化技术等。
+ 物体图形数据的输出显示，包括图形硬件和图形交互技术。
+ 实时动画和多媒体技术，研究实现高速高精度动画的各种软、硬件方法，开发工具、动画语言以及多媒体接口。
+ 制定与图形应用软件有关的技术标准。

##### 图形学在多媒体学科中的定位

<img src="Textures\多媒体学科.png" alt="多媒体学科" style="zoom:50%;" />

多媒体学科一般来说包括四种：计算机图形学、数字图像处理、计算机视觉、计算机几何学。

其中，图形学研究的是由几何数据产生图像的过程，其中几何数据是已知量，图像是未知量。所以图形学讨论如何处理规则化或半规则化的数学输入，将这些输入以像素的形式呈现在屏幕上以方便人类处理数据。

虽然图形学在严格的定义上只负责由几何数据生成图像的过程，但由于几何数据输入类型多样，而人眼对图像的要求又越来越高，所以研究计算机图形学仍不可避免地需要接触到一定的计算机几何学和数字图像处理的知识。

### 图形学知识框架

##### 图形学思维导图

![图形学知识框架](Textures\图形学知识框架.png)

##### 图形学知识概述

图形学需要解决的根本问题，可以归纳为在正确的位置显示正确的像素。

为了显示像素，我们需要显示器。显示器显示像素，需要颜色数据作为输入，这就涉及到了使用双缓冲区来存放颜色数据，以及使用RGB表示色彩的方式。

颜色数据是GPU输出给显示器的，所以我们需要了解GPU是如何告诉显示器像素的颜色的。从逻辑的角度上讲，GPU是通过**渲染流水线**来计算像素的颜色的；从物理硬件的角度上讲，为了实现渲染流水线，GPU需要大量的缓冲区和高并行的计算单元，这导致**GPU的结构**与CPU有重大的差异。

GPU不能凭空产生数据，GPU的数据来自CPU。CPU中储存了实现渲染所需的三要素：**光源、摄像机和几何形**。将三要素组合在一起，就构成了**场景**。

在图形学领域中，主要存在四种表示几何体的方案：参数法、体积法、体素法和多边形法。

**参数法是使用曲线和曲面对表面进行拟合的方法**。曲线的通用表现方法有贝塞尔曲线、埃尔米特曲线和B样条曲线(Nurbs)等。将曲线连接起来，就会生成曲面，常用的曲面有贝塞尔曲面、B样条曲面等。这种表示法一般用于工业设计领域，因为参数法能提供更适合工业机器人和机器臂的，可无限细分空间数据。另外，参数曲线和曲面也经常被用在体积法、体素法和多边形法几何体的建模过程中，在动画的插值算法中也有重要的作用。

**体积法是使用场(包括标量场和向量场)的方式记录空间中任意点的信息的方法**。常见的使用体积数据表示几何体的方法包括有向距离场(SDF)和MetaBall等。体积法也可以产生可无限细分的空间数据，但形式更多样化，可以使用简单的函数来便捷的描述无限重复、分形重复和随机分布的几何体。在物理运算和光线追踪运算中，工程师们希望尽可能地将几何体用体积法进行表示，着视因为使用体积法在所有几何体表示方法法中具有最好的相交测试和布尔运算效率。

**体素法是一种使用体素块描述物体体积信息的方案法**，其中体素(Voxel)的含义与像素(Pixel)类似，是将空间中物体的分布情况记录在紧密相邻的体素块中的方法。常见的体素数据储存在名为3D纹理的文件中，它类似3D空间中的位图，在分辨率足够高的情况下可以生成接近平滑的几何体表面。体素技术通常运用在工艺美术领域，如3D雕刻和3D打印领域，在这些领域中艺术家们可以对几何体进行随心所欲的自由修改，而不会受到数学、物理或计算机科学的限制。除此之外，在流体仿真等领域，也会使用到体素数据。

**多边形法是一种使用点、线、面的格式描述物体表面的方法**，多边形法也因此被称为网格法。网格法是3D图形学领域最常用的方法，我们熟知的\*.max、\*.obj、\*.fbx等文件格式就是多边形法的典型例子。网格法相比参数法和体积法给艺术家提供了更多的创作自由，而相比体素法压缩了大量的空间，所以是计算机视频、动画、游戏领域最常用的格式。

CPU想将场景数据传输到GPU，就需要调用图形API，也就是OpenGL或DirectX3D，它们可以让CPU告诉GPU如何进行渲染，这个过程也被称为Draw Call。在CPU中发动Draw Call的流程，被称为**渲染管线**。

图形学不是仅凭CPU或仅凭GPU就能玩转的学科，实现CPU和GPU的配合是很重要的。在介绍完CPU端和GPU端之后，我们还会针对两者的配合，介绍一些图形学中常见的优化方法或特效原理，这将需要读者对**渲染管线**和**渲染流水线**都有足够的理解。

本系列笔记不打算按照学习难度由易到难的顺序进行布局，而是严格遵守逻辑关系，先概括整个模块的大致内容，再分别讲述几个子模块。整个笔记将分为六个大的模块：渲染流程、数学知识、几何数据、场景渲染、动态渲染、渲染优化。

+ 渲染流程：完整的浏览整个渲染过程，从CPU发送Draw Call到图像显示在显示器上，对什么是渲染进行详细的阐述，并借助硬件的视野进一步深入的了解图形学最核心的渲染流程。
+ 数学知识：对在图形学当中作用极大的计算几何学和线性代数进行了针对性的介绍，使有相关数学基础的读者能对这些知识在图形学中的应用有更深的了解。随后又补充了关于色彩学和噪声生成的基础知识，有利于对先进渲染方案的理解。
+ 几何数据：介绍了三种主流的几何体建模表示方式，以及它们的渲染方法和相互转换的方法，并对与之相关的渲染技术进行了讨论。
+ 场景渲染：以真实感和风格化渲染两种渲染技术的发展方向为出发点，先讲解了进行渲染所需的纹理，然后针对这两种渲染技术分别介绍了其基础思想、经典算法和前沿算法，最后讨论了增强渲染效果的后处理方法。
+ 动效渲染：从动画、粒子和物理三个方面着手，了解图形学领域在仿真、模拟运动和动态视觉艺术方面的技术应用，并学习其中的一些经典实现方案。
+ 渲染优化：在对渲染有一定认识的基础上，从性能的角度思考整个渲染流程，对其中的时间和空间消耗进行优化，以期增加在实时渲染中的美术表现上限。

##### 推荐阅读顺序

对于第一次学习图形学的同学来说，建议按照下面的推荐阅读顺序进行阅读：



<div STYLE="page-break-after:always;"></div>

## 渲染流程

### GPU硬件结构

#### 显卡

##### GPU和显卡有什么关系？

GPU是显卡上的核心处理芯片，显卡上除了GPU，还包括显存、电路板和BIOS固件等。由于GPU在显卡上十分重要，所以时常用GPU代指显卡。GPU硬件结构如下图所示：

![GPU硬件](Textures\GPU硬件.png)

显卡也叫显示适配器，分为独立显卡和集成显卡。独立显卡由GPU、显存和接口电路组成；集成显卡和CPU共用风扇和缓存，没有独立显存，而是使用主板上的内存。

##### 什么是NVIDIA/AMD？

NVIDIA和AMD都是著名显卡品牌。

NVIDIA公司译为**英伟达**，其生产的显卡又被称为N卡。AMD译为**超微半导体**，其生产的显卡又被称为A卡。

N卡奉行大核心战略，GPU内部采用大量1D单元，在执行效率上理论可以达到100%，实际效率也可以维持在90%以上，因为架构执行效率高，灵活性强，所以在实际应用中易发挥应有性能。但是大核心的设计复杂，内部集成的SP数量也不会太多，成本和功耗也会比较高，控制单元在晶体管的消耗上占了更大比例，在相同晶体管数量的情况下，N卡能做的运算单元相对较少。N卡在软件上具有明显优势，包括微软在内的软件商都为N卡开发优化，使得大量工具软件和游戏在N卡环境下有更好的表现。

A卡奉行小核心战略，采用VLIW5或VLIW4的设计，分别采用4D+1D的设计和4D设计，可以在较小的晶体管代价和较小的核心面积下装入更多的SPU，以SP的数量取胜。其理论计算能力远超N卡，但实际执行效率并不高，一旦进入GPU的图形信息是1D或3D形式这一的非标准数据形式，A卡的执行效率最低可降至25%至20%。

显卡是个人计算机最基本组成部分之一，用途是将计算机系统所需要的显示信息进行转换以驱动显示器，并向显示器提供逐行或隔行扫描信号，控制显示器的正确显示，其高效的并行计算能力现阶段也用于深度学习等运算。

##### 什么是显卡驱动？

驱动程序是硬件厂商根据操作系统编写的配置文件，可以说没有驱动程序，计算机中的硬件就无法工作。操作系统不同，硬件的驱动程序也不同，各个硬件厂商为了保证硬件的兼容性及增强硬件的功能会不断地升级驱动程序。

最早PC显卡是不需要安装驱动的，显卡芯片会不停地读取计算机内存中固定区域的数据，并将它们转化成显示信号输出。这一过程可以完全集成到操作系统中，所以不需要专门的显卡驱动。而同时代的游戏机则率先有了显卡的形态，通过将常见的图形运算集成到硬件中，CPU可以节省大量内存用于游戏逻辑运算，这一图形化集成硬件在当时也被称为**加速卡**。由于使用了额外的硬件，才不得不使用硬件驱动程序来实现CPU和GPU的交互。

硬件驱动的作用是让不同厂商的硬件能在同一套程序的指引下产生相近的可预期的结果。显卡驱动的地位类似于C语言编译器，它的作用是将OpenGL或DirectX风格的函数调用翻译成GPU能读取的机器指令，即二进制文件。**显卡驱动**同时也负责把纹理等数据转换成GPU支持的格式。在这个过程中，显卡驱动会将CPU的调用翻译成适合自家厂商所生产的硬件的格式，并对指令进行恰当的优化。

#### 流处理器

##### 什么是流处理器SP？

流处理器又称**流处理单元**，简称**SP单元(Streaming Processor)**或SPU，有些显卡生产商也会将其称作core(核心)。流处理器的数量能直接影响显卡性能。

之前的显卡具有两个重要的运算单元——顶点处理单元和像素处理单元。但自从DirectX10开始，微软引入了流处理器这个概念，顶点处理单元和像素处理单元很快被业界抛弃。流处理器是顶点处理单元和像素处理单元的统一，负责了渲染中的顶点和像素渲染。将顶点处理单元和像素处理单元合并的概念又被称作**统一着色器架构(Unified Shader Architecture)**。

业界之所以抛弃之前的顶点+像素结构而使用SPU架构，是因为传统的顶点和像素分离渲染架构存在严重的资源分配不均的问题，两种单元渲染任务量不同，效率低下。而SP架构是统一结构，不再区分顶点和像素渲染，进行不同渲染任务时都能保证效率。

##### 什么是流多处理器SM？

SM(Streaming Multiprocessor，流多重处理器)由多个SP加上**共享内存**、**特殊函数单元**、**寄存器**、**多边形引擎**、**指令缓存**和**L1缓存**等的组合。

SM中的任务主要由SP承担，SM中SP的数量一般为32个，有时也有16或64个SP组成的SM。进行辅助计算的还有SFU(Special Function Units，特殊数学运算单元)，它们用于进行三角函数和指对数等运算。

SM由**Warp Scheduler(束管理器)**驱动，束管理器会将指令移交给**Dispatch Unit(指令分派单元)**，由于SM中每束处理的事务具有相似性，这个单元会从指令缓存读取新的指令，并一次性向整束的所有流处理器发送同一个指令。这种通过一条指令驱动若干线程的特性被称为**SIMT(Single Instruction Multiple Thread，单指令多线程)**，在这个框架下，指令分派单元可以读取一条指令，然后向多个SP分派不同的参数，以让它们在不同的寄存器地址进行读写。

SM中一般配有一个**多边形引擎(Polymorph Engine)**。这个引擎的作用是实现属性装配、顶点拉取、曲面细分、裁剪和光栅化等渲染流水线中的固定步骤。

每个SM中具有一个足够大的寄存器，一般能达到128KB。所有SP共用这个寄存器中的空间，所以如果单个线程需要的寄存器空间过大，可能使每束的最多线程数减少，影响并行性。

**L1缓存**是开始时用于储存顶点数据的缓存，在顶点处理阶段结束后，SM会将处理结果送到SM外的**L2缓存**中。多边形将在SM外进行光栅化，然后将生成的片元重新分发到SM中，这时L1缓存中储存的就是片元数据了。

一个显卡上可能有10~20个SM，一般显卡厂商将若干SM的组合称为一个**GPC(Graphic Processor Cluster，图形处理簇)**，每个簇可以处理一批(batch)顶点或片元。在有GPC结构的显卡上，L2缓存一般位于GPC中，而对于没有GPC结构的显卡，L2一般位于显存旁或显存中。

##### 什么是流处理器的矢量架构？

D是维度Dimension的意思，在图形学中的nD指**n维浮点向量运算**，nD单元指由**n个流处理单元整合成的n维浮点向量运算单元**。

像素坐标XYZW、色彩参数RGBA以及纹理坐标参数STPQ正好都是4维运算，这导致顶点处理单元和像素处理单元都是4D单元，在引入流处理器后，主流的流处理器也是4D单元。

而1D即一维向量，也就是标量。由于流处理器合并了顶点和像素处理单元，图形渲染中标量运算成分开始增多，GPU不再像早年那样只需要处理单纯的4D向量运算了。在这样的背景下，英伟达完全抛弃4D结构，设计了G80这样的1D标量处理器，将矢量运算分解为4次或更多次标量运算，这使N卡的灵活性大幅提升，在任意维度的运算环境下都可以得到满意的性能。

AMD没有放弃4D架构，而是进行了改良，增加了一个标量运算单元，这就是4D+1D矢量标量混合架构，也就是**VLIW5(Very Long Instruction Word，超长指令口令)**架构，它把需要计算的指令组合成适合4D+1D架构的长指令，比如将一个2D运算和一个3D运算合并为一个4D+1D运算，这样理论上每个统一处理器每个周期都可以进行一次4D运算加一次1D运算，是N卡1D单元运算效率的4~5倍，这种将指令组合的算法被称为**co-issue算法**。这五个ALU只需要一个发射端口，电路设计更加简单，功效与发热也更容易控制，但缺点就是依赖指令组合，一旦非最优指令组合，这些运算单元中部分维度就只能空转，运算效率将显著降低。

了解了流处理器的矢量架构后，我们可以通过在函数中一次输入四组数据，而不是调用四次函数，巧妙的优化我们的着色器代码。如在多光源光照计算中，我们使用四维向量LightPosX、LightPosY和LightPosZ来储存四个光源的坐标，代替使用四个三维向量：

~~~HLSL
float3 Shade4PointLights(float4 lightPosX, float4 lightPosY, float4 lightPosZ, float3 lightColor0, float3 lightColor1, float3 lightColor2, float3 lightColor3, float4 lightAttenSq, float3 pos, float3 normal)
{
	//使用了三次运算而不是四次运算，就得到了顶点到四个光源的向量
	float4 toLightX = lightPosX - pos.x;
	float4 toLightY = lightPosY - pos.y;
	float4 toLightZ = lightPosZ - pos.z;
	
	//使用了六次运算而不是八次运算，就得到了顶点到四个光源的距离
	float4 lengthSq = 0;
	lengthSq += toLightX * toLightX;
	lengthSq += toLightY * toLightY;
	lengthSq += toLightZ * toLightZ;
	
	lengthSq = max(lengthSq, 0.000001);
	
	float4 ndotl = 0;
	ndotl += toLightX * normal.x;
	ndotl += toLightY * normal.y;
	ndotl += toLightZ * normal.z;
	
	float corr = rsqrt(lengthSq);//距离的倒数
	ndotl = max(float4(0,0,0,0), ndotl * corr);
	
	float4 atten = 1.0/(1.0+lengthSq*lightAttenSq);//灯光衰减值
	float4 diff = ndotl * atten;
	
	float3 col = 0;
	col += lightColor0 * diff.x;
	col += lightColor1 * diff.y;
	col += lightColor2 * diff.z;
	col += lightColor3 * diff.w;
	return col;
}
~~~

#### 并行与阻塞

##### GPU和CPU的区别？

主流CPU(Central Processing Unit，中央处理器)芯片上有四级缓存，消耗了大量晶体管，在运行时需要大量电力；主流GPU(Graphics Processing Unit，图形处理器)芯片最多有两层缓存，且GPU可以利用晶体管上的空间和能耗做成ALU单元，因此GPU比CPU效率高。

CPU重在实时响应，对单任务速度要求高，需要针对延迟优化，所以晶体管数量和能耗都需要用在分支预测、乱序执行、低延迟缓存等控制部分；GPU主要使用于具有极高可预测性和大量相似运算的批处理，以及高延迟、高吞吐的架构运算，对缓存的要求相对很低，顺序运算效率很高，同时相对的乱序处理效率很低。

CPU除了负责浮点和整型运算，还有很多其它的指令集的负载，如多媒体解码和硬件解码，CPU注重单线程性能，保证指令流不中断，需要消耗更多晶体管和能耗用在控制部分，于是CPU分配在浮点运算的功耗会减少；GPU基本只进行浮点运算，设计结构简单，效率更高，GPU注重吞吐率，单指令能驱动更多的计算，相比较GPU消耗在控制的能耗就少得多，因此可以将资源留给浮点运算使用。GPU的浮点运算能力比CPU高10~12倍。

##### GPU是怎么应对阻塞的？

GPU不像CPU那样实现流水线，因为设计者认定GPU中的一批数据应当具有相对固定的处理过程，比如在一个屏幕后期处理的draw call中，GPU可能处理了1920x1080个片元，但对每个片元运行相同的一套算法。

GPU并不关心跳转，其性能瓶颈主要来自读取显存产生的阻塞而非跳转产生的阻塞。由于纹理数据储存在显存中，在每次遇到采样语句时，为了将显存中的数据调入核心，可能需要使处理器阻塞几百上千个时钟周期。

为了缓解这样的问题，GPU使用**换入换出操作(swapping)**来隐藏延迟。

SM中的寄存器可以备份若干份SP的执行状态。每当有一批片元在等待采样纹理数据时，可以将此时SP组的上下文备份保存在寄存器中，然后导入下一批片元进行处理，这个操作被称为**换出(swap-out)**。换出使得核心不需要空等采样结果，可以继续执行更多的片元。

由于GPU中的数据具有相对固定的处理过程，我们认为每一批片元都会在相同的周期后遇到一次换出操作。假设我们的GPU中只有一个核心(当然不存在这样的GPU)，运行一批具有2000个片元的数据，这2000个片元都会在运行完前5个周期后遇到第一次采样语句，然后被执行换出。假设采样结果从显存中返回到核心需要1000个时间周期，那么在运行完第200个片元并将其换出后，第1个片元所需的采样数据就可以抵达核心，这时核心将第1个片元的上下文从寄存中拷贝回SP中，继续进行这个片元采样之后的运算，直到它再次被换出，这个操作被称为**换入(swap-in)**。如果这些片元在采样语句结束后还有5个时间周期用于算数运算，那么第1个片元执行完时第2个片元也恰好可以被执行换入。以此类推，执行完每200个片元所用的总时间是2000个时间周期，假如我们不使用换入换出操作，200个片元所用的时间将是200200个时间周期。

换入换出的延迟仅仅只有不到10个时间周期，在实际的应用中，不止是采样运算，延迟超过30个周期的很多运算都会使用换入换出操作。

当然，在上面的例子中，假如我们没有2000个片元数据，而仅有20个，那么我们不得不在运行完第20个片元的换出后等待1900个周期才能继续运算。实际的应用中，如果每个线程需要的寄存器数越多，就意味着每束处理的线程数越少，能被用于储存镜像的空间也越少，镜像短缺意味着换入换出策略的效果大幅下降，将严重影响GPU的并发执行。

##### GPU是怎么实现并行的？

执行一个着色器的最小单位是线程(thread)。多个线程被打包在一起称为**线程束**(英伟达称之为warp，AMD称之为wavefront，可以统一翻译为线程束)。多个线程束被打包为一个线程组(block)，每个线程组中的所有线程可以通过共享内存来通信，不同线程组中的线程是无法通信的。

在实际运行中，处理器会将一个线程组分配给一个SM。每个SM获得线程组后，会通过束管理器将其分为若干线程束，每个线程束中线程的个数一般等于流多重处理器中流处理器的个数，如果一个SM被分为多个束，则线程束中线程的个数等于一束中流处理器的个数。如一个具有100个线程的线程组分配给一个具有32个流处理器的SM，SM会将其分为4个线程束，第0\~31号线程分为第1束，第32\~63号线程分为第2束，第64\~95号线程分为第3束，第96\~99号线程分为第4束。在运行时，每个线程束中的32个线程会被分配到32个流处理器中进行并行运算。由于GPU任务的相似性，这32个线程很可能在同一时间遇到换出操作，那么SM就会安排下一个线程束进入流处理器。

为了实现SIMT(单指令多线程)，每个SP会维护一份它自己的基地址，指令分派器在遇到寻址指令时，会向每个SP发送一批相同的偏移地址，于是每个SP都可以通过不同的基地址和相同的偏移地址找到寄存器中的不同数据。

##### 动态分支语句如何影响GPU并行效率？

动态分支包括if语句和循环语句。它们在着色器语言中存在，但可能严重影响GPU性能。

在一个线程束中，如果线程中不存在动态分支语句，那么它们的所有行为都是可预测相同的。但一旦遇见一次动态分支语句，就可能产生分裂。如果线程束中所有线程执行相同的分支，那么运行结果不会有什么不同。但我们提到过，在SM中的一束线程共享同一份指令，但凡有一个线程执行另一个分支，那么整个线程束就不得不被执行两遍，**将两个分支的结果都运行一次，并让每个线程扔掉它们各自不需要的结果**。如果在着色器编写中出现连续的分支语句，甚至复杂的循环语句，每个线程束的执行次数可能呈指数级递增，这个效应被称为**线程分歧(thread divergence)**。

根据此我们也可以发现，使用循环语句运行常数次来读取某数组中的信息并不会导致线程分歧，线程分歧的严重与否关键在于相邻的元素通过动态分支语句能否得到基本相似的分支。

当然，新的图形学硬件部分解决了GPU执行动态分支语句的问题，但由于旧的硬件设备还没有彻底离开市场，所以无限制的使用动态分支语句还存在较高的风险，目前最稳妥的解决方案还是避免动态分支语句。在Unity中，如果编译平台是HLSL，可以使用[branch]/[unroll]/[loop]等编译指令强制要求将动态分支语句编译为分别执行的指令，即在执行if语句段时不再执行else语句段，反之在执行else语句段时不再执行if语句段。为了避免编译平台不是HLSL导致的编译异常，可以使用以下宏来避免：

~~~ShaderLab
#if defined(UNITY_COMPILER_HLSL)
	#define UNITY_BRANCH [branch]     //在if条件前使用，强制分支
	#define UNITY_FLATTEN [flatten]   //在if条件前使用，强制线程分歧
	#define UNITY_LOOP [loop]		  //在for条件前使用，强制分支
	#define UNITY_FASTOPT [fastopt]   //在for条件前使用，强制线程分歧
#else
	#define UNITY_BRANCH 
	#define UNITY_FLATTEN 
	#define UNITY_LOOP 
	#define UNITY_FASTOPT 
#endif
~~~

#### 显存

##### 显存是什么？

显卡储存体系的设计哲学是更大的内存带宽而非更低的访问延迟，这也是显存访问的特点：高带宽，高延迟。

显存既可以是物理上的，也可以是逻辑上的。

对集成显卡如Intel HD Graphics来说，GPU使用CPU专门划分出来的一份内存空间，即**UMA(Unified Memory Architecture，一致性储存架构)**作为显存，GPU和CPU用不同的虚拟地址对UMA中的同一个物理地址寻址。使用集成显卡时，CPU和GPU共享总线。在渲染时，CPU将顶点等数据存入主存，然后GPU可以通过**GART(Graphic Address Remapping Table，显存地址重定位表)**访问UMA。GART的作用是将UMA虚拟地址映射到GPU寻址空间。而由于UMA属于CPU内存范围，CPU可以直接访问它。

对独立显卡来说，GPU可以使用专门的显存条，并使用显存条的物理地址进行寻址，这是最常见的显卡类型。在独立显卡结构中，GPU可以直接从显存中读写信息。而CPU访问显存条中的储存空间时，需要映射一部分GPU储存空间到CPU地址空间，典型大小为256MB或512MB，CPU地址空间的获取一般由API完成。

无论使用哪种显卡，CPU和GPU交流必然要经过总线。独立显卡中CPU与显卡的沟通，是通过异步的**DMA(Direct Memory Access，内存直接访问)**实现的。主机将DMA命令块写入内存，DMA命令块由传输来源的源地址、传输目标地址和传输的字节数组成。CPU将这个命令块的地址写入DMA控制器，然后继续其它工作。随后DMA控制块会直接操纵内存总线，脱离主CPU的帮助下实现传输，将数据提供给显卡驱动。反过来，显卡驱动发送信息给DMA控制器请求线路，DMA控制器于是占用内存总线，并发送所需地址到内存地址总线，然后发送信号到DMA确认线路。当显存收到DMA确认信号时，他就传输数据到内存，并清除DMA请求信号。每当一次沟通结束，DMA都会触发一次CPU中断。

##### 显卡有什么样的储存结构？

显卡中的物理储存器按存取速度的大小从快到慢依次有：寄存器、共享内存、L1缓存、L2缓存、纹理缓存、常量缓存、全局内存(显存)。

寄存器位于SM中，访问速度是1个时间周期，SP运行时可以随意的读和写寄存器。在指令分派单元的控制下，每个线程都会获得自己的寄存器空间，

共享内存和L1缓存都位于SM中，它们都可以被SM中的所有SP共用。L1缓存重点存放顶点和片元数据，而共享内存重点存放材质参数、光照、摄像机等常量数据。共享内存和L1缓存的访问速度都低于32个时间周期。

L1缓存和L2缓存重点用于顶点和片元数据的交换。L2缓存位于SM之外，它的访问速度相对L1较慢，大致需要32至64个时间周期，但由于大量SM会共用一个L2缓存，L2缓存的吞吐量是数个L1缓存的总和。在有些显卡设计中，L2缓存还可以作为纹理缓存和常量缓存的新一层缓存，供其它显卡硬件使用。

纹理缓存和常量缓存是显卡上重要的缓存类型，它们都是显存的直接缓存，访问延迟在400个时间周期以上，如果出现未命中，要等待访问显存的话，这个延迟甚至很可能超过1000个时间周期。

全局内存也就是显存，它的数据直接来自CPU总线。显卡驱动会在GPU流水线空闲时将任务数据从GPU缓冲区导入显存。显存的访问延迟在500个时间周期以上，显卡的访问一般在纹理缓存和常量缓存缺失时才会发生。但存在一种高吞吐GPU结构，使用更大量的GPU而减少了缓存的层数，这种结构中显存可能直接被共享内存访问，通过高吞吐量与高带宽来缓解缓存层级少带来的性能缺陷。

<div STYLE="page-break-after:always;"></div>

### 渲染流水线

#### 渲染流水线综述

##### 渲染流水线的运行过程是怎样的？

渲染流水线是在显存中开启的。CPU将网格(顶点数组)、材质、贴图、着色器等注入显存后(**Draw Call**发生后，详情见**【渲染控制】**章)，GPU开始渲染流水线。

渲染流水线分为应用阶段(渲染控制)、几何阶段、光栅化阶段和像素阶段，在某些书籍中将光栅化阶段和像素阶段合称为光栅化阶段。渲染流水线的目的是最终将运算结果送到显示器的缓冲区中。

几何阶段分为**顶点着色器->曲面细分着色器(DirectX11和OpenGL4.x以上可编程)->几何着色器->裁剪->屏幕映射**五个步骤。

光栅化阶段分为**三角形设置->三角形遍历->片元着色器->逐片元操作**四个步骤，其中三角形设置和三角形遍历阶段可以合称为光栅化。

显卡厂商会通过硬件实现常用且功能变化不大的几个流水线阶段，因为通过硬件实现的效率远高于软件实现。这些阶段有的根据API的设计，提供了一些可以设置的参数，但总的来说不会脱离GPU的控制。

在这个过程中我们可以知道，顶点着色器和片元着色器的线程数并不是等同的。

重点强调GPU渲染流水线和CPU的指令流水线并不同：一种常见的**误解**是认为渲染流水线和指令流水线一样，不同阶段处理的是不同的Draw Call，较后的阶段处理着较早提交的Draw Call，较前的阶段处理着较晚提交的Draw Call。**这种理解是错误的。**正确的理解是，渲染流水线中，不同阶段运行的是同一次Draw Call，但一次Draw Call中先提交的数据可以不等待它之后的数据就进入下一个流水线阶段。在一个GPC中，可能有若干个SM执行顶点着色器，若干个SM执行片元着色器，同时一个GPC中的其它硬件也在运转，但它们**都在执行同一个Draw Call里的不同数据**。

可见，GPU的并行是一次Draw Call中各个数据间的并行，而非多个Draw Call之间的并行。GPU处理连续的多个Draw Call是串行的，已被GPU接受但还来不及处理的Draw Call以及其它CPU指令被暂存在GPU的指令缓冲区中，等待GPU的任务运行结束后读取新的Draw Call指令。单个Draw Call中不同元素的执行顺序是不可预测的，但多个Draw Call间的运行顺序和CPU的提交顺序是严格一致的。

##### 流水线中有多少缓冲区？

<img src="Textures\流水线缓冲区.png" alt="流水线缓冲区" style="zoom:33%;" />

+ 顶点缓冲区(Vertex Buffer)：由于GPU与CPU是异步的，顶点缓冲区被用于平衡两种速度不一致的硬件。通过顶点缓冲区，GPU可以访问CPU设定的顶点数组，通过图形API我们可以手动定制顶点缓冲区的大小和格式。顶点缓冲区位于显存中。
+ 帧缓冲区(Frame Buffer)：分为前置缓冲区和后置缓冲区，通过交换两个缓冲区可以保证显示器渲染的连续性，避免屏幕撕裂。帧缓冲区的大小主要由颜色缓冲区的大小决定。帧缓冲区位于显存中，通过图形API我们也可以定制帧缓冲区的大小和格式，在引擎中这一步被高度封装以至于我们很少在意对帧缓冲的设置。
+ 颜色缓冲区(Color Buffer)：颜色缓冲区是帧缓冲区的一部分，和帧缓冲区、显示器中的视频控制器相连。颜色缓冲区早期用4个字节来储存颜色，俗称十六位图，但现在的计算机一般通过32位RGBA储存颜色，俗称真彩。实现了HDR技术的显示器配置的显卡，可能具有64位RGBA的颜色缓冲区。
+ 深度缓冲区(Z-Buffer)：如果场景中两个物体在同一个像素产生片元，GPU会比较二者的深度，保留离观察者较近的物体。如果两个片元的深度一致，由于GPU的并行性，无法确定某个片元始终处于另一个之上，进而使这两个片元出现闪烁，这个效应被称为**深度冲突(Z-Fighting)**。深度缓冲位数过低时，深度冲突发生的可能性就会增加，目前的深度缓冲一般使用24位或32位精度。
+ 模板缓冲(Stencil Buffer)：模板缓冲为每个像素保存一个无符号整数值，这个值的含义由开发者定义。模板缓冲也可以由图形API定制，可以用于实现很多有趣的功能。模板测试发生在透明度测试之后，深度测试之前。一般的模板缓冲使用8位无符号整数。
+ 几何缓冲(Geometry Buffer，或G-Buffer)：详情见**【渲染控制-延迟渲染】**

整个渲染流水线，可以被理解为以顶点缓冲区为输入，以帧缓冲区，尤其是颜色缓冲区，为输出的过程。

##### 缓冲区内存该如何计算？

假设屏幕在真彩模式下显示一个2160×1080的图像，那么每个像素需要4个字节储存颜色，那么单个颜色缓冲需要的空间是：
$$
2160*1080*4B=8.90MB
$$
使用双缓冲区技术，则空间翻倍，每个像素使用8个字节储存颜色，再加上24位的深度缓冲，8位的模板缓冲，现在占用的空间是：
$$
2160*1080*(2*4B+3B+1B)=26.70MB
$$
如果使用硬件抗锯齿处理，比如多重采样抗锯齿，或者HDR技术，则需要的储存空间会更多，一般来说会增加4倍。

##### 什么是HLSL/GLSL/CG/CUDA？

HLSL、GLSL和CG是着色语言，专门用于编写着色器，在图形学中的地位类似传统编程当中的C语言，是对GPU硬件的抽象，能直接指导GPU每个运算单元的逻辑。其中HLSL(High Level Shading Language)属于DirectX，GLSL(OpenGL Shading Language)属于OpenGL，而CG(C for Graphic)是NVIDIA研发的，因为英伟达与微软合作密切，CG语法与HLSL极其相似。

一直以来，CG语言在HLSL和GLSL当中夹缝求生，使其成为兼容性最好的着色器语言，这也是Unity一开始选择CG作为其ShaderLab的基础语言的原因。但随着CG语言自2012年起不再更新，Unity开始寻求替补语言，这时由于CG语言与HLSL具有高度的相似性，考虑到用户的学习成本，Unity选择了HLSL作为新的官方着色器语言。

CUDA(Compute Unified Device Architecture，统一计算架构)也是NVIDIA研发的，和前三者类似，但并不专注于图形领域，常用在机器学习等领域，不需要像着色语言那样使用图形计算的逻辑进行数字运算。

#### 顶点阶段

##### 顶点着色器是如何工作的？

**顶点着色器(Vertex Shader)**的输入是CPU通过Draw Call发出的顶点数组(注意，**不包含索引数组**，所以顶点着色器并不了解顶点间的拓扑结构)，GPU从顶点缓冲中将顶点数组分批送入L1缓存并执行顶点着色器运算。不是所有顶点缓冲中的顶点都能被渲染，因为它们可能被遮挡或位于视锥体外。CPU在执行Draw Call时会使用**包围盒相交测试**判断网格是否**完全**处于视锥体外，如果是，则根本不会执行这次Draw Call，这样的操作被称为**剔除(Culling)**。即使是包围盒处于视锥体内的网格，也不一定存在位于视锥体内的顶点。这些位于视锥体外的顶点将在之后进行的若干次裁剪中尽可能快地被抛弃。

顶点着色器可以深度自定义，开发者需要根据显存中的原始数据(raw data)**输出顶点的齐次裁剪空间坐标**(如果使用曲面细分或几何着色器，也可以留待之后的着色器中再变换到齐次裁剪空间坐标输出)，同时可以自定义需要被传递到片元的其它参数。

束管理器将第一批顶点数据(输入的顶点数与SP数相同)以及其它需要的法线等顶点属性和变换矩阵等常量数据存入寄存器，然后将顶点着色器代码存入指令缓存，并要求指令分派单元从指令缓存中读取指令分派给SP。这个过程结束后，L1缓冲中必须至少保存着所有顶点的目标位置，如果有需要，还可以将其它需要硬件插值并送入几何、片元等着色器的数据一起送入L1缓冲。

为了将输入的顶点数据由模型坐标空间转移到齐次裁剪空间，顶点着色器应该经过**模型空间变换(从模型坐标转移到世界坐标)**→**观察空间变换(从世界坐标转移到摄像机坐标)**→**投影空间变换(从摄像机坐标转移到齐次裁剪空间坐标)**。这三个步骤使用的矩阵分别被称为**Model**矩阵，**View**矩阵和**Projection**矩阵。它们的复合矩阵，也就是从模型坐标直接转移到齐次裁剪空间坐标的矩阵，被称为**MVP矩阵**。详见【数学变换-空间变换】。

在这之后，SM中顶点着色器阶段产生的运算结果会被存入L1缓存，然后由GPU将多个SM的结果组合冲入位于SM结构外的L2缓存。这些储存在寄存器中的数据，会随之达到L2缓存中。经过裁剪、屏幕映射和三角形设置之后，在三角形遍历阶段生成片元时中它们会被硬件自动执行线性插值，然后到达片元着色器阶段。

##### 曲面细分着色器是如何工作的？

**曲面细分着色器(Tesselation Shader)**又被译为镶嵌着色器，它是两个可编辑着色器的统称，由**细分控制着色器**和**细分计算着色器**组成，在显卡上的一个名为**视口变换器(Viewport Transform)**的硬件模块上运行实现，一个显卡上可能有1~4个视口变换器集成电路，它们有些是可编程的有些是不可编程的，在具有GPC结构的显卡中，视口变换器一般位于GPC外。视口变换器可以从L2缓存中获取顶点和索引数组，并将L2缓存作为它与显存沟通的桥梁。

很多情况下，若使用了曲面细分着色器，顶点着色器可能就不会处理空间变换，将这一步曲面计算着色器或几何着色器中进行。

曲面细分着色器将复杂的曲面转换为简单的点、线、三角形。曲面细分着色器可以递归的增加网格细度，并在细分后的顶点上生成插值属性。由于它处理了邻接顶点的信息，它处理的结果会更加平滑，而不会产生跳跃间隙。在比较老旧的API版本中曲面细分着色器由硬件实现无法编程。在DirectX11以上或OpenGL4.x以上的版本中，可以通过API编辑这个着色器的工作任务。

下面用Unity Shader实现了一个简单的曲面细分着色器：

~~~ShaderLab
#pragma hull hs //细分控制着色器
#pragma domain ds //细分计算着色器

//............略去顶点着色器
struct v2t
{
	float4 vertex : INTERNALTESSPOS;
	float3 noraml : NORMAL;
}

//定义了对三角形进行细分的方法，InputPatch<v2t, 3>表明输入的patch为3个v2t结构体，对应三角形的3个顶点。
//这种方法可以更大程度上自定义每个三角形的细分方式
UnityTessllationFactors hsConstFunc(InputPatch<v2t, 3> v) 
{
	UnityTesslationFactors o;
	//设定将三角形的每条边分割为三段
	o.edge[0] = 3;
	o.edge[1] = 3;
	o.edge[2] = 3;
	//设定在三角形内部生成3个额外顶点
	o.inside = 3;
	return o;
}

/*
//使用Unity内置函数的版本，只需要控制一个_Density属性就可以根据摄像机的距离智能的进行细分。
UnityTessllastionFactors hsConstFunc(InputPatch<v2t, 3> v)
{
	UnityTesslationFactors o;
	float minDist = MIN_DISTANCE; //细分最小距离，小于细分不在增加
	float maxDist = MAX_DISTANCE; //细分最远距离，超出不在细分
	float density = _Density;     //细分密度，这个属性是可调的，可以控制整体的细分密度
	float4 factor = UnityDistanceBasedTess(v[0].vertex, v[1].vertex, v[2].vertex, minDist, maxDist, density);
	o.edge[0] = factor.x;
	o.edge[1] = factor.y;
	o.edge[2] = factor.z;
	o.inside = factor.w;
	return o;
}
*/

//在细分控制着色器中计算得出了顶点的重心坐标SV_DomainLocation
[UNITY_partioning("fraction_odd")]//声明了细分因子的舍入方式
[UNITY_outputtopology("triangle_cw")]//声明输出格式是顺时针三角形
[UNITY_patchconstantfunc("hsConstFunc")]//声明细分因子由前面的函数hsConstFunc定义
[UNITY_outputcontrolpoints(3)]//声明输出patch时的顶点数量为3，即每个三角形将新增3个顶点
v2t hs(InputPatch<v2t,3> v, uint id : SV_OutputControlPointID)
{
	return v[id];
}

//细分控制着色器结束，细分计算着色器开始
//通过重心坐标求出细分顶点的坐标和法线，将它们输入后面的流水线
[UNITY_domain("tri")]//声明输入的bary是三角形重心坐标
t2f ds(UnityTessellationFactors tessFactors, const OutputPatch<v2t,3> vi, float3 bary : SV_DomainLocation)
{
	t2f v; 
	v.vertex = vi[0].vertex * bary.x + vi[1].vertex * bary.y + vi[2].vertex * bary.z;
	v.normal = vi[0].normal * bary.x + vi[1].normal * bary.y + vi[2].normal * bary.z;
	return v;
}
//......略去后面的其它流水线
~~~

**细分控制着色器一般由两个函数组成，第一个函数是计算UnityTessellationFactors**，这个细分因子用于决定几何图元如何进行细分，这个函数的输入为InputPatch<v2t, 3> v，指明了输入的类型为3个v2t结构体组成的三角形。**函数的输出为o.edge和o.inside，分别指明每条边细分的段数(即边上的细分顶点数+1)，以及三角形内部额外生成的顶点数。**内部细分分为两种情况，如果indise等于偶数，那么在三角形正中心会出现一个细分点，如果等于奇数，则中心没有细分点。

如下图中，o.edge分别为3,5,2，o.inside=3：

<img src="Textures\细分控制因子.png" alt="细分控制因子" style="zoom:33%;" />

通常，考虑到LOD效果，我们可以调用Unity提供的函数UnityDistanceBasedTess来计算不同等级的细分：

~~~ShaderLab
//下面展示的全部为Unity系统函数，互相存在调用关系，着色器开发者仅需调用UnityDistanceBasedTess
float4 UnityDistanceBasedTess (float4 v0, float4 v1, float4 v2, float minDist, float maxDist, float tess)
{
	float3 f;
	//每个三角面的三个顶点基于距离的细分值
	f.x = UnityCalcDistanceTessFactor (v0,minDist,maxDist,tess);
	f.y = UnityCalcDistanceTessFactor (v1,minDist,maxDist,tess);
	f.z = UnityCalcDistanceTessFactor (v2,minDist,maxDist,tess);

	return UnityCalcTriEdgeTessFactors (f);
}

//根据minDist，maxDist映射出实际的tess
float UnityCalcDistanceTessFactor (float4 vertex, float minDist, float maxDist, float tess)
{
	float3 wpos = mul(_Object2World,vertex).xyz;
	float dist = distance (wpos, _WorldSpaceCameraPos);
	//由上面计算出的dist与minDist,maxDist重映射出细分值（1~tess)
	float f = clamp(1.0 - (dist - minDist) / (maxDist - minDist), 0.01, 1.0) * tess;
	return f;
}

float4 UnityCalcTriEdgeTessFactors (float3 triVertexFactors)
{
	float4 tess;
	tess.x = 0.5 * (triVertexFactors.y + triVertexFactors.z);
	tess.y = 0.5 * (triVertexFactors.x + triVertexFactors.z);
	tess.z = 0.5 * (triVertexFactors.x + triVertexFactors.y);
	tess.w = (triVertexFactors.x + triVertexFactors.y + triVertexFactors.z) / 3.0f;
	return tess;
}
~~~

UnityDistanceBasedTess的前三个参数分别是三个顶点的模型空间坐标，第四第五个参数是细分距离的上界和下界，函数会计算顶点到像机的距离，并得出合适的细分因子。这个函数一个四维向量，返回值的x、y、z分量分别是UnityTesseFactors的o.edge[0~2]，返回值的w分量是细分因子中的o.inside。

**细分控制着色器的第二个函数是hs函数，它通过#pragma hull hs编译指令进行指定**。这个函数需要通过四个特性语句来控制行为。

特性**[UNITY_patchconstantfunc("hsConstFunc")]指定了细分因子的计算方式**。

显然，细分因子应该为四个整数，但我们构建的函数将返回四个浮点数，我们需要在**[UNITY_partitioning(“fractional_odd”)]特性中声明细分因子的舍入方式**，fractional_odd意为在[1,max]范围内将Factor截断，然后取整到小于此数的最大奇数整数。

**特性[UNITY_outputtopology(“triangle_cw”)]决定图元的朝向**，由组成三角形的三个顶点的顺序所产生的方向决定，cw为clockwise顺时针，ccw为counter clockwise逆时针。

**特性[UNITY_outputcontrolpoints(3)]应为hull shader输出的outputpatch中的顶点数量**。

~~~ShaderLab
[UNITY_partioning("fraction_odd")]//声明了细分因子的舍入方式
[UNITY_outputtopology("triangle_cw")]//声明输出格式是顺时针三角形
[UNITY_patchconstantfunc("hsConstFunc")]//声明细分因子由前面的函数hsConstFunc定义
[UNITY_outputcontrolpoints(3)]//声明输出patch时的顶点数量为3，即每个三角形将新增3个顶点
//细分控制着色器函数hs的格式是固定的，如下所示
v2t hs(InputPatch<v2t,3> v, uint id : SV_OutputControlPointID)
{
	return v[id];
}
~~~

**细分计算着色器由ds构成，它通过#pragma domain ds编译指令指定**。这个函数负责计算由hs传入的细分顶点的位置，以及空间转换。ds函数的输出t2f将被送到几何着色器阶段，或屏幕映射阶段。

~~~
[UNITY_domain("tri")]//声明输入的bary是三角形重心坐标
t2f ds(UnityTessellationFactors tessFactors, const OutputPatch<v2t,3> vi, float3 bary : SV_DomainLocation)
{
	t2f v; 
	v.vertex = vi[0].vertex * bary.x + vi[1].vertex * bary.y + vi[2].vertex * bary.z;
	v.normal = vi[0].normal * bary.x + vi[1].normal * bary.y + vi[2].normal * bary.z;
	return v;
}
~~~

**特性[UNITY_domain("tri")]决定了输入数据SV_DomainLocation的类型，不同类型图元在定义bary参数时需要使用不同长度的向量**，其中tri定义了bary参数为三角形重心坐标，为float3类型。根据三角形重心坐标计算公式可得细分顶点的坐标为：

~~~
v.vertex = vi[0].vertex*bary.x + vi[1].vertex*bary.y + vi[2].vertex*bary.z;
~~~

在使用曲面细分着色器的情况下，一般不再在顶点着色器里进行顶点空间转换，而在细分计算着色器的末尾(或之后的几何着色器中)进行空间变换。如果没有顶点动画等需求，则顶点着色器只起到传递数据的作用。

##### 几何着色器是如何工作的？

**几何着色器(Geometry Shader)**是一个可选着色器，也在视口变换器上实现，它需要保证输出到裁剪阶段的顶点位于齐次裁剪空间。

几何着色器位于顶点和片元着色器之间，它以一个或多个顶点或三角形等**基本图形**作为输入，输出另外的一个顶点序列或三角形序列，以此改变图形的形状。仅仅是改变形状那它的作用就和顶点着色器没区别了，几何着色器最大的特点是能改变顶点的数量。对于每一个输入，几何着色器可以返回0到40个输出，但如果一个输入对应的输出数大于27，性能会明显的下降。

几何着色器不止能用于修改顶点坐标和数量，它相比顶点着色器还有一个特点，即**几何着色器是在视口变换器中实现的，可以访问索引数组查询顶点的拓扑结构**，这是顶点着色器无法做到的。在一个经典的线框渲染算法中，就使用几何着色器访问了三角形，并计算了每个顶点到对边的垂线长，以此巧妙地通过插值获得了三角形中所有点到三角形边的最短距离。

几何着色器相比曲面细分着色器，更注重变换而非细分，换句话说，曲面细分着色器擅长在不改变网格外形的前提下，通过硬件对网格进行平滑的细分操作；而几何着色器关注通过增加顶点的数量来对网格进行顶点着色器难以实现的变形。几何着色器关心逐图元操作，这里的图元指点/线/三角形等基本图形。几何着色器在生成简单的几何形状时尤其有效，比如使用几何着色器可以实现MarchingCubes【见几何数据-体积建模-步进立方体算法】。

以HLSL为例，典型的几何着色器定义如下：

~~~hlsl
#pragma geometry geom

//...略去前面的着色器
[maxvertexcount(6)]
void geom(triangle v2g input[3], inout PointStream<g2f> outstream)
{
	for(int i = 0; i < 3; i++)
	{
		g2f o = (g2f)0;
		o.vertex = input[i].vertex;
		o.uv = input[i].uv;
		outstream.Append(o);
	}
	outstream.RestartStrip();
}
//...略去后面的着色器
~~~

通过标签[maxvertexcount(N)]来定义几何着色器单次调用输出的最大顶点个数，这个输出顶点个数应当是输出类型对应的顶点数的整数倍。在几何着色器中，通过使用outstream.Append(o)来将一个g2f顶点输出，每次调用几何着色器的输出个数应当等于maxvertexcount定义的顶点个数。

函数的第一个参数类型决定了可以输入的基本图形，它们包括：

> point：每批处理一个顶点，声明格式为point v2g input[1]。
>
> line：每批处理一条线段上的两个顶点，声明格式为line v2g input[2]。
>
> triangle：每批处理一个三角形上的三个顶点，声明格式为triangle v2g input[3]。最常用的输入类型就是triangle，注意，三角形的输出顺序相对法向量是逆时针的。
>
> lineadj：每批处理一条线段及其相邻两端的两条线段，共三条线段四个顶点，声明格式为lineadj v2g input[4]。
>
> triangleadj：每批处理一个三角形及其邻接的三个三角形，共四个三角形六个顶点，声明格式为triangleadj v2g input[6]。

注意，在通常情况下，网格是由三角形组成的，所以只能接收triangle类型的输入，使用其它类型的输入时可能导致网格无法渲染。如果想使用其它输入序列，需要保证网格索引数组的拓扑类型和这里的输入类型相匹配，在设置网格的EBO时可以对此进行设置：

~~~C#
mesh.SetIndices(idx, MeshTopology.Points, 0);//Triangles、Quads、Lines、Points、LineStrip
~~~



<img src="Textures\d3d11-gsinputs1.png" alt="输入类型" style="zoom: 67%;" />

函数的第二个参数决定了几何着色器的输出类型，它们可以是TriangleStream、LineStream和PointStream，即HLSL中的**流输出对象类型(Stream-Output Object)**，它们分别对应了3个、2个和1个顶点。

几何着色器的输出是线性连续的，如果我们希望确保一次输出的结尾不会跟下次输出的开头连接在一起，使用outstream.RestartStrip()来保证输出流的格式正确。

##### 裁剪阶段是如何工作的？

**裁剪(Cliping)**也由视口变换器实现，但它完全不可编辑。裁剪可能发生在流水线中的不止一处，但通常指代的是在**透视投影之后，齐次除法之前**执行的裁剪。

在齐次裁剪空间中，而非之后的NDC或屏幕空间中执行裁剪，是因为模型空间变换、观察空间变换和投影空间变换都属于仿射变换，所以**齐次裁剪空间中的顶点之间仍然是线性相关的**，可以直接使用线性插值。而透视除法将齐次坐标转化为三维坐标，**NDC中顶点之间的关系已经不再是线性相关的**，不能再使用线性插值。在齐次裁剪空间中运算可以有效保证在线段截取和三角形截取当中的插值运算准确性。详见【数学变换-空间变换】。

齐次坐标系是由$[Math Processing Error](x,y,z,w)$构成的四维坐标系，而透视除法后的NDC对应的空间是$[Math Processing Error][-1,1]\times[-1,1]\times[-1,1]$的三维标准立方体。理论上，在NDC中需要考虑的六个裁剪平面为$[Math Processing Error]x=\pm 1,y=\pm1,z=\pm1$，根据三维空间的裁剪方法可以得到被标准立方体裁剪过后的点集，将这些顶点重新齐次化(这步是为了保证插值结果线性相关)，然后再进行线性插值，见【多边形建模-布尔运算】。

但实际上裁剪过程可以直接在齐次坐标系中完成，为了实现在四维空间中的裁剪，我们需要找到在齐次裁剪空间中组成四维超立方体的八个裁剪超平面，即在四维空间中对应了$[Math Processing Error]x=\pm 1,y=\pm1,z=\pm1$的六个超平面，然后对其进行线性裁剪。

假设一个点$P(x,y,z,w)$位于裁剪空间内部，那么经过透视除法后应该满足以下不等式：
$$
-w\le x,y,z\le w\\
near\le w\le far
$$
根据上述不等式，就可以得到八个裁剪超平面：$w=\pm x, w=\pm y, w=\pm z,w=near,w=far$。找到裁剪平面后，只要判断点与面的关系，即内侧还是外侧，然后判断线段与面的关系，最后求出相交线段与平面的交点，即插值参数$t$即可。

判断点与面的关系只要使用前面的不等式即可，所以我们关注求插值参数的方法。以$w=x$平面为例，假设点$P_1(x_1,y_1,z_1,w_1)$位于裁剪空间内，点$P_2(x_2,y_2,z_2,w_2)$位于裁剪空间外，线段$P_1P_2$与$w=x$超平面交于点$I$，可以得到$I$的参数表达式$I=P_1+t(P_2-P_1)$。由于$I$位于$w=x$上，有$I.x=I.w$，即$w_1+t(w_2-w_1)=x_1+t(x_2-x_1)$，得：
$$
t=\frac{w_1-x_1}{(w_1-x_1)-(w_2-x_2)}
$$
$[Math Processing Error]w=\pm x, w=\pm y, w=\pm z$平面的交点参数都可以类比求得。对于平面$[Math Processing Error]w=near$，则有$[Math Processing Error]t=\frac{near-w_1}{w_2-w_1}$。

了解了裁剪平面和裁剪插值的求法，自然就可以应用**Cohen-Sutherland算法**或**Liang-Barsky算法**等裁剪算法完成对线段和三角形的裁剪。详见【多边形建模-布尔运算】。

##### 屏幕映射是如何工作的？

屏幕映射紧跟在裁剪阶段之后，也由视口变换器负责，不可编辑。它的输入是齐次裁剪空间的顶点坐标，输出屏幕空间的顶点坐标。为了实现这一点，屏幕映射阶段需要进行两步空间变换，先将顶点从齐次裁剪空间坐标转换到**归一化设备坐标(NDC)**，然后将NDC转换到屏幕空间坐标。

首先，视口变换器将齐次裁剪空间坐标通过**透视除法(齐次除法)**转化为**NDC(Normalized Device Coordinates，归一设备坐标)**。不同图形API中的NDC格式可能不同，其中OpenGL规定了顶点着色器输出的NDC是左手坐标系，位于坐标范围在(-1,-1,-1)至(1,1,1)的立方体中，随后运行时编译器会将输出转化成对应硬件需要的规格。

第二步，将NDC中的坐标通过**屏幕空间变换**转换到屏幕坐标系(Screen Coordinate)。所谓屏幕坐标系，就是将范围在(-1,1)的x和y轴坐标，缩放到与目标分辨率相同大小，而z坐标则不做处理。目标分辨率的值与执行图形渲染的硬件和软件(如窗口大小)有关。这个阶段将输出屏幕坐标系下的顶点坐标、顶点深度、顶点法线方向、视角方向等顶点属性。详见【数学变换-屏幕空间变换】。

在OpenGL中，屏幕的左下角为屏幕坐标系原点，右上角为坐标最大值。而在DirectX中，屏幕左上角为最小屏幕坐标，而右下角为最大屏幕坐标。这个差异是OpenGL和DirectX很多不兼容性产生的源泉。

#### 光栅化阶段

##### 三角形设置是如何工作的？

这个阶段由视口变换器负责，不可编辑。

在这个阶段，我们需要获得三角形网格每条边的顶点信息，在此光栅化引擎通过读取索引数组理清顶点和顶点间的关系，并组织成三角形边界数据。GPU会对这些三角形边界数据编号，并利用它们计算所覆盖的片元。三角形设置的输出就是这样的边界数据，它的目的是为三角形遍历做准备。

GPU的处理策略是，处理过某批顶点的SM，尽量用来处理由同一批顶点生成的片元。

##### 三角形遍历是如何工作的？

视口变换器将打包好的网格数据交给GPC后，GPC会将这些网格交给名为**ROP(Raster Operations Units，光栅化引擎)**的元件，在这里网格被进行**扫描变换(Scan Conversion)**，ROP中的元件ROPU并行地计算像素是否被网格覆盖，如果是，则产生一个**片元(fragment)**，其中片元的状态是对网格3个顶点的信息进行**插值**得到的。ROP不可编程。

在生成片元的同时，ROP还会同时进行裁剪、背面剔除和**早期深度测试(Early-Z Testing)**。这几个操作是可以通过API进行配置的，但不可编程。

在片元着色器中的大量片元都可能因为不能通过深度测试而被抛弃，所以浪费了大量性能用于不必要的光照运算。实现Early-Z技术的GPU在三角形遍历阶段维护G-Buffer，提前抛弃了不需要渲染的片元——详情见【什么是前向渲染，什么是延迟渲染】。

片元还不是一个像素(pixel)，一个片元是用于生成一个像素的数据包，它包含了坐标、颜色、深度、法线、导数和纹理坐标等一系列计算像素所需要的数据。而像素则是片元经过整个光栅化阶段后，由片元所含的数据计算得出的，仅包含坐标和颜色信息。

在生成片元后，ROP将片元分配给同一个GPC中的几个SM。

##### 着色插值模式是什么？

我们将光照的计算分为三种，分布叫做**Flat Shading**、**Gourand Shading**和**Phong Shading**，其中Gourand Shading也被称作**逐顶点着色**，Phong Shading也被称作**逐像素着色**，它们的差别体现在插值的方式。

> 注：区别Phong Shading和Phong BRDF。

Flat Shading根据顶点法向量计算面法向量，并将面法向量应用到整个面的所有片元上。它的速度最快，可以得到符合人认知的渲染结果，但会轻易暴露物体的多边形本质。对于不需要任何平滑效果的网格，如具有硬线条的机器时，可以使用这种着色模式。

**Gourand Shading也就是逐顶点着色**，它的特点是先根据顶点法线计算顶点光照，得到顶点颜色，然后通过线性插值得到片元颜色。这个模式的平滑感比Flat Shading好了不少，但在渲染高光时很容易出现不真实的几何形状的光斑。

**Phong Shading也就是逐片元着色**，它的特点是先根据顶点法线插值得到片元法线，然后在片元着色器中计算片元颜色。这种模式是平滑效果最好的模式，但也是对GPU消耗最高的模式。

**一种折中的策略是，在顶点着色器到片元着色器的阶段同时插值顶点颜色和顶点法线**。先使用Gourand Shading策略对顶点着色，如果亮度较高则认为该顶点位于高光范围内，切换到Phong Shading策略进行法线插值，在片元着色器中利用法线数据对其重新着色；如果亮度没有达到高光阈值，则片元着色器直接舍弃法线数据直接提交插值颜色。由于片元数量一般都远大于顶点数量，这个策略能在达到**比Gourand Shading更好的高光效果的同时获得比Phong Shading更好的性能**。

注意，由于从顶点到片元着色器传递参数时的插值过程是直接烧录在硬件中的，所以我们不能通过修改顶点或片元着色器来实现Flat Shading。Flat Shading本质上是保证三角形中任意一个片元的法线方向一致，我们可以通过重新设置顶点法线，使其与三角形法线的几何方向一致来实现Flat Shading。在Unity中，可以直接在导入设置里将法线的生成方式修改为Generate，并将平滑范围设置为0。除此之外，我们还可以在几何着色器中对所有三角形进行法线重生成。如果想将适配Flat Shading的模型预生成并缓存下来，也可以使用这样的CPU代码对网格进行重运算：

~~~C#
void ProcessTriangles(Mesh oldMesh, out Mesh newMesh)
{
    Vector3[] oldVerts = oldMesh.vertices;
	int[] oldTris = oldMesh.triangles;
	Vector3[] newVerts = new Vector3[oldTris.Length]; //注意长度用Tris而非Verts
    int[] newTris = new Vector3[oldTris.Length]
	for (int i = 0; i < oldTris.Length; i++)
	{
		newVerts[i] = oldVerts[oldTris[i]];
 	    newTris[i] = i;
	}
	newMesh.vertices = newVerts;
	newMesh.triangles = newTris;
	newMesh.RecalculateBounds();
	newMesh.RecalculateNormals();
}
~~~

##### 曲线是如何被光栅化的(中点画线法)？

曲线是通过被细分成许多段直线进行光栅化的，所以我们必须先了解平面上直线的光栅化。

设直线的斜率为k，当$k=0,k=\pm1,k=\pm\infty$时，直线的光栅化方法显而易见。所以我们只考虑在$0\lt|k|\lt1$的情况下直线的光栅化算法，这样也可以类比推出在$1<|k|<\infty$下的算法。

**DDA算法**是最简单的直线光栅化算法，它的核心思路在于步进和通过舍入选择像素。

当$0\lt|k|\lt1$时，选择对x进行步进，即$x_{i+1}=x_i\pm1$，则$y_{i+1}=y_i\pm k$。相对的，当$1<|k|<\infty$时，选择对y进行步进，即$y_{i+1}=y_i\pm1$，$x_{i+1}=x_i\pm\frac1k$。最后选择对$x_{i+1}$和$y_{i+1}$进行舍入后的像素点进行光栅化。

**中点画线法**是经典的直线光栅化算法。它相比于DDA的进步在于巧妙的避免了浮点运算，有效提升了渲染效率。

直线的一般式方程为$F(x,y)=Ax+By+C=0,B\ge 0$，这条直线将平面分为三个区域：直线上方的点、直线下方的点以及直线上的点。其中，对于直线上方的点，$F(x,y)>0$；直线下方的点，$F(x,y)<0$；直线上的点，$F(x,y)=0$。

中点画线法也使用步进思想，但它在每次移动时，根据中点误差项判断而非舍入来决定是在水平/竖直方向上移动一个像素，还是在斜方向上移动一个像素。

<img src="E:/Textures/图形学/中点画线示意图.png" alt="中点画线示意图" style="zoom:50%;" />

在上图中，像素点$P(x_i,y_i)$已经被选中，接下来直线既穿过了像素点$P_d$又穿过了像素点$P_u$，我们应当在这两个像素中选择一个进行绘制。我们考察直线与线段$P_dP_u$的交点$Q$，如果它更靠近$P_d$则绘制$P_d$，否则绘制$P_u$。

于是我们将$P_u$与$P_d$的中点$M$纳入考量，将$M$点坐标代入方程$F(x_i,y_i)$，即，如果结果小于0，则$M$在$Q$下方，取$P_u$，如果结果大于0，则$M$在$Q$上方，取$P_d$。如果恰等于0，则两者皆可。

我们记$d_i=F(x_i+1,y_i+0.5)$，当然，每次移动都代入方程来计算$d_i$是合理的，但并不高效。我们试图通过找到递推公式$d_{i+1}=d_i+\Delta d$来减少代入方程带来的计算。
$$
d_i=F(x_i+1,y_i+0.5)=A(x_i+1)+B(y_i+0.5)+C
$$
当$d_i<0$时，取$P_u$，即右上方的像素，则
$$
d_{i+1}=F(x_i+2,y_i+1.5)=A(x_i+2)+B(y_i+1.5)+C=d_i+A+B
$$
当$d_i \ge 0$时，取$P_d$，即右方的像素，则
$$
d_{i+1}=F(x_i+2,y_i+0.5)=A(x_i+2)+B(y_i+0.5)+C=d_i+A
$$
总结可得：
$$
d_{i+1}=
\left \{ \begin{array}{c}
d_i+A+B&d_i<0\\d_i+A&d_i\ge 0
\end{array}\right.\ \ \ \ \ \ \ \ \ \ ,d_0=A+0.5B
$$

由于d只关心符号，所以可以使用2d代替d来避免浮点运算，进一步提升性能。
$$
d_{i+1}'=
\left \{ \begin{array}{c}
d_i'+2A+2B&d_i<0\\d_i'+2A&d_i\ge 0
\end{array}\right.\ \ \ \ \ \ \ \ \ \ ,d_0'=2A+B
$$


**Bresenham算法**与中点画线法实际上是同一算法，只是对算法的解释不同：Bresenham定义了$d_y=y_2-y_1$，$d_x=x_2-x_1$，并定义了$\Delta E=2d_y$，$\Delta$ NE$=2(d_y-d_x)$。令$d_0=2d_y-d_x$，并在$d\ge0$时对x和y坐标同时递增，使$d$增加$\Delta$ NE​；在$d\lt0$时之增加x坐标，同时使$d$增加$\Delta E$。下面我们演示用Bresenham算法计算直线段$P_1(0,0)P_2(7,5)$生成的片元序列：
$$
d_y=y_2-y_1=5,d_x=x_2-x_1=7\\\Delta E=2d_y=10,\Delta NE=2(d_y-d_x)=-4\\
d_0=2d_y-d_x=3
$$

| i    | $x_i$ | $y_i$ | $d_i$ | changes                 |
| ---- | ----- | ----- | ----- | ----------------------- |
| 0    | **0** | **0** | 3     | x++, y++, d+=$\Delta$NE |
| 1    | **1** | **1** | -1    | x++, d+=$\Delta$E       |
| 2    | **2** | **1** | 9     | x++, y++, d+=$\Delta$NE |
| 3    | **3** | **2** | 5     | x++, y++, d+=$\Delta$NE |
| 4    | **4** | **3** | 1     | x++, y++, d+=$\Delta$NE |
| 5    | **5** | **4** | -3    | x++, d+=$\Delta$E       |
| 6    | **6** | **4** | 7     | x++, y++, d+=$\Delta$NE |
| 7    | **7** | **5** | 3     |                         |

在光栅化曲线时，只需要将曲线参数方程映射到像素坐标系下，对不同的参数取值将曲线细分成多条线段，在线端上应用直线光栅化算法即可。

##### 多边形是如何被光栅化的(扫描变换法)？

扫描变换算法，又称为**扫描线填充算法(Scanline Algorithm)**，是一种由GPU硬件自动执行的多边形光栅化经典算法。

扫描变化的目的是根据输入的顶点序列，得到多边形覆盖的片元坐标，并生成对应的片元。在此过程中，GPU还会对所有顶点属性进行自动插值。在生成扫描线时，我们不断生成平行于x轴的线段来与多边形相交，并取出位于多边形内部的片元进行处理。我们可以观察多边形与扫描线的交点情况，可以得出两个结论：

<img src="Textures\扫面线填充.gif" alt="扫面线填充" style="zoom:67%;" />

+ 每次只有相关的几条边可能与扫描线有交点，所以一个最优的算法不应对所有的边进行求交计算；
+ 扫描线每次固定上移一个单位，所以其与同一条线段的多个交点间的坐标变化量也固定，这个变化量与线段斜率有关；

为了减少计算量，我们需要维护一张由“活动边”组成的表，称为“**活动边表(AET)**”。例如在上图中，扫描线4的“活动边表”由P1P2和P3P4两条边组成，而扫描线7的“活动边表”由P1P2、P6P1、P5P6和P4P5四条边组成。

假设当前扫描线与多边形的某一条边的交点已经通过直线段求交算法计算出来，得到交点的坐标为$(x, y)$，则下一条扫描线与这条边的交点不需要再求交计算，通过步进关系可以直接得到新交点坐标为$(x + \Delta x, y + 1)$。设直线方程为$ax+by+c=0$，若$y=y_i,x=x_i$，当$y=y_{i+1}$时，有$x_{i+1}=x_i-b/a$，其中$\Delta x=b/a$为常数。这个方法被称为增量法，使用增量法计算时，我们需要知道一条边何时不再与下一条扫描线相交，以便及时把它从活性边表中删除出去。于是，对于AET中的每个节点(交点)，至少应当保存的信息包括：

+ 当前扫描线与边的交点坐标X值
+ 当前扫描线与边交点，和下一条扫描线与同一条边交点间的增量$\Delta X$
+ 该边所交的最高扫描线编号Y
+ 指向同一个扫描线与下一条边(右侧)的交点的节点

对AET中的每个扫描线，遍历其所有节点，对其中的每对节点，生成一排片元。

为了方便活性边表的建立与更新，我们为每一条扫描线建立一个新边表(**NET**)，存放在该扫描线第一次出现的边。也就是说，若某边的较低端点为ymin，则该边就放在扫描线ymin的新边表中。对于NET中的每个节点，应该保存的信息包括：

+ 当前边的起始点(ymin)的坐标X值
+ 该边的增量$\Delta X$
+ 该边所交的最高扫描线编号Y
+ 指向下一条边的交点的节点

对于上图中的多边形，可以构造NET如下，其中每个节点的第一项为终止点的扫描线编号，第二项为起点的X坐标，第三项为$\Delta X$：

<img src="Textures\扫描变换NET.png" alt="扫描变换NET" style="zoom: 25%;" />

根据NET我们可以迭代的构造AET，如下：

<img src="Textures\扫描变换AET.png" alt="扫描变换AET" style="zoom: 25%;" />

> AET是在扫描过程中动态变化生成的，上图中展示的是扫描线从1迭代到9的过程中NET随时间的变化，并不是同时存在的。

在AET中，我们对每一对边取出将要渲染的片元序列。对每一条扫描线$S_i$遍历其AET中的节点，对每一对节点$N_{left}$和$N_{right}$，生成像素坐标为$P(X,Y)$的片元，其中$N_{left}.x\le X\lt(\le) N_{right}.x,X\in N^+$且$Y=i$。

生成片元的方法可以是左闭右开的，也可以是左闭右闭的，下面展示基于前者生成出的片元序列：

> (9,9);  
>
> (8,8)......(10,8);  
>
> (2,7)......(4,7),  (8,7)......(11,8); 
>
> (2,6)......(6,6),  (7,6)......(12,6);  
>
> (2,5)......(12,5);  
>
> (2,4)......(10,4);  
>
> (1,3)......(8,3);
>
> (3,2)......(6,2);

在GPU硬件中，由于导入的多边形数据不可能超过4条边，所以我们可以对NET进行进一步的简化，没有必要对每个扫描线构造多个个节点链表，而只需要将3或4条个节点按从下到上→从左到右的顺序组成单个链表就行了。为此，我们需要在原来NET节点中插入起点的Y坐标。

例如，输入三角形$A(1,3),B(5,7),C(7,6)$，这个三角形组成的NET为$(1,3,5,1)\rightarrow(1,3,7,\frac12)\rightarrow(7,6,7,-2)$，其中第一项为起点X坐标，第二项为起点Y坐标，第三项为重点Y坐标，第四项为$\Delta X$。我们可以通过按顺序遍历NET来更新AET并生成片元。

##### 什么是抗锯齿(反走样)?

在三角形遍历阶段中我们将三角形交给了ROP，我们将三角形画在这样的像素图上比对：

<img src="Textures\抗锯齿_0.jpg" alt="抗锯齿_0" style="zoom: 67%;" />

可以发现，有的像素点全部在三角形中，有的部分在其中，有的完全没在其中。但在屏幕上绘制时却不能只绘制像素点的一部分，要么全部不绘制，要么全部都绘制。假设我们判断像素是否绘制的标准是像素的中点是否被三角形覆盖，则得到以下的片元分布图：

<img src="Textures\抗锯齿_1.jpg" alt="抗锯齿_1" style="zoom: 67%;" />

这个图形和一个完美的三角形还有很大的差距，这种和平滑图形差距很大的问题就是我们要解决的“锯齿”。

锯齿的出现可以理解为像素精度不够，那么我们该如何巧妙地提升像素精度呢？

**SSAA(Super-Sampling Anti-Aliasing，超级采样抗锯齿)**的思路简单粗暴——我们只要在采样时将分辨率翻倍就好了。假设屏幕分辨率是1920×1080，那么4×SSAA采样就会在3840×2160的分辨率等级上生成片元，并将片元渲染到3840×2160的缓冲区上，**等所有渲染过程结束后，再向下采样**取得1920×1080的图像。SSAA可以得到最好的抗锯齿效果，但问题也很明显，就是带来了四倍的光栅化阶段工作量和内存消耗。这种奢侈的算法几乎被所有显卡厂商抛弃了。

**MSAA(Multi-Sampling Anti-Aliasing，多重采样抗锯齿)**在SSAA的基础上进行了优化，放弃了扩大分辨率，而是估算三角形覆盖像素的面积占整个像素的比例，这样那些中心不位于三角形内，但部分被三角形包含的片元也得到了计算。为了实现估算，我们在每个像素中放入了多个采样点，将三角形覆盖的采样点个数与每个像素中采样点个数的比值作为估算的依据，如图：

<img src="Textures\抗锯齿_2.jpg" alt="抗锯齿_2" style="zoom: 67%;" />

注意到，为了使采样结果尽可能的多样，我们不能将采样点整齐的摆放成正方形。理想的采样点摆放是稀疏的，对于N个采样点，任意两个采样点不应该出现在一个N×N网格的同一行、列及对角线上。通过著名的N皇后问题算法可以解出满足这种稀疏摆放条件的采样点。

于是我们可以根据三角形包围的像素点的数量来加权的计算颜色值，得到如下的效果：

<img src="Textures\抗锯齿_3.jpg" alt="抗锯齿_3" style="zoom: 67%;" />

这个抗锯齿的性能比SSAA好了不少，且效果也不差，但它又有一个致命的问题，就是对满大街都是的延迟渲染结构不友好——在延迟渲染中片元会被光栅化到G-Buffer上，而不是直接生成片元。假如一个像素具有4个采样点，那么G-Buffer上每个像素都可能需要保存最多4个不同的颜色数据，为了保存这些颜色，G-Buffer需要4倍的空间储存颜色，这使得MSAA需要的缓存又回到了和SSAA差不多的数量级。抛开延迟渲染问题不谈，MSAA是商业运用中产生效果最好的一种抗锯齿算法。

为了适应延迟渲染，现代引擎里一般使用**FXAA(Fast Approximate Anti-Aliasing，快速近似抗锯齿)**和**TXAA (Temporal Anti-Aliasing，时间滤波抗锯齿)**这类技术。它们的普遍特点是抛弃精度，通过相邻像素模糊来实现平滑和抗锯齿，开启了这类抗锯齿效果的场景会有明显的边界不清问题，但它们应该是性能最好的抗锯齿效果了。

以FXAA为例，MSAA和SSAA都属于硬件设计范畴，而FXAA属于软件优化范畴，在三角形遍历阶段通过ROP解决。而**FXAA可以被理解为一次额外的屏幕后处理**。FXAA的核心思路是，对每个像素计算它与周围像素的梯度，确认该像素是否位于边缘，如果是，判断边缘的方向，对边缘方向的连续两个或三个像素进行加权颜色混合，以实现模糊的效果。

#### 像素阶段

##### 片元着色器是如何工作的？

**片元着色器(Fragment Shader)**可以深度编程，开发者需要根据提供的片元数据输出一个像素颜色。输入的片元数据至少包括**片元的屏幕坐标(只读)和深度(只读)**，一般来说，还包含法线和纹理坐标等信息，具体传入哪些数据是高度自定义的。

束管理器会将片元数据存入寄存器，然后将片元着色器代码存入指令缓存，并要求指令分派单元从指令缓存中读取指令分派给SP。每一批次中指令会从寄存器中取出若干片元数据开始处理，如果一束有32个线程，则就是32个片元，准确来说是8个2x2的片元块，**2x2是片元着色器的最小工作单位**。所有线程运行完后，寄存器中必须生成所有片元的目标颜色。

在片元着色器中会将2x2的相邻片元作为不可分割的一组送入同一个SM内的4个不同的SP中。这么做的目的是实现在片元着色器中，通过插值进行屏幕空间下的求导(ddx和ddy函数)。由于求导函数是通过2x2结构实现的，所以通过函数求导时求得的是左导数还是右导数实际上是由片元的屏幕坐标决定的。在使用求导操作时，需要同时对4个片元进行求值，然后通过共享内存求出差值，所以在动态分支语句中，由于线程分歧的存在，求导函数可能无法得出正确的解。为了避免线程分歧对求导函数的影响，GLSL和HLSL不约而同地选择了禁止在动态分支语句中使用求导函数。

注意，2x2块中可能存在无效像素，当网格覆盖的片元不是完整的2x2块时，比如说一个网格只覆盖了单个片元，那么在进入片元着色器时，会将它与相邻的3个空片元绑定到一起，这会导致有3个SP空转。在极端环境下，整个网格可能全部都处于这样的状态，使得SM的效率低至25%。这种为了覆盖完整2x2片元而浪费资源的情况被称为**过度渲染(Over Draw)**。

这些计算得到的目标颜色会和片元坐标一起存入L1缓冲，然后由GPU将多个SM的结果组合冲入L2缓存，并随后进入可见性测试阶段。片元着色器的输出不会立刻被写入帧缓冲区，其**运算结果必须在之后的流水线中通过模板测试和深度测试才能写入帧缓冲区**。在新一代的GPU结构中，深度测试也会在片元着色器之前开始，这种架构被称为**早期深度测试(Early-Z Test)**。

##### 逐片元操作(输出合并阶段)是如何工作的？

这个步骤在显卡上的一个名为**渲染输出器**的元件中实现，它从L2缓存中读取片元，处理可见性测试和混合。这两个个过程是可配置的，但不可编程。

以模板测试和深度测试为例：渲染输出器会首先将片元与**模板缓冲(Stencil Buffer)**中的模板值比对，舍弃没有通过**模板测试**的片元。片元通过模板测试后，渲染输出器就会将该片元与**深度缓冲(Z-Buffer)**中的深度信息比对进行**深度测试**，舍弃掉没有通过深度测试的片元。通过深度测试的片元就会与**后置缓冲区(Back Buffer)**中的像素进行混合。由于数据是高度可并行的，渲染输出器中的多个**渲染输出单元**会并行的执行这个过程。

在像素混合时，深度和颜色的设置必须是原子操作，否则会发生同步异常。所以GPU中对深度缓冲的读+写由一个专门的原子指令实现。

在一次渲染结束后，视频控制器会将后置缓冲区与**前置缓冲区(Front Buffer)**交换(交换指针而非内存)，而显示器可以直接读取前置缓冲区中的像素进行打印。使用前置和后置两个缓冲区的这种策略被称为**双重缓冲区(Double Buffer)**策略，二者合称为**帧缓冲区(Frame Buffer)**，它可以保障显示器显示的连续性，由于渲染过程始终在幕后发送，可以避免显示器打印出正在处理中的图元以致于产生屏幕撕裂。一般来说，由于前置缓冲区和显示器连接，而显示器不需要了解像素的深度值和模板值，所以前置缓冲区可以只包含颜色缓冲。注意，尽管显示器不需要了解像素的透明度，但前置缓冲区的颜色缓冲仍包含Alpha通道。

##### 什么是垂直同步？

早期CRT显示器中，电子枪从上到下进行扫描，扫描完成后显示器就显示一帧画面，然后电子枪回到初始位置进行下一次扫描。为了同步显示器的使用过程和系统的视频控制器，显示器会用硬件时钟产生一系列的定时信号：当电子枪换行扫描时，显示器会发送一个水平同步信号，简称**HSync**，当一帧画面绘制完成后，电子枪回复到原位，准备画下一帧前，显示器会发出一个垂直同步信号，简称**VSync**。

技术升级带来液晶屏后，**视频控制器(Vedio Controller)**仍在根据同步信号逐帧读取帧缓冲区的数据。为了缓解单缓冲下帧缓冲区的读取和刷新效率低的问题，GPU中常用两个缓冲区，即**双缓冲机制**，在一帧渲染完后，视频控制器会将两个缓冲互换。由视频控制器直接读取的缓冲区称为**前缓冲区**，而在后台承担渲染任务的叫**后缓冲区**。

双缓冲会引入一个新问题：在视频控制器读取前缓冲区只读取了一半时，GPU将新的一帧内容提交到帧缓冲区，并把两个缓冲区交换。视频控制器可能把新一帧数据的后半段渲染到了屏幕上，这使得屏幕得上半部分和下半部分分属不同得两帧，产生画面撕裂。

为了解决这个问题，GPU等待显示器发送得垂直同步信号来交换缓冲区，垂直同步信号沿用了的VSync这一称谓。垂直同步解决了画面撕裂现象，也增加了画面流畅度，但需要消费更多计算资源，也会带来部分延迟。

##### 什么是可见性测试？

透明度测试是简单的将透明度低于开发者设置的阈值的片元丢弃，透明度测试不能用于实现半透明效果，只能用于实现镂空效果。实现半透明应该使用透明度混合。

模板测试有一个对应的模板缓冲，这个缓冲区有一个大小等于目标分辨率的数据结构，对每一个像素储存了一个整数。模板测试是高度可编程的，可以通过图形API设定在模板缓冲中的什么部位写入什么数值，在渲染其它网格时可以将这些数值读取出来进行测试。比如“魔镜”效果：场景中有一些通常不能被看到的“幽灵”物体，只有透过一个“魔镜”去观察才能看到幽灵物体。这时我们可以让“魔镜”进行模板写入，将其覆盖的部分模板值写为1，而让幽灵物体进行模板测试，只渲染模板值为1的片元，以实现这个效果。

模板测试和模板写入的编程自由度非常大，可以使用很多数学运算和逻辑运算，这使得模板测试有很多高级的用法，如渲染阴影和渲染轮廓等。

深度测试用于抛弃那些被其它片元遮挡的片元，它是高度可定制但不可编程的。与深度测试对应的是深度写入，深度缓冲记录着当前离摄像机最近的片元的深度坐标，只有深度比这个片元更小的片元才有权利通过深度测试并将新的深度写入对应区域。我们可以关闭深度测试或深度写入，比如透明或半透明物体应该关闭深度写入，因为我们不希望透明物体遮挡它背后的片元。

##### 什么是模板测试?

**模板测试(Stencil Test)**的名字起源于印刷行业中的版面模子(Stencil)，在印刷时我们需要在模子上抠出需要的图案，然后将模子盖在要被印刷的材质上，透过抠出的洞涂颜色，而不需要上色的部分则会被模子挡住，避免了涂色越界。

模板测试就是将屏幕上的部分像素用一个8bit的通道盖住，在渲染前，剔除该通道上的值不等于测试值的像素，类似于在模子上挖洞，只渲染通过该测试的像素。

**模板测试主要应用于：延迟渲染(用于区分使用不同光照模型的物体)、外轮廓描边(法线外扩模板描边)、镜面反射(剔除超出边缘的像)和后处理(如多层叠加毛玻璃)等**。

在主流的GPU架构中，模板缓冲区和深度缓冲区是连在一起的，在一个32位的储存区域中有24位记录深度信息，紧邻着8位的模板信息。正是因为模板缓冲区和深度缓冲区关系如此之紧密，我们可以在模板测试中访问深度测试。如前面所说的，**模板测试先于深度测试**。

在Unity Shader中可以用以下语法进行模板缓冲区的读写：

~~~ShaderLab
Pass
{
	Stencil 
	{
		//当前像素stencil值与0进行比较
		Ref referenceValue  //0-255
		//一口气比较若干位stencil,此处比较最低的两位
		ReadMask 3
		//一口气写好多位stencil,此处修改最低的两位
		WriteMask 1
        //测试条件：测试是否相等，可以使用Comp、CompBack和CompFront分别测试全部/背面和正面
        //测试条件包括Greater、GEqual、Less、LEqual、Equal、NotEqual、Always、Never
        Comp Equal  
        //通过测试的结果，与测试条件对应使用Pass、PassBack和PassFront
        //如果测试通过对此stencil值进行的写入操作：保持当前stencil值, 所以操作类型的列举在本段代码最后
        Pass keep       //default:keep
        //测试失败的结果，与测试条件对应使用Fail、FailBack和FailFront
        //如果测试失败对此stencil值进行的写入操作：保持当前stencil值
        Fail keep       //default:keep
        //如果深度测试失败对此stencil值进行的写入操作：循环递增
        ZFail IncrWrap  //default:keep
        //操作方式包括Keep、Zero、Replace、IncrSat、DecrSat、Invert、IncrWrap、DecrWrap
        //其中Replace是把Ref写进去，Sat是有边界加/减，Wrap是溢出(循环)加/减
	}
}
~~~

举个例子，使用模板测试实现简易描边：

~~~ShaderLab
Stencil 
{
	Ref 0          //0-255
    Comp Equal     //default:always
    Pass IncrSat   //default:keep
    Fail keep      //default:keep
    ZFail keep     //default:keep
}
~~~

在默认情况下第一个pass前屏幕上所有像素模板都为0，则物体所有像素通过模板测试并使模板值加1，同时成功渲染物体。然后在第二个pass中，在法线方向放大所有顶点的位置，那么只有之前没有渲染过的新增的部分可以通过测试，这样就可以实现描边了。

模板缓冲在每一帧的整幅图像只清空一次，所以依靠模板缓冲可以轻易做到跨Pass和跨Shader的信息沟通。如果想在一帧中重复使用多次模板缓冲，可以设定好渲染顺序，在模板缓冲使用结束后用一个额外的Pass来手动清空模板缓冲。在一个大型团队中，你难以预料是否有其他人和你使用了同样的模板缓冲参考值，这可能导致bug。最好的解决方法是全团队进行协商分配不同的参考值序列，但如果这么做对你们的团队来说不可能，那就无论如何都额外写一个Pass来清理你的模板缓冲区，这可能导致性能下降，但总比渲染错误好。

##### 什么是混合？

在一个片元写入后置缓冲区中时，后置缓冲区中的对应位置有可能已经存在有像素信息了。妥善的处理旧的和新的像素信息就是混合要做的事。混合操作是高度可订制但不可编程的。

最常见的，同时也是默认混合策略是**覆盖(Cover)**，有时叫做**关闭混合(Blend Off)**。覆盖策略将旧像素信息丢弃而用新像素信息重写它，这种策略在所有不透明物体上使用，因为通过了深度测试的不透明新片元显然会遮挡旧片元。

对半透明物体来说，最常见的混合策略是**透明度混合(Alpha Blend)**，其思路是将新旧像素对透明度带权进行加法，得到新的颜色存入后置缓冲区。详见【色彩与混合-透明度混合】。

<div STYLE="page-break-after:always;"></div>

### 渲染控制

#### 图形指令

##### OpenGL和DirectX有什么区别？

OpenGL和DirectX都是图像编程接口(图形API)，是通过CPU对GPU发出指令的指令库，可以被各种高级语言包含(一般是C++)。图形API是渲染控制的基础，是CPU端与GPU端交流的桥梁。

注意OpenGL和GLSL的区别，以及DirectX与HLSL的区别，GLSL、HLSL是着色器语言，属于完全的GPU端语言，而OpenGL、DirectX是CPU端库文件，可以用来给GPU指定着色器，也就是发起Draw Call。

OpenGL是纯粹的图形API；DirectX是多种API的集合体，其中DirectX包含图形API——Direct3D和Direct2D。

DirectX支持Windows和Xbox；OpenGL支持Windows、MacOC、Linux等更多平台，在Android、IOS上允许使用OpenGL的简化版本OpenGL ES。

OpenGL相对来说易上手门槛低；DirectX难上手门槛高。OpenGL渲染效率相对低，特性少；DirectX相对效率高，特性多。如DirectX12提供了底层API，允许用户一定程度上绕过显卡驱动之间操纵底层硬件。

二者在图形学领域一样重要。OpenGL在各种领域都吃香，包括许多专业领域如特效和CG建模软件。DirectX在游戏中更通用。

##### 什么是Draw Call？

调用一次图像编程接口的渲染指令(OpenGL中的glDrawElements或DirectX中的DrawIndexedPrimitive)，以**命令GPU进行渲染的过程就称为一次Draw Call**。一般来说，每个摄像机和每个光源需要对场景调用一批Draw Call，其中摄像机调用Draw Call一般是为了渲染实时显示的画面，而光源调用Draw Call是为了渲染阴影纹理，这个纹理将在阴影效果的实现中起到重要作用(详见**【光和光照-光源与阴影】**)。

在每次Draw Call中，CPU都需要向GPU传递以下四种类型的数据：**顶点数组**(包括顶点坐标和其它顶点属性)、**索引数组**、**硬件渲染状态**(Early-Z、模板测试、背面剔除和混合等)以及**外部变量**(包括物体、光源和摄像机的变换矩阵，以及其它被定义为uniform的变量)。这四种数据共同指导GPU在一次Draw Call中的渲染行为，它们组成的数据集可以被称为一个**批(Batch)**。

一般来说，**Batch数量和Draw Call的次数相差不大**，而由于相比起渲染准备阶段传输数据消耗的时间，Draw Call指令本身的时间显得微乎其微，所以我们一般在谈到Draw Call次数和Batch次数时可以将二者混为一谈。Batch的数量一般接近于视锥体范围内不同的网格的数量，且可以通过批处理来减少。

除此之外，CPU还必须保证**着色器二进制文件**和**纹理(包括2D/3D/Cubemap)文件**已经存在于显存的Cache中。这两者严格意义上不能被计入Batch，因为并非每次Draw Call都需要传递这两种数据。**在进入新场景时，CPU会将场景中用到的所有着色器加载到CPU内存**，但直到**第一次使用一个Pass时才会将对应Pass编译，然后将编译为二进制的着色器文件送入GPU**，随后存入现显存Cache中。同理，只有到**第一次使用一个纹理时它才会被送入GPU**，并存在显存Cache中。显存的Cache自然有限，硬件使用类似CPU缓存区的方法管理GPU缓存。

在Draw Call时，CPU需要向GPU指定要使用的**着色器ID**(实际上是Pass ID，因为**引擎中由多Pass组成的着色器在底层实现上是多个着色器**)，但实际上由于GPU会缓存上一次使用的Pass，所以**只有在渲染状态需要切换时才需要向GPU传输新的Pass ID，这一步被称为Set Pass Call**。Set Pass Call的次数可以表达渲染状态切换的次数。

Set Pass Call的数量一般接近于视锥体内不同的材质的Pass数的总和加上后处理使用的Pass数的总和，换句话说，一次Set Pass Call一定代表了一个或多个 Draw Call。显然，Set Pass Call的数量高时，Draw Call次数一定高，但Set Pass Call数量低时，Draw Call次数不一定低。Set Pass Call次数可以通过优化Draw Call顺序，或者使用**常量缓冲区(CBuffer)**相关技术来减少。

总结CPU在Draw Call发生前和发生时的动作：

+ CPU把一个网格的顶点数组和索引数组从硬盘中加载到内存中。
+ 如果这是第一次使用一个Pass，CPU会将它编译成二进制文件。

+ CPU为这个网格准备一个Batch，这个Batch包括顶点数组、索引数组、硬件渲染状态和外部变量等。如果这是第一次使用一个Pass，或第一次使用一个纹理，CPU会将它们也放进这个Batch中。
+ 如果当前使用的Pass和上一次Draw Call时使用的Pass不同，CPU发出一次Set Pass Call。

+ CPU调用图形命令发出Draw Call，将Batch数据包交给DMA，由DMA将数据包通过总线传入显卡。

各类图形指令到达显卡驱动程序后，驱动会首先检查指令的合法性，如果指令非法，驱动会通过DMA向CPU发送错误信息。如果指令合法，驱动通过DMA确认Draw Call接收，然后将指令放入GPU指令缓冲区。

一段时间后当显卡中存在空闲，或者CPU显式发送flush命令后，驱动程序把缓冲区中的第一份指令发送给GPU，GPU通过主机接口接受命令，并开始处理命令。

DMA将所有顶点存入**顶点缓冲区(Vertex Buffer)**，GPU中的**图元分配器(Primitive Distributer)**开始通过顶点生成三角形，并将他们按照批次(batch)，发送给一个或多个GPCs，如果显卡不存在GPCs，则直接分发给多个SM。SM获得数据后，束管理器安排多边形引擎将三角形数据提取出来存入SM的L1缓存，随后开始顶点着色器阶段。

Draw Call顺序直接决定了渲染顺序。调整Draw Call顺序的核心是保证渲染在准确的基础上尽量高效。

GPU会按顺序串行处理指令缓存区中的每一个指令，指令的处理顺序与CPU提交Draw Call的顺序有关。正因如此，CPU总是最后提交透明物体的Draw Call。等某一帧中的所有Draw Call处理完毕后，显示器才会发送同步信号将图像打印在屏幕上。

对同一个Draw Call来说，GPU会按顶点的提交顺序将顶点注入流水线。所以在批处理中，也有必要按照网格距离摄像机的远近顺序来保存顶点缓冲。

透明物体一般都关闭了深度写入，开启了深度测试。这表明，如果透明物体先于不透明物体调用Draw Call，不透明片元就无法判断应该遮挡住透明片元还是调用透明度混合，而不透明片元如果调用了透明度混合，就会导致新的不透明片元在覆盖时丢失透明片元的原始信息，这就会导致错误。这代表，**透明物体的Draw Call必须晚于不透明物体。**

对不透明物体来说，渲染的策略是让尽量多的片元被深度测试抛弃，这样如果某些着色器开启了早期深度测试，就可以节省大量的光照计算。所以一般引擎的渲染策略是将**不透明物体尽量按距离摄像机的距离从近到远提交Draw Call**。这可以解释为什么天空盒总是在不透明物体之后渲染，在透明物体之前渲染，因为它是距离摄像机最远的不透明物体。

对于透明物体来说，由于CPU并不知道着色器的混合策略，所以不能保证透明物体的混合顺序是否会影响混合效果。为了保证准确性，**透明物体必须按距离摄像机的距离从远到近提交Draw Call**。这带来一个问题：如果两个使用了同一种渲染状态的透明物体A<sub>1</sub>和A<sub>2</sub>之间如果夹杂了使用另一种渲染状态的透明物体B(B在A<sub>1</sub>后面但在A<sub>2</sub>前面)，为了满足A<sub>2</sub>->B->A<sub>1</sub>的提交顺序，A<sub>2</sub>和A<sub>1</sub>不得不被拆分成两次Draw Call提交，而不能参与批处理。

##### 什么是批处理？

批处理的主要思想是，将多个物体渲染所需的数据，通过一个Batch而不是多个Batch传递到GPU中，可以大幅降低Batch的次数。Draw Call的性能瓶颈是CPU而非GPU。CPU每发送一个Batch，都要调用一次DMA将数据输入显存。在每次调用时，对显存的映射寻址、DMA控制块的注入、等待DMA响应等系统消耗都会浪费时间周期，多次发送Batch就会有多次消耗。同时，DMA擅长一次传输大量数据，而不擅长多次传输少量数据。这使得降低Batch数对于优化显示性能很有必要。

批处理主要分成静态批处理和动态批处理。

静态批处理，是一种在项目打包前将使用相同材质和纹理的物体打包成同一个渲染对象的预处理技术，**在设置时，需要指定哪些物体需要被计入静态批处理，系统会自动在预运算时统计与它使用相同材质的物体**。

由于静态批处理是一种预运算计数，所以静态批处理可以容纳的顶点数的理论上限是索引数组的上限，在旧的渲染设备中是65536个，在较新的渲染设备中为$2^{32}$个。

静态批处理的问题主要有两个：一是**不能处理任何动态物体**，如果在引擎中将一个物体设置成静态批处理物体，那么无论脚本中如何尝试移动它，都不能改变它渲染的位置(但可能会改变它的碰撞器坐标，使碰撞器与渲染的位置脱离从而造成诡异的异常)；二是**会大幅增加所需的硬盘空间和内存空间**，因为场景中使用相同网格的物体在不进行静态批处理的情况下，只需要储存一份Mesh数据和若干份坐标(Transform)数据，但使用静态批处理后就不得不保存若干份Mesh数据。例如，在茂密的森林场景中将数标记为静态会严重影响内存。

动态批处理，是一种在运行时将使用相同材质和纹理的物体打包成同一个渲染对象的实时优化计数，**在设置时，需要指定哪些材质需要被计入动态批处理，系统会自动在渲染时统计视锥体内使用相同材质的符合条件的物体**。

对透明物体和UI来说，如果两个或多个透明元素**同时满足**以下条件则可以被执行动态批处理：

+ 使用相同的材质与纹理，即使用相同渲染状态。
+ 这些元素的层级相同，或这些元素之间不夹杂使用其它渲染状态的元素的层级(原因见【如何控制Draw Call的顺序】)。

对网格(模型)来说，如果两个或多个网格**同时满足**以下条件则可以被执行动态批处理：

+ 使用相同的材质与纹理，即使用相同渲染状态。
+ 顶点总数不超过一个阈值，该阈值大小与使用的处理引擎有关，一般要求不超过总计900个顶点属性。
+ 网格没有镜像的缩放，即允许存在Scale(2,2,1)，但不允许存在Scale(-1,1,1)。
+ 物体的lightmap指向同一个位置。
+ 网格没有蒙皮骨骼动画。

无论是静态批处理还是动态批处理，批处理会将网格合并，这意味着**批处理后网格会获得一组新的模型坐标**。通常来说，引擎会将其中单个网格的轴点或者所有网格轴点的几何中心作为合并网格的坐标中心。这个特性会对一设计模型坐标计算的顶点或片元动画产生影响。

##### 指令缓冲有什么用？

**指令缓冲区(Command Buffer)**是**位于CPU中**的一个队列，很多设计模式都会使用指令缓冲区的概念，这里我们特指图形指令缓冲区。

在指令缓冲区中我们维护一系列的**渲染指令(Render Command)**，渲染指令本质上是一条或多条图形API指令的抽象，如OpenGL中的glClear指令和glDrawElement指令等，这些图形API指令我们会在**【图形指令】**一章中详细描述。

对于每一个摄像机和光源，引擎都会替我们维护一个指令缓冲区。在一次完整的渲染过程中，引擎会替我们遍历所有光源和摄像机，将它们的指令缓冲区中的指令翻译为OpenGL或DirectX的形式执行，然后OpenGL和DirectX会负责处理这些指令带来的Draw Call。我们可以通过往指定摄像机或光源的指令缓冲区中插入指令来对整个渲染流程进行自定义，并且对于重复性的指令来说，使用指令缓冲区也能产生复用的效果，减少性能消耗。

在下面这幅图中，用蓝色标注了可以使用Command Buffer在渲染周期中插入指令的阶段，以及他们。

<img src="Textures\CommandBuffer.png" alt="CommandBuffer.png" style="zoom: 67%;" />

用示例代码简单展示在Unity中使用指令缓冲区的方式：

~~~C++
[ExecuteInEditMode]
public class CommandBufferExample : MonoBehaviour
{
    //这个字典用于保存每个摄像机对应的CommandBuffer。
    private Dictionary<Camera, CommandBuffer> m_Cameras = new Dictionary<Camera, CommandBuffer>();
    
    //新建并绑定指令缓冲区，以插入到AfterLighting阶段为例：
	public CommandBuffer AttachCommandBuffer(Camera camera)
	{
	    CommandBuffer m_CBAfterLighting = new CommandBuffer();
		m_CBAfterLighting.name = "Custom Lighting";//这个name只用于Debug，不会影响渲染
		camera.AddCommandBuffer(CameraEvent.AfterLighting, m_CBAfterLighting);
        m_Cameras[camera] = m_CBAfterLightng;
        return m_CBAfterLightng;
	}

	//清空指令缓冲区
	public void ClearCommandBuffer(Camera camera)
	{
		m_Cameras[camera].Clear();
	}

	//向指令缓冲区中注入指令
	public void FillCommandBuffer(CommandBuffer cb)
	{
 	    //在这里插入任意数量的指令，格式如下：
 	    //cb.DrawMesh(......);
 	    //cb.SetGlobalColor(......);
	    //可以使用几乎所有Graphics中存在的指令
	}

	//在实际生命周期中运用这些函数，OnWillRenderObject会在每个摄像机进行渲染前被调用一次
	private void OnWillRenderObject()
	{
    	Camera cam = Camera.current;
        if(!cam) return;
        
        CommandBuffer cb;
        if(m_Cameras.ContainsKey(cam))
        {
            ClearCommandBuffer(cam);
            cb = m_Cameras[camera];
        }
        else
        {
            cb = AttachCommandBuffer(cam);
        }
        
        //在这段代码中假设FillCommandBuffer每帧的指令不同，所以需要Clear后重新填充，如果CommandBuffer是不变的，那么只需要注入一次然后不再清空即可
        FillCommandBuffer(cb);
	}
}
~~~

Command Buffer可以用来代替普通的后处理，也可以储存临时变量跨生命周期来使用。比如说，我们使用Command Buffer将每个渲染阶段中的图片保存下来，就实现了一个简单的FrameDebugger：

~~~C++
using UnityEngine;
using UnityEngine.Rendering;

public class CommandBufferTest : MonoBehaviour
{
    public Shader imageShader;

    void Start()
    {
        var cam = Camera.main;
        var mat = new Material(imageShader);

        {
            var tex0 = Shader.PropertyToID("GetTex0");

            CommandBuffer cmd1 = new CommandBuffer();
            cmd1.name = "GetTex0";

            cmd1.GetTemporaryRT(tex0, Screen.width + 1, Screen.height + 1, 0, FilterMode.Bilinear, RenderTextureFormat.ARGB32);   // RenderTexture详见【屏幕后处理-什么是RenderTexture】
            cmd1.Blit(BuiltinRenderTextureType.GBuffer0, tex0);
            cmd1.SetGlobalTexture(Shader.PropertyToID("_Tex0"), tex0);
            cmd1.ReleaseTemporaryRT(tex0);

            cam.AddCommandBuffer(CameraEvent.AfterGBuffer, cmd1);
        }

        {
            var tex1 = Shader.PropertyToID("GetTex1");

            CommandBuffer cmd2 = new CommandBuffer();
            cmd2.name = "GetTex1";

            cmd2.GetTemporaryRT(tex1, Screen.width + 2, Screen.height + 2, 0, FilterMode.Bilinear, RenderTextureFormat.ARGB32);   // RenderTexture详见【屏幕后处理-什么是RenderTexture】
            cmd2.Blit(BuiltinRenderTextureType.CameraTarget, tex1);
            cmd2.SetGlobalTexture(Shader.PropertyToID("_Tex1"), tex1);
            cmd2.ReleaseTemporaryRT(tex1);

            cam.AddCommandBuffer(CameraEvent.BeforeLighting, cmd2);
        }

        {
            CommandBuffer cmd3 = new CommandBuffer();
            cmd3.name = "_PostEffect";

            cmd3.Blit(BuiltinRenderTextureType.CurrentActive, BuiltinRenderTextureType.CameraTarget, mat);
            cam.AddCommandBuffer(CameraEvent.AfterImageEffects, cmd3);
        }
    }
}
~~~

##### Unity封装了哪些图形指令？

Unity中可以实现渲染的工具类包括GL类、Graphics类和CommandBuffer类。

Unity中的**GL是对OpenGL的简单封装**，可以处理激活转换矩阵，发出类似于OpenGL立即模式的渲染命令以及执行其他低级图形任务。注意：在几乎所有情况下，使用Graphics.DrawMesh或CommandBuffer都比使用立即模式绘图更有效。

GL立即绘图函数使用当前设置的"current material"(使用Material.SetPass指定)，因此，除非你在使用GL绘制函数前将其显式设置为某种东西，否则材质可能是任何东西。另外，如果你从GL绘图代码中调用任何其他绘图命令，它们可以将材质设置为其他内容，因此也需要确保其也处于受控状态。

GL绘图命令立即执行。这意味着，如果在Update()中调用它们，它们将在渲染照相机之前执行，并且照相机很可能会清除屏幕，从而使GL绘图不可见。调用GL绘图的通常位置是在摄像机附带的脚本的OnPostRender()或OnRenderImage()。

使用GL进行画线的例子：

~~~C++
 public void OnRenderObject()
    {
        CreateLineMaterial();
        //应用材质
        lineMaterial.SetPass(0);
        GL.PushMatrix();
        //设置用于绘制的变化矩阵以匹配我们的变换:将当前的模型矩阵设置为指定矩阵
        //用于将顶点从模型空间转到世界空间
        GL.MultMatrix(transform.localToWorldMatrix);
        //绘制lines
        GL.Begin(GL.LINES);
        for(int i = 0; i < lineCount; ++i)
        {
            float a = i / (float)lineCount;
            float angle = a * Mathf.PI * 2;
            //顶点颜色从红色到绿色
            GL.Color(new Color(a, 1 - a, 0, 0.8F));
            //在物体位置的顶点
            GL.Vertex3(0, 0, 0);
            //圆边上的另一个顶点
            GL.Vertex3(Mathf.Cos(angle) * radius, Mathf.Sin(angle) * radius, 0);
        }
        GL.End();
        GL.PopMatrix();
    }
~~~

Unity中的**Graphics是优化过的网格绘图功能的高级快捷方式，**可以控制指令缓冲区的工作，Graphics和指令缓冲区的函数和功能几乎一致，但Graphics不能实现将图形指令插入到任意阶段，可以将CommandBuffer理解为Graphics的升级。

<div STYLE="page-break-after:always;"></div>

#### 渲染管线

##### 什么是渲染管线和渲染路径？

**渲染管线**是通过安排CPU发出Draw Call的内容以及顺序，指导GPU进行一系列操作，并协同将开发者期待的图像渲染在屏幕上。

对一个渲染管线来说，它需要处理以下三个方面的问题：

+ 剔除(Culling，注意与裁剪Cliping区分开)
+ 渲染
+ 后期处理

一个渲染管线可以支持不同的渲染路径，CPU会根据渲染路径的区别来改变提交Draw Call的策略。

**渲染路径**是一系列对光照和着色(尤其是光照)的控制选项的集合。不同的渲染路径支持不同的效果并具有不同的性能表现。

主流的引擎实现的渲染路径包括前向渲染和延迟渲染，除此之外还包括一些其它的路径或是自定义渲染路径。

<img src="Textures\渲染路径.png" alt="渲染路径" style="zoom:50%;" />

上图展现了一种主流引擎的渲染管线，这个渲染管线具有两种可选的渲染路径，每个渲染路径由一系列渲染步骤组成。

在后文中我们还会介绍如何通过指令缓冲区来对渲染流程进行自定义，上图中绿色的点就是可以插入自定义命令的地方。

##### 前向渲染和延迟渲染有什么区别？

前向渲染和延迟渲染是两种不同的渲染路径，即两种不同的光照渲染模式。

前向渲染的逻辑是这样的：

+ 对每个摄像机渲染这样的一个画面，循环：
  + 对摄像机视锥体中的每个对象，循环：
    + 在ForwardBase通路中，输入网格和主有向平行光、靠近的逐顶点光源、靠近的球谐光源作为一个Batch，Drawcall
    + 计算逐顶点光照、球谐光照
    + 光栅化，在Early-Z中对片元进行深度测试，抛弃没有通过的片元
    + 计算主有向平行光的逐片元光照，更新帧缓冲
    + 对对象周围的每一个除了主有向平行光外的逐片元光源，循环：
      + 在ForwardAdd通路中，输入网格和一个逐片元光源作为一个Batch，Drawcall
      + 光栅化，在Early-Z中对片元进行深度测试，抛弃没有通过的片元
      + 计算该光源的逐片元光照，更新帧缓冲

可见，**在前向渲染路径中，场景中总的Batch次数大约等于摄像机个数×待渲染网格数×(除主有向平行光外的逐片元光照数+1)**。在片元光照的数量变多的情况下，前向渲染的Batch数将大幅度提升。为了降低这样的消耗，**大部分引擎会限制逐片元光照和逐顶点光照的数量**，而是推荐使用球谐光照进行渲染。在传统的固定管线中，动态光照的数量不能超过8盏。即使是当今的图形硬件，动态光也很难超过100盏。

前向渲染的另一个性能瓶颈在于深度测试，由于GPU的并行性，我们不能控制GPU取出片元的顺序。在极端条件下，每个片元都通过了早期深度测试，每个片元都进行了逐片元光照，但大部分片元都被后来的片元覆盖了，这浪费了巨量的性能。所以，如果场景中物体数量很多，并且不性的全部叠加在一起时，前向渲染的性能损耗会特别严重。

解决传统前向渲染的问题，多数引擎都引入了延迟渲染路径，它类似于一种后处理算法，并且引入了G-Buffer：

+ 对每个摄像机渲染这样的一个画面，循环：
  + 对摄像机视锥体中的每个对象，循环：
    + 在Deferred通路中，输入网格和法线贴图、漫反射贴图、镜面反射强度、烘焙光照贴图、阴影贴图等数据作为一个Batch，DrawCall
    + 光栅化，在Early-Z对片元进行深度测试，抛弃没有通过的片元
    + 计算网格在屏幕空间的法线、深度和漫反射的数据，并存入G-Buffer
  + 然后再对场景中的所有光源，循环：
    + 在一个可自定义的后处理通路中，将G-Buffer缓冲区与光源作为一个Batch，Drawcall
    + 通过G-Buffer中的数据与光源信息计算光照，并更新帧缓冲

可见，**延迟渲染将Batch数优化到了摄像机个数×(待渲染网格数+逐片元光照数)**，其中第一个循环被称为**几何通路**，第二个循环被称为**光照渲染通路**。延迟渲染把参数保存了下来，没有像前向渲染那样边运行片元着色器边进行输出合并，而是先完成完整的深度检测，再运行片元着色器，对于每个像素只进行一次光照计算就实现了效果，大大节约了光照计算复杂度。光源越多、计算越复杂，节省下的性能就越明显。

更形象的理解延迟渲染，可以将G-Buffer看成一个纹理。在几何通路中，系统使用与渲染后处理纹理类似的方式获取场景的几何信息，将它们储存在G-Buffer中；然后在光照处理通路中，系统读取G-Buffer中的几何数据对场景进行渲染。

<img src="Textures\GBuffer.png" alt="GBuffer" style="zoom:50%;" />

然而，由于延迟渲染的特性，只能保存观察空间中距离摄像机最近的片元信息，所以半透明网格会在G-Buffer中将它后面的片元完全覆盖。换句话来说，**延迟渲染完全不支持透明度混合**。同时，由于G-Buffer需要的带宽远超过帧缓冲，在多重采样抗锯齿下难以满足负载，所以**延迟渲染也不能实现多重采样抗锯齿的功能**。

一般的G-Buffer精度为64位，旧的分配方式是分别使用16位浮点数储存Normal.x、Normal.y、深度信息和漫反射颜色(十六位图)。一种新的分配模式是去掉深度，同时使用8位浮点数分别储存Normal.x、Normal.y、漫反射颜色、高光颜色，再使用24位储存RGB色彩，这样还留下了一个空闲的8位通道用作机动，并且色彩精度也提升了。新分配模式的问题是normal位数下降了很多必须通过片元着色器来代行平滑。

新的支持延迟渲染的显卡可能提供超过64位的精度，可以使延迟渲染的效果更上一层楼。

##### 如何使用前向渲染路径？

在真实感光照一节中，我们会知道逐顶点光照比逐片元光照性能好得多，而使用球面调谐函数(简称球谐函数)进行光照近似计算的性能更好。所以在前向渲染路径中，我们只对若干个能照亮所有物体的光源执行逐片元光照计算。剩下的光源中，默认最多4个点光源(该数量可自定义)执行逐顶点光照计算。剩下的光源则以球面调谐(Spherical Harmonics)的方式计算光照【见真实感渲染-间接全局光照模型】。

一个光源是否以逐片元光照的方式计算，主要取决于以下三个条件：

+ 用户可以主动设定光源为**非逐片元光照**，如在Unity中，被设置为Not Important的光源必定不使用逐片元光照模式进行光照计算。
+ 用户可以主动设定光源为逐片元光照，如在Unity中，被设置为Important的光源必定使用逐片元光照模式进行光照计算。
+ 除了用户设定的逐片元光之外，**没有被设置成Not Important的亮度最高的有向平行光将使用逐片元光照模型进行计算**。
+ 如果逐片元光照的光源个数没有超过设定好的上限(在Unity中，这个值在Quality Setting -> Pixel Light Count中设置)，则距离摄像机最近的若干个光源也会以逐片元模式进行光照计算，直到补足逐片元光照上限。

前向渲染的渲染通路(Pass)有两种，分别是基本通路(ForwardBase)和附加通路(ForwardAdd)。**在基本通路中，对所有默认的有向平行光源以及有向平行光产生的阴影进行逐片元光照，同时，对其它基于顶点的和基于球谐光照的光源，以及物体的自发光、统一环境光(作为常数叠加的环境光)和光照贴图进行计算。**在**附加通路中，则对剩下的使用逐片元光照的光源进行光照计算，且这个Pass对每个额外的光源都会执行一次**，由于每个Pass计算单独的一个光源的效果，所以在着色器的附加通路中一般使用Blend One One标签来实现光照强度的叠加。

假如场景中存在八个光源A\~H，它们的颜色和亮度都一样，而用户也没有手动设置它们是否使用逐片元光照。这八个光源到物体O的距离按照A\~H的顺序递增，则距离物体最近的前四个光源A\~D将以逐片元光照计算，D\~G使用逐顶点光照计算，G\~H使用球谐函数计算。其中，光源D会参与逐片元和逐顶点两种模式的计算，光源G会参与逐顶点和球谐两种模式的计算，这是为了光照效果能平滑地变化而不至于显得突兀。

前向渲染中的光源主要分为有向平行光、点光源和锥形光三种，注意线光源和面光源一般只会在离线渲染中使用，在前向渲染管线中可以通过读取预先烘焙好的光照贴图来获得线光源和面光源的光照效果：

+ 有向平行光源

  **平行光**，有时也被称为无穷远点光源，是最容易实现的光源，其几何定义是最简单的。由于平行光没有确定的光源位置，也不存在衰减，所以平行光的光强场始终返回一个固定的值。它时常用于表现阳光，因为由于太阳距离地球很远，在一定范围内其强度和方向可以视为不变。

  平行光的几何属性只有方向，且没有光源位置和衰减。换句话说，平行光的强度不会随距离改变。在Unity中，为了方便着色器调用，定义了宏_WorldSpaceLightPos0，它虽然名为LightPos，但在计算平行光时其含义是平行光的方向。

+ 点光源

  点光源能照亮的空间是有限的，它是由空间中的一个球体定义的。点光源可以模拟由一个点发出的，向所有方向延伸的光。光的亮度值随着与光源的距离变大而逐渐变小，当超过照亮范围时亮度值截断为0。点光源发出的光线在某点的光度值与某点到光源距离的平方成反比。

  点光源的几何属性包括了光源坐标_WorldSpaceLightPos0、强度、衰减等。一般来说点光源也会包含一个光照边界属性来加速渲染，在点光源球心处的光照强度最强，然后光照强度随着距离衰减，而在距离超过这个边界时截断到0，这么做可以有效控制每个光源照射到的物体的个数，减少渲染消耗。

+ 锥形光源

  锥形光源又名聚光灯光源，它由一个锥形定义，用于表示由一个特定位置出发，向特定方向延展的光。这个锥体的几何属性包括光源坐标_WorldSpaceLightPos0、空间角的大小、强度和衰减。聚光灯光源也包括一个光照边界，其逻辑与点光源类似。

一般来说，为了避免在着色器中重复计算复杂的光照衰减函数，会对点光源或锥形渲染一个**光照衰减纹理**，在着色器中不再使用算法来计算光照强度，而是直接在这个衰减纹理中采样获得衰减程度。对于参数基本保持不变的光源来说使用这个纹理的性能比在着色器中根据参数实时计算要好。在使用光照衰减纹理时，可以将顶点或片元的世界坐标通过一个矩阵变换到光源坐标空间，这个矩阵在Unity中名为\_LightMatrix0。变换后，根据光源的类型求得在光照衰减纹理中采样的坐标，在光照衰减纹理中采样。这个光照衰减纹理在Unity中被命名为\_LightTexture0。

在Unity中，由于ShaderLab的特殊性，一个Shader文件可能包含多个pass。在实现光照时，引擎可能向同一个shader中的不同pass填充不同的光照变量。为了控制光照变量，在CGPrograme块中使用#pragma multi_compile_fwdbase和#pragma multi_compile_fwdadd来区分需要填充的变量类型。

~~~ShaderLab
Shader "Forward"
{
    Properties
    {
		_Diffuse("Diffuse",Color) = (1,1,1,1)
		_Specular("Specular",Color) = (1,1,1,1)
		_Gloss("Gloss",Range(8.0,256)) = 8.0
    }
    SubShader
    {
        Tags { "RenderType"="Opaque" }
        LOD 100

        Pass //ForwardBase
        {
			Tags{"LightMode" = "ForwardBase"}
            CGPROGRAM
			#pragma multi_compile_fwdbase 
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"
			#include "Lighting.cginc"
			fixed4 _Diffuse;
			fixed4 _Specular;
			fixed _Gloss;

            struct appdata
            {
                float4 vertex : POSITION;
                float2 uv : TEXCOORD0;
				float3 normal:NORMAL;
            };

            struct v2f
            {
                float4 vertex : SV_POSITION;
				float3 worldNormal : TEXCOORD0;
				float3 worldPos:TEXCOORD1;
				float3 vertexLight : TEXCOORD2;
            };

            sampler2D _MainTex;
            float4 _MainTex_ST;
			
			//在ForwardBase中顶点着色器需要处理逐顶点光照和球谐光照
            v2f vert (appdata v)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(v.vertex);
				o.worldNormal = UnityObjectToWorldNormal(v.normal);
				o.worldPos = mul(unity_ObjectToWorld,v.vertex).xyz;
				
				#ifdef LIGHTMAP_OFF
				float shLight = ShadeSH9(float4(v.normal,1.0)); //球谐光照
				o.vertexLight = shLight;
				#ifdef VERTEXLIGHT_ON
				float3 vertexLight = 		
				Shade4PointLights(unity_4LightPosX0, unity_4LightPosY0, unity_4LightPosZ0,
					unity_LightColor[0].rgb,
					unity_LightColor[1].rgb,unity_LightColor[2].rgb,unity_LightColor[3].rgb,
					unity_4LightAtten0,o.worldPos,o.worldNormal); //这个函数一次性计算了4个逐顶点光照
				o.vertexLight += vertexLight;
				#endif
				#endif
                return o;
            }
			
			//ForwardBase中片元着色器需要处理有向平行光的逐片元光照，环境光和自发光(此处没有写自发光)
            fixed4 frag (v2f i) : SV_Target
            {
				fixed3 worldNormal  = normalize(i.worldNormal);
				fixed3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos));
				fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz;
				fixed3 halfLambert = dot(worldNormal , worldLightDir) * 0.5 + 0.5;
				fixed3 diffuse = _LightColor0.rgb * _Diffuse.rgb * halfLambert;
				
				fixed3 viewDir = normalize(UnityWorldSpaceViewDir(i.worldPos));
				fixed3 halfDir = normalize(worldLightDir + viewDir);

				fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0,dot(worldNormal,halfDir)), _Gloss);

                return fixed4(ambient + diffuse + specular + i.vertexLight, 1.0);
            }
            ENDCG
        }

		Pass //ForwardAdd
		{
			Tags{"LightMode" = "ForwardAdd"}
			Blend One One

			CGPROGRAM
			#pragma multi_compile_fwdadd
            #pragma vertex vert
            #pragma fragment frag

			#include "Lighting.cginc"
			#include "AutoLight.cginc"

			fixed4 _Diffuse;
			fixed4 _Specular;
			fixed _Gloss;

			struct a2v
			{
				float4 vertex:POSITION;
				float3 normal:NORMAL;
			};
			struct v2f
			{
				float4 pos:SV_POSITION;
				float3 worldNormal:TEXCOORD0;
				float3 worldPos: TEXCOORD1;
				LIGHTING_COORDS(2,3);
			};
			
			//ForwardAdd不需要在顶点着色器计算光照
			v2f vert(a2v v)
			{
				v2f o;
				o.pos = UnityObjectToClipPos(v.vertex);
				o.worldNormal = UnityObjectToWorldNormal(v.normal);
				o.worldPos = mul(unity_ObjectToWorld,v.vertex).xyz;
				TRANSFER_VERTEX_TO_FRAGMENT(o);
				return o;			
			}
			
			//用UnityWorldSpaceLightDir函数和LIGHT_ATTENUATION函数自动分辨了光源的类型，并给出了正确的光源方向和衰减
			fixed4 frag(v2f i):SV_Target
			{
				fixed3 worldNormal = normalize(i.worldNormal);
				fixed3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos));

				fixed3 halfLambert = dot(worldNormal,worldLightDir) *0.5 + 0.5;
				fixed3 diffuse = _LightColor0.rgb * _Diffuse.rgb *halfLambert;

				fixed3 viewDir = normalize(UnityWorldSpaceViewDir(i.worldPos));
				fixed3 halfDir = normalize(worldLightDir + viewDir);
				fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0,dot(viewDir,halfDir)),_Gloss);

				fixed3 atten = LIGHT_ATTENUATION(i);

				return fixed4((diffuse + specular)*atten,1.0);
			}
			ENDCG
		}
    }
}
~~~

其中，UnityWorldSpaceLightDir函数和LIGHT_ATTENUATION函数可以理解为(伪代码)：

~~~ShaderLab
fixed3 UnityWorldSpaceLightDir(float3 pos)
{
#ifdef USING_DIRECTIONAL_LIGHT
	fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz); //平行光的LightPos参数就是方向本身
#else 
	fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz - pos);
#endif
	return worldLightDir;
}

fixed3 LIGHT_ATTENUATION(v2f i)
{
#ifdef USING_DIRECTIONAL_LIGHT
	fixed3 attend = fixed3(1,1,1);
#else
	float3 lightCoord = mul(_LightMatrix0, float4(i.worldPosition, 1)).xyz;
	fixed3 atten = tex2D(_LightTexture0, dot(lightCoord, lightCoord).rr);
#endif
	return atten;
}
~~~

##### 如何使用延迟渲染路径？

在使用延迟渲染时，对渲染物体进行照明的光源格式在理论上是无限的，即所有的光源偶读可以执行逐片元光照，且所有的光源都能使用cookie纹理并产生阴影。但延迟渲染也存在以下缺点：

+ 不支持真正的抗锯齿算法
+ 无法处理半透明物体的渲染
+ 对显卡的性能要求较高，需要显卡支持**多重渲染目标(Multiple Render Target, MRT)功能**、支持深度纹理和双面模板缓冲

延迟渲染中实时光源的渲染开销只与该光源能照射到的像素数有关，而与场景复杂度无关。所以照射范围很小的光源的光照计算是很高效的，并且在它们完全或部分被场景中的物体遮挡时尤其高效。当然，要产生阴影的光源比不产生阴影的光源性能消耗大得多。在延迟渲染中，投射阴影的光源仍然需要额外执行一次或多次渲染计算。

在执行延迟渲染时，需要提供至少两个渲染通路(Pass)。其中第一个渲染通路，也就是**几何通路**被用于渲染G-Buffer。在几何通路中，会把待渲染物体的漫反射颜色、镜面反射颜色、平滑度、法线、自发光颜色及深度值信息渲染到基于屏幕空间的G-Buffer中。每一个待渲染物体仅执行一次此通路。在Unity中，默认的G-Buffer包含以下几个渲染目标：

| 可渲染纹理或缓冲区 | 可渲染纹理或缓冲区的纹素格式              | 功能描述                                                    |
| ------------------ | ----------------------------------------- | ----------------------------------------------------------- |
| RT0                | ARGB32                                    | RGB分量储存了漫反射颜色值，A分量储存了遮蔽值(occlusion)     |
| RT1                | ARGB32                                    | RGB分量储存了镜面反射颜色值，A分量储存了粗糙度值(roughness) |
| RT2                | ARGB2101010                               | RGB分量储存了基于世界坐标的法线值，A分量未使用              |
| RT3                | ARGB2101010(非HDR模式)或ARGBHalf(HDR模式) | 储存了自发光值，光照贴图信息，光照探针信息                  |
| RT4                | ARGB32                                    | 如果使用了阴影蒙版和距离式阴影蒙版，则储存了光源遮蔽信息    |
| 模板缓冲区         | ——                                        | ——                                                          |
| 深度缓冲区         | ——                                        | ——                                                          |

其中，ARGB2101010是一种12位的纹素格式，其中10位用来储存颜色，2位用于储存Alpha值。

根据上面的表格，我们可以编写一个几何通路Pass，这个Pass对应的Material需要被挂载到所有场景物体上，每个物体执行一次该Pass，最终获取GBuffer。在几何通路中，我们使用了**多重渲染目标**【见可编程渲染管线】技术来一次输出到多个RenderTexture：

~~~ShaderLab
Shader "Custom/deferred"
{
    //unity参数入口 
	Properties
	{
		_MainTex("Main Texture",2D)="white"{}
		_Diffuse("Diffuse",Color) = (1,1,1,1)
		_Specular("Specular",Color) = (1,1,1,1)
		_Gloss("Gloss",Range(1,100)) = 50
	}
	SubShader
	{
        //非透明队列
		Tags { "RenderType" = "Opaque" }
		LOD 100
        //延迟渲染几何通路
		Pass
		{
            //设置 光照模式为延迟渲染
			Tags{"LightMode" = "Deferred"}
			CGPROGRAM
				#pragma target 3.0
				#pragma vertex vert
				#pragma fragment frag

				#include"UnityCG.cginc"
				#pragma multi_compile __ UNITY_HDR_ON

				uniform sampler2D _MainTex;
				uniform float4 _MainTex_ST;
				uniform float4 _Diffuse;
				uniform float4 _Specular;
				uniform float _Gloss;
				
				struct a2v
				{
					float4 pos : POSITION;
					float3 normal : NORMAL;
					float2 uv : TEXCOORD0;
				};
				
				struct v2f
				{
					float4 pos : SV_POSITION;
					float2 uv : TEXCOORD0;
					float3 worldPos : TEXCOORD1;
					float3 worldNormal : TEXCOORD2;
				};
				
				// 延迟渲染所需的输出结构。正向渲染只需要输出1个Target，而延迟渲染的片元需要输出4个Target  
				struct DeferredOutput
				{
					// RGB存储漫反射颜色，A通道存储遮罩
					float4 gBuffer0:SV_TARGET0;
					// RGB存储高光（镜面）反射颜色，A通道存储高光反射的指数部分，也就是平滑度
					float4 gBuffer1:SV_TARGET1;
					// RGB通道存储世界空间法线，A通道没用
					float4 gBuffer2:SV_TARGET2;
					// Emission + lighting + lightmaps + reflection probes (高动态光照渲染/低动态光照渲染)用于存储自发光+lightmap+反射探针深度缓冲和模板缓冲
					float4 gBuffer3:SV_TARGET3;
				};
				
				v2f vert(a2v v)
				{
					v2f o;
					o.pos = UnityObjectToClipPos(v.pos);
					o.uv = TRANSFORM_TEX(v.uv, _MainTex);
					o.worldPos = mul(unity_ObjectToWorld, v.pos).xyz;
					o.worldNormal = UnityObjectToWorldNormal(v.normal);
					return o;
				}
				
				DeferredOutput frag(v2f i)
				{
					DeferredOutput o;
					// 像素颜色 = 贴图颜色 * 漫反射颜色
					fixed3 color = tex2D(_MainTex, i.uv).rgb * _Diffuse.rgb;
					// 默认使用高光反射输出
					o.gBuffer0.rgb = color; // RGB存储漫反射颜色
					o.gBuffer0.a = 1; // 漫反射的透明度
					
					o.gBuffer1.rgb = _Specular.rgb; // RGB存储高光反射颜色，
					o.gBuffer1.a = _Gloss / 100; // 高光反射强度
					
					o.gBuffer2 = float4(i.worldNormal * 0.5 + 0.5, 1); // RGB通道存储世界空间法线，A通道没用
					
					// 如果没开启HDR，要给颜色编码转换一下数据exp2，后面在lightpass2里则是进行解码log2
					#if !defined(UNITY_HDR_ON)
						color.rgb = exp2(-color.rgb);
					#endif
					
					o.gBuffer3 = fixed4(color, 1);
					return o;
				}
			ENDCG
		}
	}
}
~~~

第二个渲染通路，也就是**光照处理通路**用于执行真正的光照计算，这个通路会使用第一个渲染通路得到的数据来计算最终的光照颜色，并输出到帧缓冲区中。

和前向渲染不同的是，这两个渲染通路并不需要写在同一个Shader文件中，因为**前向渲染的两个Pass都是针对每个渲染物体生效的，而延迟渲染的第一个Pass针对渲染物体生效，第二个Pass针对GBuffer生效**。所以在不需要自定义的情况下，引擎会使用默认的光照处理通路在几何通路之后自动对每个光源进行计算。在Unity中，光照处理通路可以在Editor->Project Settings->Graphics->Deffered中修改。这里我们尝试自己编写一个光照处理通路代码：

~~~ShaderLab
Shader "Unlit/deferredLight"
{
    SubShader
    {
        // 第一个pass用于合成灯光
        Pass
        {
            // 由于像素信息已经经过深度测试，所以可以关闭深度写入
			ZWrite Off           
			Blend [_SrcBlend] [_DstBlend]
			
            CGPROGRAM

			#pragma target 3.0
			#pragma multi_compile_lightpass 
			#pragma exclude_renderers nomrt
			#pragma multi_compile __ UNITY_HDR_ON
            
            #pragma vertex vert
            #pragma fragment frag
            
			#include "UnityCG.cginc"
			#include "UnityDeferredLibrary.cginc"
			#include "UnityGBuffer.cginc"
            
			uniform sampler2D _CameraGBufferTexture0;// 漫反射颜色
			uniform sampler2D _CameraGBufferTexture1;// 高光、平滑度
			uniform sampler2D _CameraGBufferTexture2;// 世界法线
            
			struct a2v
			{
				float4 pos : POSITION;
				float3 normal : NORMAL;
			};
            
			struct v2f
			{
				float4 pos : SV_POSITION;
				float4 uv : TEXCOORD;
				float3 ray : TEXCOORD1;
			};
            
			v2f vert(a2v v)
			{
				v2f o;
				o.pos = UnityObjectToClipPos(v.pos);
                // 获取屏幕上的顶点坐标
				o.uv = ComputeScreenPos(o.pos);
                // 模型空间转视角空间坐标
				o.ray =	UnityObjectToViewPos(v.pos) * float3(-1,-1,1);
                // 插值
				o.ray = lerp(o.ray, v.normal, _LightAsQuad);
				return o;
			}
            
            //设置片段渲染器输出结果的数据格式。如果开始hdr就使用half4,否则使用fixed4
			#ifdef UNITY_HDR_ON
			    half4
			#else
			    fixed4
			#endif
			frag(v2f i) : SV_Target
			{
				float3 worldPos;
				float2 uv;
				half3 lightDir;
				float atten;// 衰减
				float fadeDist;// 衰减距离
                //计算灯光数据，并填充光照属性数据，返回灯光的坐标，uv、方向衰减等等
				UnityDeferredCalculateLightParams(i, worldPos, uv, lightDir, atten, fadeDist);

				// 灯光颜色
				half3 lightColor = _LightColor.rgb * atten;

				half4 diffuseColor = tex2D(_CameraGBufferTexture0, uv);// 漫反射颜色
				half4 specularColor = tex2D(_CameraGBufferTexture1, uv);// 高光颜色
				float gloss = specularColor.a * 100;//平滑度
				half4 packedNormal = tex2D(_CameraGBufferTexture2, uv);// 法线
				float3 worldNormal = normalize(packedNormal.xyz * 2 - 1);
               
				fixed3 viewDir = normalize(_WorldSpaceCameraPos - worldPos);
				fixed3 halfDir = normalize(lightDir + viewDir);

				half3 diffuse = lightColor * diffuseColor.rgb * max(0,dot(worldNormal, lightDir));
				half3 specular = lightColor * specularColor.rgb * pow(max(0,dot(worldNormal, halfDir)),gloss);
				half4 color = float4(diffuse + specular,1);

                //如果开启了hdr则使用exp2处理颜色
				#ifdef UNITY_HDR_ON
				    return color;
				#else 
				    return exp2(-color);
				#endif
			}

            ENDCG
        }

		//转码pass,主要用于LDR转码
		Pass
		{
            //使用深度测试，关闭剔除
			ZTest Always
			Cull Off
			ZWrite Off
            //模板测试
			Stencil
			{
				ref[_StencilNonBackground]
				readMask[_StencilNonBackground]

				compback equal
				compfront equal
			}
			CGPROGRAM
            //输出平台
			#pragma target 3.0
			#pragma vertex vert
            #pragma fragment frag
            // 剔除渲染器
			#pragma exclude_renderers nomrt
            //
			#include "UnityCG.cginc"
            //缓冲区颜色
			sampler2D _LightBuffer;
            struct a2v
			{
				float4 pos:POSITION;
				float2 uv:TEXCOORD0;
			};
			struct v2f
			{
				float4 pos:SV_POSITION;
				float2 uv:TEXCOORD0;

			};
            //顶点渲染器
			v2f vert(a2v v)
			{
				v2f o;
                // 坐标转为裁剪空间
				o.pos = UnityObjectToClipPos(v.pos);
				o.uv = v.uv;
                // 通常用于判断D3D平台，在开启抗锯齿的时候图片采样会用到
				#ifdef  UNITY_SINGLE_PASS_STEREO
				    o.uv = TransformStereoScreenSpaceTex(o.uv,1.0);
				#endif
				return o;
			}
            //片段渲染器
			fixed4 frag(v2f i): SV_Target
			{
				return -log2(tex2D(_LightBuffer,i.uv));
			}

			ENDCG
		}
    }
}
~~~

在延迟渲染结束后，可以完全按照前向渲染的方式来实现半透明物体的渲染。

##### 如何实现一个渲染管线？



<div STYLE="page-break-after:always;"></div>

#### OpenGL

##### 如何理解OpenGL和其环境？

**OpenGL是一个针对图形开发的标准，它是由若干基于C++的图形开发库实现的**。虽然可以通过一些外部库或者小技巧实现用其它语言开发OpenGL程序，但OpenGL最常用的编程语言仍是C++(DirectX也是这样)。

与OpenGL相关的环境包括：**GL**、**Glu**、Wgl、Glaux、**Glut**、Freeglut、Glfw、Glx、Agl、Glext、Wglext、Glee、**Glew**、**Glad**等，其它常用的库还包括**SDL2**、**SDM**和**KHR**等。这些环境经常令初学者感到迷惑。

GL.h是常用的OpenGL头文件，函数名的前缀以gl开头，定义了OpenGL使用的函数和常量声明，包括了最基本的3D函数。

Glu.h是OpenGL实用库使用的函数和常量声明，包含有43个函数，函数名的前缀为glu，是对GL.h的封装，可以使用较简单的函数实现一些复杂的功能，如纹理、坐标和基本形状等。它属于OpenGL标准的一部分，而以下的各种库则不属于。

Wgl.h是OpenGL中针对Windows开发的部分，是Gl.h的扩展。

Glaux.h是OpenGL辅助库，包含有31个函数，函数名前缀为aux。它提供了创建窗口、处理键盘鼠标事件、设置调色板等OpenGL本身不提供，但在编写OpenGL程序时又经常用到的功能。目前这个库已经过时，使用它前必须使用#include <windows.h>或具有类似功能的头文件。

Glut.h是OpenGL实用工具库，大致功能与GLAUX类似，主要用于实现跨平台的窗口界面，可以在窗口上实现字体和图像。目前许多OpenGL教程使用这个库来编写演示程序。一些编译系统可能不直接提供这个库，需要单独下载安装。这个头文件自动包含了<GL/gl.h>和<GL/glu.h>。

Freeglut.h是Glut的一个开源版本。

Glfw.h和Glut类似，是轻量级的跨平台工具库，提供了渲染物体所需的最低限度的接口。主要用于管理窗口、读取输入和处理事件。

Glx.h是X窗口系统的OpenGL扩展库，专门用于实现X窗口(一种用于UNIX系统的标准图形化用户界面)系统。

Agl.h是苹果IOS系统的OpenGL扩展库。

Glext.h是扩展头文件，专门针对在VC系列编译器中的OpenGL编程，由于微软不重视OpenGL，gl.h中包含的仍是OpenGL1.1版本的定义，Glext中扩展了在OpenGL更高版本使用的常数和函数声明。

Wglext.h和Wgl.h对应，是Glext.h的扩展，包含针对windows开发的部分。

Glee.h是GLEE开源库的头文件，这是一个组织对Glext.h的优化，可以使用更好用的函数来实现OpenGL高版本的功能。Glee与Gl、Glu、Glext冲突，不能同时使用。

Glew.h是跨平台的C++扩展库，它能自动识别平台所支持的最高级的OpenGL特性，也就是说，只要包含一个glew.h头文件，就能使用gl,glu,glext,wgl,glx的全部函数，并且Glew适配几乎所有流行的操作系统和显卡。

Glad.h与Glew的功能类似，可以看作是它的升级版本。

目前主流的OpenGL开发环境可以选用Glfw+Glew，Glfw+Glad，Glut+Glad等。

Khrplatform是一个针对跨平台开发的开源组件，Glut和Glfw就是基于Khrplatform实现跨平台的识别和自适应的。

OpenAL是一个与OpenGL类似的三维音效处理库。

DEVIL是一个跨平台的纹理处理库，适配了各种常用类型的图片，并且API与OpenGL的一致性很高。

STB是一个轻量级的纹理处理库，相比DEVIL具有更简单的函数功能。

Assimp是一个读取外部网格的库，可以处理stl、obj、fbx等常见网格类型。

SDL2是一个跨平台的多媒体库，可以实现基于OpenGL的音频、视频的导入和导出，以及键鼠、游戏手柄等的输入输出等。这个库也基于KHR组件。

GLM是一个第三方数学库，它提供了和GLSL相同的命名约定和功能实现，可以极大方便OpenGL开发，并且扩展了矩阵变换、四元数、随机数等功能，在图形学和图像处理领域都有很大的作用。

OpenGL的实现逻辑是一个巨大的状态机，换句话说，OpenGL中存在大量外部变量，这些外部变量被用于描述OpenGL和GPU应当如何运行。这些外部变量组成的状态被称为**OpenGL上下文(OpenGL Context)**。我们可以将OpenGL的函数分为状态设置函数和状态应用函数，前者被用于设置上下文(相当于Set Pass Call)，而后者会根据当前上下文执行一些操作(相当于Draw Call)。

比如说，当我们想告诉OpenGL去画线段而不是三角形的时候，我们通过改变一些上下文变量来改变OpenGL状态，从而告诉OpenGL如何去绘图。一旦我们改变了OpenGL的状态为绘制线段，下一个绘制命令就会画出线段而不是三角形。

一般来说，我们会先通过状态设置函数来设置上下文并操作缓冲区，然后通过状态应用函数告知OpenGL使用当前上下文进行渲染，剩下的有关于CPU和GPU的沟通的细节将由OpenGL负责解决。

##### 什么是顶点缓冲对象VBO？

**VBO(顶点缓冲对象)**是在显卡中开辟出的一块内存缓冲区，用于储存顶点的各类属性信息，如顶点坐标，顶点法向量，顶点颜色数据等。OpenGL允许同时开辟很多个VBO，但同时只能有一个VBO被标注为顶点缓冲区。每个VBO在OpenGL中有它的唯一标识ID，这个ID对应着具体的VBO的显存地址，通过这个ID可以对特定的VBO内的数据进行存取操作。

在OpenGL中，声明、配置一个VBO的流程如下：

~~~c++
GLuint vboID;//预设一个缓冲区句柄，这个句柄将指向接下来生成的VBO
glGenBuffer(1, &vboID);//让GPU在显存中开辟出一个缓冲区，并将预设的句柄指向它，第一个参数"1"表明要生成的缓冲区数是1个。
glBindBuffer(GL_ARRAY_BUFFER, vboID);//将这个缓冲区设定为顶点缓冲，之后顶点着色器将从这里读取顶点数据
glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW);
//把储存在vertices数组中的顶点数据传输到当前绑定的显存顶点缓冲区中
/*
	第四个参数指定了我们希望显卡如何管理给定的数据。它有三种形式：
	GL_STATIC_DRAW ：数据不会或几乎不会改变。
	GL_DYNAMIC_DRAW：数据会被改变很多。
	GL_STREAM_DRAW ：数据每次绘制时都会改变。
*/
~~~

##### 什么是顶点属性指针VAP？

顶点着色器允许我们指定任何形式的顶点数据作为输入，这使得图形学编程具有更强的灵活性，但也意味着我们必须手动指定输入数据的哪一个部分对应着哪个顶点的哪种顶点属性。所以，我们必须在渲染前指定OpenGL解释顶点数据的方式。

假设我们希望自己的顶点缓冲区会被解析为下面这个样子：

<img src="Textures\VBO Parsing.png" alt="VBO Parsing.png" style="zoom: 33%;" />

每个属性是一个三维向量，向量的每个维度是长度为4字节的浮点数，两个顶点间没有空隙，数据中第一个值在缓冲开始的地方。

所以我们使用glVertexAttributePointer函数来定义**顶点属性指针(VAP)**，OpenGL会维护一个**顶点属性指针列表**，用于告知着色器在尝试获得某个属性时的寻址方式。这个列表在每次更换顶点缓冲区指向的VBO时会被清空，这要求我们每次更新需要渲染的物体都必须重新设置VAP。

~~~C++
glVertexAttributePointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(GLfloat), (GLvoid*)0);
//	定义顶点属性指针告知GPU解析当前顶点缓冲中的数据的方式。
/* 	第一个参数p1指定顶点属性的位置ID，这告诉着色器，当希望使用ID为0的属性时应该从这个缓冲区中寻找数据。
	第二个参数p2指定顶点属性向量的维度，这里我们输入的是三维向量，所以填入3。
	第三个参数p3指定向量中每个维度的数据类型，GLSL中的向量vec都是由4字节浮点数组成的。
	第四个参数p4定义是否希望数据被标准化(映射到0~1)，这里设置了否。
	第五个参数p5是步长，指定在连续的顶点属性之间的间隔，这里设置了3×4=12个字节，这告诉着色器在取完第一个向量后，12个字节之后取下一个向量。如果我们将这个属性设置为0，OpenGL会直接使用p2×p3作为步长的默认值。
	第六个参数p6表示我们的位置数据在缓冲区起始位置的偏移量，这告诉着色器第一个顶点属性位于这个缓冲区的开头。
*/
~~~

在编写顶点着色器时，可以通过这样的语法来声明输入属性的位置ID：

~~~glsl
layout (location = 0) in vec3 position;
//	layout(location = 0) 设置了这个属性的位置ID为0
//	in关键字说明这个属性是一个输入属性
//	vec3关键字说明这个属性的类型是3维向量
//	position标识符声明了属性的名字
~~~

整个设置的逻辑是，当着色器需要获得第i个顶点的属性position时，着色器先查找position属性的位置ID为0，然后在顶点属性指针列表中寻找p1=0的顶点属性指针，然后从首地址为0 + 12\*i的位置开始，取长度为3\*4的向量作为输入。

最后我们通过glEnableVertexAttribArray来启用设置好的VAP：

~~~c++
glEnableVertexAttribArray(0);//参数是顶点属性的位置ID，这里我们启用了0号属性类型
~~~

如果我们需要绑定不止一种顶点属性，可以将多种顶点属性有规律的存放到同一个顶点缓冲区中，多次使用glVertexAttributePointer函数，指定GPU对不同的属性ID使用不同的寻址方式，不同的属性组合，不同的顶点缓冲结构需要使用不同的VAP设置，如下图所示：

<img src="Textures\VAP.png" alt="VAP" style="zoom: 33%;" />

##### 什么是顶点数组对象VAO？

使用VBO和VAP来对一次渲染进行准备并不复杂，但当顶点属性数量开始增多且物体数量也开始增多，使用VBO+VAP就过于麻烦了，因此我们引入了**顶点数组对象VAO**。

VAO可以理解为将VAP封装在了同一个数据结构中，这样我们每次渲染物体时，只需要选定VAO，而不再需要对VAP进行重复设置了。除了VBO和VAP，VAO还可以封装我们即将提到的EBO。VAO的结构如图所示：

<img src="Textures\VAO.png" alt="VAO.png" style="zoom:50%;" />

OpenGL内核要求渲染必须使用VAO，如果我们没有绑定VAO，OpenGL会拒绝渲染任何东西。

创建VAO的格式和创建VBO类似：

~~~C++
GLuint vaoID;
glGenVertexArrays(1, &vaoID);
glBindVertexArrays(vaoID);
~~~

接下来我们配置VBO和VAP，配置的结果都会被储存在当前绑定的VAO上：

~~~C++
glBindBuffer(GL_ARRAY_BUFFER, vboID);
glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW);
glVertexAttribPointer(0, 3, GL_FLOAT, 3 * sizeof(GLfloat), (GLvoid*)0);
glEnableVertexAttribPointerArray(0);
~~~

在设置好一个VAO后就可以解绑，在使用前重新绑定即可：

~~~C++
glBindVertexArrays(0);
~~~

在解绑VAO后，VBO和VAP(以及接下来的EBO)的设定就被保存到了VAO中，于是在解绑VAO后就可以解绑VBO和EBO了。

##### 什么是索引缓冲对象EBO？

到目前为止的绘制方式不可避免地会遇到顶点重复的问题：举一个实例，假设我们需要绘制这样的一个六边形：

<img src="Textures\EBO重复顶点示例.png" alt="EBO重复顶点示例.png" style="zoom: 33%;" />

~~~C++
GLfloat vertices[] = 
{
	-0.25f, 0.433f, 0.0f, 	0.25f,  0.433f, 0.0f,	 0.5f,    0.0f, 0.0f,
    -0.25f, 0.433f, 0.0f,  	 0.5f,    0.0f, 0.0f,   0.25f, -0.433f, 0.0f,
    -0.25f, 0.433f, 0.0f,	0.25f, -0.433f, 0.0f,	-0.5f,    0.0f, 0.0f,
     -0.5f,   0.0f, 0.0f,	0.25f, -0.433f, 0.0f,  -0.25f, -0.433f, 0.0f,
};
~~~

这段顶点数组占用了144个字节，但实际上只有6个不同的顶点，也就是说重复属性数达到了50%。

**索引缓冲对象(Element Buffer Object, EBO, 又称Index Buffer Object, IBO)**使用顶点属性的索引而非顶点属性的值来构建三角形，这可以用于避免在储存重复的顶点属性时导致的空间浪费。我们用索引的方法来储存上面的六边形，那么情况就变成了这样：

~~~C++
GLfloat vertices[] = 
{
    -0.25f,  0.433f, 0.0f, //0
     0.25f,  0.433f, 0.0f, //1
      0.5f,    0.0f, 0.0f, //2
     0.25f, -0.433f, 0.0f, //3
    -0.25f, -0.433f, 0.0f, //4
     -0.5f,    0.0f, 0.0f, //5
};

GLuint indices[] = 
{
    0,1,2, 0,2,3, 0,3,5, 5,3,4
};
~~~

这样将两个数组加起来，只使用了120个字节，比不使用索引缓冲对象节省了1/6的空间。

进行简单的计算，我们可以验证使用索引缓冲的高效性：对重复率为α的一组n维属性，使用索引缓冲可以节省$\frac{\alpha n-1}{n}$的空间，也就是说只要满足$\alpha n\gt 1$，就有必要使用索引缓冲，而这种情况在复杂的多边形环境下基本上一定会发生。

创建并绑定EBO的语句如下：

~~~C++
GLuint eboID;
glGenBuffers(1, &eboID);
glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, eboID);
glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices, GL_STATIC_DRAW);
~~~

这些语句也可以将EBO同步绑定到VAO中，最终绑定了EBO的VAO结构如图所示：

<img src="Textures\EBO.png" alt="EBO" style="zoom: 33%;" />



##### 如何编译、链接着色器？

在OpenGL结构中，**着色器是以字符串的形式保存的**(C语言字符串)。

由于着色器需要转移到GPU当中进行运行，所以和顶点数据一样，我们也需要预先给着色器声明在显存中的空间。

~~~C++
GLuint vsID;//预设一个缓冲区句柄，这个句柄将指向接下来生成的着色器缓冲
vsID = glCreateShader(GL_VERTEX_SHADER);//让GPU在显存中开辟一个缓冲区用于存放着色器
~~~

接下来设置准备编译的着色器源码，然后实时编译它：

~~~C++
glShaderSource(vsID, 1, &vertexShader, NULL);//这个函数用于设置编译器的上下文，为后面的实时编译做准备
/*
	第一个参数是准备编译的着色器对象
	第二个参数是要编译的字符串的数量
	第三个参数是源码，以c语言中的字符串形式输入
*/
glCompileShader(vsID);//在设置好的上下文中对着色器源码进行编译
~~~

实现动态编译，就必须引入动态调试，我们用下面的代码来检测编译错误。

~~~c++
GLint success;
GLChar infoLog[512];
glGetShaderiv(vsID, GL_COMPILE_STATUS, &success);
if(!success)
{
    glGetShaderInfoLog(vsID, 512, NULL, infoLog);
    std:cout << "ERROR::SHADER::VERTEX::COMPILATION_FAILED\n" << infoLog << std::endl; 
}
~~~

同理的我们对片元着色器进行编译：

~~~C++
GLuint fsID;
fsID = glCreateShader(GL_FRAGMENT_SHADER);
glShaderSource(fsID, 1, &fragmentShader, null);
glCompileShader(fsID);
//省略检测片元着色器编译错误的语句
~~~

在设置渲染状态时，CPU需要向GPU指定一个完整的由顶点着色器和片元着色器(或更多可选着色器)组成的**着色器程序对象(Shader Program Object)**，我们需要将刚才编译好的两个着色器链接成着色器程序对象。

~~~C++
GLuint spoID;
spoID = glCreateProgram();//在显存中开辟一个区域给SPO，并用spoID指向它
glAttachShader(spoID, vsID);//将编译好的顶点着色器设置为待链接着色器
glAttachShader(spoID, fsID);//将编译好的片元着色器设置为待链接着色器
glLinkProgram(spoID);//运行链接函数
~~~

在链接时，如果顶点着色器的输出与片元着色器的输入不匹配，就会发生链接错误，与前面检测错误的代码类似：

~~~C++
glGetProgramiv(spoID, GL_LINK_STATUS, &success);
if(!success)
{
    glGetProgramInfoLog(spoID, 512, NULL, infoLog);
    std:cout << "ERROR::SHADER::SHADER_PROGRAM::LINK_FAILED\n" << infoLog << std::endl; 
}
~~~

在链接得到SPO后，我们就不再需要分离的顶点和片元着色器了，将它们删除以节约显存：

~~~C++
glDeleteShader(vsID);
glDeleteShader(fsID);
~~~

##### 如何应用着色器进行渲染？

在OpenGL中使用函数调用glUseProgram来激活SPO：

~~~C++
glUseProgram(spoID);
~~~

在glUseProgram函数调用之后，接下来每个着色器调用和渲染调用就都会使用这个程序对象了。

接下来我们需要绑定指定的VAO(VAO中储存了配置好的VBO、VAP和EBO)，然后使用绘图函数进行绘制：

~~~C++
glBindxVertexArray(vaoID);
glDrawArrays(GL_TRIANGLES, 0, x);//不使用索引缓冲，从顶点缓冲的第0位开始渲染x个顶点(x/3个三角形)
/*
	第一个参数设置了渲染方式为渲染三角形
	第二个参数设置了从顶点缓冲的第0位开始
	第三个参数设置了需要渲染的顶点数
*/
~~~

~~~C++
glBindxVertexArray(vaoID);
glDrawElements(GL_TRIANGLES, x, GL_UNSIGNED_INT, 0);//使用索引缓冲，从索引缓冲的第0位开始连续渲染x个顶点(x/3个三角形)
/*
	第一个参数设置了渲染方式为渲染三角形
	第二个参数设置了需要渲染的顶点数
	第三个参数设置了索引缓冲的类型,这里设置索引缓冲中每个数据类型为无符号整型
	第四个参数设置了从索引缓冲的第0位开始
*/
~~~

##### 如何使用OpenGL渲染一个三角型？

在下面的例子来自LearnOpenGL网站，在这个示例中，我们先创建了一个窗口，编译了简单的着色器，然后输入了顶点数组，使用VBO、VAP和VAO完整的对一个三角形进行了渲染。这个示例虽然十分简单，但能一定程度上增强对OpenGL的理解：

~~~C++
#include <iostream>

// GLEW
#define GLEW_STATIC
#include <GL/glew.h>

// GLFW
#include <GLFW/glfw3.h>


// Function prototypes
void key_callback(GLFWwindow* window, int key, int scancode, int action, int mode);

// Window dimensions
const GLuint WIDTH = 800, HEIGHT = 600;

// Shaders
const GLchar* vertexShaderSource =
"#version 330 core\n"
"layout(location = 0) in vec3 aPos; "
"layout(location = 1) in vec3 aColor; "// 颜色变量的属性位置值为 1
"out vec3 ourColor;" // 向片段着色器输出一个颜色
"void main()"
"{"
"    gl_Position = vec4(aPos, 1.0);"
"    ourColor = aColor;" // 将ourColor设置为我们从顶点数据那里得到的输入颜色
"}";

const GLchar* fragmentShaderSource = 
"#version 330 core\nout vec4 FragColor;in vec3 ourColor;void main(){FragColor = vec4(ourColor, 1.0);}";

// The MAIN function, from here we start the application and run the game loop
int main()
{
    // Init GLFW
    glfwInit();
    // Set all the required options for GLFW
    glfwWindowHint(GLFW_CONTEXT_VERSION_MAJOR, 3);
    glfwWindowHint(GLFW_CONTEXT_VERSION_MINOR, 3);
    glfwWindowHint(GLFW_OPENGL_PROFILE, GLFW_OPENGL_CORE_PROFILE);
    glfwWindowHint(GLFW_RESIZABLE, GL_FALSE);
    
    // Create a GLFWwindow object that we can use for GLFW's functions
    GLFWwindow* window = glfwCreateWindow(WIDTH, HEIGHT, "LearnOpenGL", nullptr, nullptr);
    glfwMakeContextCurrent(window);

    // Set the required callback functions
    glfwSetKeyCallback(window, key_callback);

    // Set this to true so GLEW knows to use a modern approach to retrieving function pointers and extensions
    glewExperimental = GL_TRUE;
    // Initialize GLEW to setup the OpenGL Function pointers
    glewInit();

    // Define the viewport dimensions
    int width, height;
    glfwGetFramebufferSize(window, &width, &height);
    glViewport(0, 0, width, height);


    // Build and compile our shader program
    // Vertex shader
    GLuint vertexShader = glCreateShader(GL_VERTEX_SHADER);
    glShaderSource(vertexShader, 1, &vertexShaderSource, NULL);
    glCompileShader(vertexShader);
    // Check for compile time errors
    GLint success;
    GLchar infoLog[512];
    glGetShaderiv(vertexShader, GL_COMPILE_STATUS, &success);
    if (!success)
    {
        glGetShaderInfoLog(vertexShader, 512, NULL, infoLog);
        std::cout << "ERROR::SHADER::VERTEX::COMPILATION_FAILED\n" << infoLog << std::endl;
    }
    // Fragment shader
    GLuint fragmentShader = glCreateShader(GL_FRAGMENT_SHADER);
    glShaderSource(fragmentShader, 1, &fragmentShaderSource, NULL);
    glCompileShader(fragmentShader);
    // Check for compile time errors
    glGetShaderiv(fragmentShader, GL_COMPILE_STATUS, &success);
    if (!success)
    {
        glGetShaderInfoLog(fragmentShader, 512, NULL, infoLog);
        std::cout << "ERROR::SHADER::FRAGMENT::COMPILATION_FAILED\n" << infoLog << std::endl;
    }
    // Link shaders
    GLuint shaderProgram = glCreateProgram();
    glAttachShader(shaderProgram, vertexShader);
    glAttachShader(shaderProgram, fragmentShader);
    glLinkProgram(shaderProgram);
    // Check for linking errors
    glGetProgramiv(shaderProgram, GL_LINK_STATUS, &success);
    if (!success) {
        glGetProgramInfoLog(shaderProgram, 512, NULL, infoLog);
        std::cout << "ERROR::SHADER::PROGRAM::LINKING_FAILED\n" << infoLog << std::endl;
    }
    glDeleteShader(vertexShader);
    glDeleteShader(fragmentShader);

    GLfloat vertices[] = {
        // 位置              // 颜色
         0.5f, -0.5f, 0.0f,  1.0f, 0.0f, 0.0f,   // 右下
        -0.5f, -0.5f, 0.0f,  0.0f, 1.0f, 0.0f,   // 左下
         0.5f,  0.5f, 0.0f,  1.0f, 0.0f, 1.0f,    // 顶部
         -0.5f,  0.5f, 0.0f,  0.0f, 0.0f, 1.0f    // 顶部
    };

    unsigned int indices[] = { // 注意索引从0开始! 
    0, 1, 2, // 第一个三角形
    1, 2, 3,  // 第二个三角形
    };
    
    GLuint VBO, VAO, EBO;
    glGenVertexArrays(1, &VAO);
    glGenBuffers(1, &VBO);
    glGenBuffers(1, &EBO);
    glBindVertexArray(VAO);

    glBindBuffer(GL_ARRAY_BUFFER, VBO);
    glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW);
    glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO);
    glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices, GL_STATIC_DRAW);

    // 位置属性
    glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 6 * sizeof(float), (void*)0);
    glEnableVertexAttribArray(0);
    // 颜色属性
    glVertexAttribPointer(1, 3, GL_FLOAT, GL_FALSE, 6 * sizeof(float), (void*)(3 * sizeof(float)));
    glEnableVertexAttribArray(1);

    glBindVertexArray(0); // Unbind VAO (it's always a good thing to unbind any buffer/array to prevent strange bugs)
    glBindBuffer(GL_ARRAY_BUFFER, 0); // Note that this is allowed, the call to glVertexAttribPointer registered VBO as the currently bound vertex buffer object so afterwards we can safely unbind
	glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, 0);

    // Game loop
    while (!glfwWindowShouldClose(window))
    {
        // Check if any events have been activiated (key pressed, mouse moved etc.) and call corresponding response functions
        glfwPollEvents();

        // Render
        // Clear the colorbuffer
        glClearColor(0, 1, 1, 1.0f);
        glClear(GL_COLOR_BUFFER_BIT);

        // Draw our first triangle
        glUseProgram(shaderProgram);
        glBindVertexArray(VAO);
        //glDrawArrays(GL_TRIANGLES, 0, 6);
        glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0);
        glBindVertexArray(0);

        // Swap the screen buffers
        glfwSwapBuffers(window);
    }
    // Properly de-allocate all resources once they've outlived their purpose
    
    glDeleteVertexArrays(1, &VAO);
    glDeleteBuffers(1, &VBO);
    // Terminate GLFW, clearing any resources allocated by GLFW.
    glfwTerminate();
    return 0;
}

// Is called whenever a key is pressed/released via GLFW
void key_callback(GLFWwindow* window, int key, int scancode, int action, int mode)
{
    if (key == GLFW_KEY_ESCAPE && action == GLFW_PRESS)
        glfwSetWindowShouldClose(window, GL_TRUE);
}
~~~

<div STYLE="page-break-after:always;"></div>

## 数学知识

### 数学变换

#### 坐标和坐标系

##### 什么是笛卡尔坐标系？

笛卡尔坐标系是我们一直以来接触的，也是计算机科学中最常用的坐标系类型，它有一个更通俗的名称——直角坐标系。笛卡尔坐标系也分为二维笛卡尔坐标系和三维笛卡尔坐标系，它们也可以分别被称作平面直角坐标系和空间直角坐标系。

一个二维笛卡尔坐标系由两部分组成：原点和两条相互垂直的基向量。基向量也被广泛的称为x轴和y轴，在图形学中有时也被称为u轴和v轴，在线性代数中它们被称作标准正交基。使用垂直而非其它夹角是因为直角符合人类哲学，并且能最简洁的表达位置。

三维笛卡尔坐标系也由两部分组成：原点和三条相互垂直的基向量，这三条基向量也被广泛的称为x轴、y轴和z轴。

同一空间中任何一个顶点都可以找到相对于坐标系的方向和距离。同时，坐标系也用于标志同一空间中向量的方向。没有坐标系，坐标和方向都没有意义。在图形学中，明确坐标和方向位于哪个坐标系至关重要。

##### 什么是坐标系的手性？

二维笛卡尔坐标系实际上只有两种：y轴在x轴逆时针方向90°的坐标系，和y轴在x轴顺时针方向90°的坐标系。因为任意二维笛卡尔坐标系经过二维平面内的旋转、平移和缩放后都只能归为这二者之一。

三维笛卡尔坐标系类似的，也有两种：从z轴负方向向z轴正方向看去，y轴在x轴逆时针方向90°的坐标系，以及从z轴负方向向z轴正方向看去，y轴在x轴顺时针方向90°的坐标系。这样的描述太过冗余，所以我们将前者称为左手坐标系，而右者称为右手坐标系。任意三维笛卡尔坐标系经过三维空间内的旋转、平移和缩放后也只能归为这二者之一。

在数学上，二维笛卡尔坐标系实际上只有一种，因为在数学中二维平面是不分正反的，两种坐标系只需要进行翻转就可以重合。但在计算机领域我们一般认为二维平面以面向屏幕的方向为正面，所以这两种坐标系事实上有必要进行区分。

左右手坐标系同时定义了旋转的方向。在左手坐标系中，绕某轴旋转的正方向由左手法则定义，即从该轴正方向向负方向看去的顺时针方向；而右手坐标系中，绕某轴旋转的正方向由右手法则定义，即从该轴正方向向负方向看去的拟时针方向。判断叉积结果方向时也会用到左手或右手定则。

由于在业界没有对左右手坐标系的统一规定，所以经常遇到需要从左手坐标系导出网格到右手坐标系，或需要从右手坐标系导出网格到左手坐标系的情况。接下来我们分别讨论平移、旋转和缩放矩阵在遇到手性取反时应做的操作：

对于平移矩阵，在手性取反时应将其中一个维度的平移方向取反，即本应向$(X,Y,Z)$平移的矩阵，改为向$(X,Y,-Z)$方向平移。只需确认场景中所有物体取反同一个维度即可。

对于旋转矩阵，需要对旋转矩阵的一整行取反，注意，这一行应当与平移矩阵选择的取反维度相同。比如平移矩阵选择了取反Z轴，那么在使用绕X轴旋转的和绕Y轴旋转的旋转矩阵时，应当取反第三行的所有数。

对于缩放矩阵，它不会受到手性的影响。

综上所述，**对于一个几何变换或坐标空间变换矩阵，如果需要对手性进行取反，只需要选择一个轴，将矩阵的对应一行全部取反**。

#### 几何变换

##### 什么是齐次空间和仿射变换？

变换是按照某种规则，将顶点或向量进行有针对性的修改的过程。

在图形学中，常见的变换包括了平移、缩放、旋转、错切、镜像、正交投影等。这些变换可以改变顶点的位置，也可以改变向量的位置，还可以改变坐标系的位置。

变换分为线性变换和非线性变换，其中线性变换是满足以下公式的变换：
$$
f(x)+f(y)=f(x+y)
$$

$$
kf(x)=f(kx)
$$

在上面提到的几种变换中，除了平移外，都属于线性变换。所谓线性变换，就是可以通过一个变换矩阵来对顶点的坐标进行可逆的修改，在这个过程中不丢失顶点间的相对关系，如共线和共面。

在线性代数中我们知道，n×n的矩阵是可以用于实现n维线性变换的。

由于平移不属于线性变换，所以我们没办法用3×3的矩阵来处理平移。

于是我们引入了**仿射变换(affine transform)**，通过将三维顶点坐标引入四维空间，我们就可以通过4×4的矩阵来解决平移问题了。而为了实现仿射变换而引入的四维空间，就被称为**齐次坐标空间(homogeneous space)**。

在齐次坐标空间中，坐标的第四个分量w被用于实现平移，设为常量1。而向量是不需要平移的，所以分量w被设为常量0。

在实际工作中，要先**保证顶点处于其次坐标空间下**，再对其进行变换。由于三维向量左乘四维矩阵时，第四个维度会被默认填1，这导致对三维的顶点进行变换时，会用应用向量变换的效果，而非顶点变换的效果。换而言之，会保留旋转和缩放变换而丢弃平移变换。这可能不符合开发者的意图，而导致一个异常。

##### 如何构造一个变换矩阵?

我们已经知道要同4×4的矩阵来表示平移、旋转和缩放。我们把表示纯平移、纯旋转和纯缩放的矩阵叫做基础变换矩阵。我们可以把一个基础变换矩阵分为4部分：
$$
\begin{bmatrix}
M_{3×3}&t_{3×1}\\
0_{1×3}&1
\end{bmatrix}
$$
其中，M<sub>3×3</sub>被用于旋转和缩放，t<sub>3×1</sub>被用于平移，0<sub>1×3</sub>是零矩阵，右下角是标量1。

平移矩阵定义：
$$
M_T=\begin{bmatrix}
1&0&0&t_x\\0&1&0&t_y\\0&0&1&t_z\\0&0&0&1
\end{bmatrix}
$$
其中t<sub>x</sub>、t<sub>y</sub>、t<sub>z</sub>分别表示在x、y、z轴上平移的长度。

平移矩阵作用在顶点(w分量为1)上时：
$$
\begin{bmatrix}
1&0&0&t_x\\0&1&0&t_y\\0&0&1&t_z\\0&0&0&1
\end{bmatrix}
\begin{bmatrix}
x\\y\\z\\1
\end{bmatrix}=
\begin{bmatrix}
x+t_x\\y+t_y\\z+t_z\\1
\end{bmatrix}
$$
平移矩阵作用在向量(w分量为0)上时：
$$
\begin{bmatrix}
1&0&0&t_x\\0&1&0&t_y\\0&0&1&t_z\\0&0&0&1
\end{bmatrix}
\begin{bmatrix}
x\\y\\z\\0
\end{bmatrix}=
\begin{bmatrix}
x\\y\\z\\0
\end{bmatrix}
$$
缩放矩阵定义：
$$
M_S=\begin{bmatrix}k_x&0&0&0\\0&k_y&0&0\\0&0&k_z&0\\0&0&0&1
\end{bmatrix}
$$
其中k<sub>x</sub>、k<sub>y</sub>、k<sub>z</sub>分别表示在x、y、z轴上缩放的比例。

缩放矩阵作用在顶点上时：
$$
\begin{bmatrix}k_x&0&0&0\\0&k_y&0&0\\0&0&k_z&0\\0&0&0&1
\end{bmatrix}\begin{bmatrix}x\\y\\z\\1
\end{bmatrix}=\begin{bmatrix}k_xx\\k_yy\\k_zz\\1
\end{bmatrix}
$$
绕过点(a,b,c)方向为(x,y,z)的轴旋转角度θ的旋转矩阵定义如下：
$$
M_R=\begin{bmatrix}
x^2+(y^2+z^2)cos\theta&xy(1-cos\theta)-zsin\theta&
xz(1-cos\theta)+ysin\theta&(a(y^2+z^2)-x(by+cz))(1-cos\theta)+(bz-cy)sin\theta\\
xy(1-cos\theta)+zsin\theta&y^2+(x^2+z^2)cos\theta&
yz(1-cos\theta)-xsin\theta&(b(x^2+z^2)-y(ax+cz))(1-cos\theta)+(cx-az)sin\theta\\
xz(1-cos\theta)-ysin\theta&yz(1-cos\theta)+xsin\theta&
z^2+(x^2+y^2)cos\theta&(c(x^2+y^2)-z(ax+by))(1-cos\theta)+(ay-bx)sin\theta\\
0&0&0&1
\end{bmatrix}
$$
这个矩阵很难记忆，但几乎每个图形学相关的类库中都会含有这个矩阵。推导过程略过。

作为旋转矩阵的特例，绕x、y、z轴旋转角度θ的旋转矩阵分别是：
$$
M_{Rx}=\begin{bmatrix}
1&0&0&0\\0&cos\theta&-sin\theta&0\\0&sin\theta&cos\theta&0\\0&0&0&1
\end{bmatrix}
$$

$$
M_{Ry}=\begin{bmatrix}
cos\theta&0&sin\theta&0\\0&1&0&0\\-sin\theta&0&cos\theta&0\\0&0&0&1
\end{bmatrix}
$$

$$
M_{Rz}=\begin{bmatrix}
cos\theta&-sin\theta&0&0\\sin\theta&cos\theta&0&0\\0&0&1&0\\0&0&0&1
\end{bmatrix}
$$

这三个矩阵常用来进行欧拉变换。若给定一个欧拉变换(α，β，γ)，其旋转矩阵如下：
$$
M_{Eular}(\alpha,\beta,\gamma)=M_{Ry}(\gamma)M_{Rx}(\alpha)M_{Rz}(\beta)
$$
M<sub>Eular</sub>也被称为欧拉旋转矩阵。欧拉旋转矩阵中的顺序是可以变化的，一般由平台决定，欧拉角的变换顺序也被称为**顺规**。业界常用的欧拉顺规足足有12种，这里给出的世界空间Z->世界空间X->世界空间Y的顺序是Unity的欧拉变换顺规。基于世界空间的顺规，代表在旋转过程中坐标轴是不变的；相对的，基于模型空间的顺规，代表在旋转过程中坐标轴会随之旋转。

##### 如何进行复合变换？

复合变换实际上就是一次进行多个变换。

一般来说，我们会使用一个变换矩阵表达一个物体在场景中的状态，即位置、旋转状态和缩放状态。这个变换矩阵象征着物体从坐标中心(在坐标系中与坐标轴重合，保持默认旋转和默认缩放)变换到目标状态(应被渲染的平移、旋转、缩放)的复合变换矩阵。而对物体应用变换矩阵，相当于对物体的所有顶点应用变换矩阵。我们必须保证复合变换的结果符合人的直观预期，所以必须保证复合变换的顺序。

复合变换的顺序是：**先缩放、再旋转、最后平移**，这个变换顺序是基于几何变换的应用场景的。

+ 若先进行平移再进行缩放，则缩放会计算平移后的顶点坐标，导致平移的距离随缩放的倍率变化，不复合人的直观预期。
+ 若先进行平移再进行旋转，则旋转会计算平移后的顶点坐标，导致平移的方向被改变，不符合人的直观预期。
+ 若先进行旋转再进行缩放，则缩放时使用的是旋转后的顶点坐标，导致物体不是沿模型空间的XYZ轴进行缩放，而是沿世界坐标轴方向缩放，这在大部分场合下都不符合人的直观预期。

复合变换矩阵等于子变换的连续乘积，由于我们使用的是列矩阵，所以阅读顺序是从右向左，如下：
$$
P_{new}=M_TM_RM_SP_{old}
$$

##### 如何表达旋转？

我们知道，向量可以表达方向，但不能表达旋转，因为仅通过向量我们无法确定物体绕向量方向旋转的多少。

一个向量不足以表达旋转，我们需要一对向量，用一对向量表达旋转被称为**双向量法(Double Vector)**。通过确定一个物体的上方和前方向量，我们可以唯一的确定一个物体的旋转。双向量表示法的问题有三个：

+ 使用两个三维向量，需要六个浮点数的储存空间。

+ 我们没有办法保证这两个向量一定是垂直的，在遇到非法输入时需要额外的处理程序。

+ 双向量进行插值计算不一定得到合法的结果，不适用于插值计算在图形学领域是一个极其致命的问题。

为了解决双向量法的问题，我们可以改用**欧拉角(Eular Angle)**表示法。

欧拉角是一个直观的利用三维向量表达旋转的方法，它源于刚体动能计算，后来广泛用于航空业，随后被计算机领域引入描述旋转。一种常用的欧拉角方法中的三个维度记录了绕x轴旋转x度，绕y轴旋转y度，绕z轴旋转z度，通过这三个数据我们可以得到一个欧拉变换矩阵。如果我们把物体前方朝向z轴正方向，上方朝向y轴正方向的状态作为欧拉角中的零向量，那么欧拉角就可以表达一个旋转状态。

在这里我们使用比物理专业术语更易懂的航空术语进行解释。如果我们将物体模型坐标的y轴作为物体的上方，z轴作为前方，在左手坐标系中x轴作为右方，右手坐标系中x轴作为左方，我们将欧拉角中的x分量称为**俯仰(Pitch)**，因为绕x轴旋转会使物体的面向方向朝上或下旋转；将欧拉角中的y分量称为**偏航(Yaw)**，因为绕y轴旋转会使物体的面向方向朝左或右旋转；将欧拉角中的z分量称为**桶滚(Roll)**，因为绕z轴旋转不会改变物体面向的方向，而是绕面向的方向滚动。在不同的坐标体系中，欧拉角的定义可能完全不同，比如在某些三维软件中将z轴作为物体的上方，x轴作为物体的前方。

> 在刚体物理学中，将以质心为原点，以世界坐标系的x/y/z轴为基向量的坐标系记为OXYZ，其中世界坐标系z轴朝上；将以质心为原点，以刚体上方为z轴，刚体正方为x轴，侧方为y轴的坐标系记作Oxyz。则称∠zOZ为章动角θ，称平面zOZ的垂线ON为节线，称∠XON为进动角ψ，称∠xON为自转角φ。这套欧拉角表达式可以用于描述在陀螺自转问题中陀螺的三种运动状态，计算欧拉矩阵时按照模型坐标Z->模型坐标X->世界坐标Z的顺序进行变换。这个顺规也被称为ZXZ顺规。

> 欧拉角分为静态欧拉角和动态欧拉角，静态欧拉角沿世界坐标的三个轴旋转，而动态欧拉角沿模型坐标的三个轴旋转。航空领域的欧拉角使用动态欧拉角，物理专业领域常用静态欧拉角，而在计算机领域我们既可以使用静态欧拉角也可以使用动态欧拉角。如在Unity中，就存在选项在这两种欧拉变换中切换。

欧拉角使用和三维向量相同的储存方式，它占用的空间比双向量小了一半，而且不会遇到非法输入。

但欧拉角也有三个主要的问题：

+ 欧拉角没有统一的行业标准。同一个欧拉角向量在使用不同顺规时表达完全不同的旋转。甚至有些平台会使用刚体物理中对欧拉角的定义，这使得在跨平台时不得不重新计算欧拉向量的值，其跨平台性非常差。
+ 欧拉角的三个分量在某些顺规下并不总是正交的，非正交性导致动态欧拉变换存在俗称**万向节死锁(存在某些无法表示的角度，以及存在某些具有多种表示的角度)**的问题，这使得欧拉变换的结果具有不可预见性，程序员很难准确的预测通过某个欧拉变换后物体的新朝向。
+ 欧拉角仍然不适用于插值计算，对两个欧拉角插值得到的并不是两个旋转的中间量。

于是我们引入一个新的概念，它被称为**四元数(Quaternion)**。

##### 什么是四元数？

四元数是一种超复数，它有一个实部和三个虚部：
$$
q=w+xi+yj+zk=s+\vec{v}
$$
将四元数所在的数域称为H。

四元数具有以下特性：
$$
i^2=j^2=k^2=-1
$$

$$
ij=-ji=k
$$

$$
jk=-kj=i
$$

$$
ki=-ik=j
$$

对空间几何敏感的话可以注意到，ijk的乘法运算实际上和三维空间中三个互相垂直的向量的叉乘逻辑一致。

四元数乘法性质：
$$
设q_1=s_1+\vec{v_1},q_2=s_2+\vec{v_2},则
$$

$$
\begin{split}q_1q_2
&=s_1s_2+s_1\vec{v_2}+s_2\vec{v_1}+[(x_1i+y_1j+z_1k)\cdot(x_2i+y_2j+z_2k)]\\
&=s_1s_2+s_1\vec{v_2}+s_2\vec{v_1}+[(y_1z_2-y_2z_1)i+(x_2z_1-x_1z_2)j+(x_1y_2-x_2y_1)k-x_1x_2-y_1y_2-z_1z_2]\\
&=s_1s_2+s_1\vec{v_2}+s_2\vec{v_1}+(\vec{v_1}\times\vec{v_2}-\vec{v_1}\cdot\vec{v_2})\\
&=(s_1s_2-\vec{v_1}\cdot\vec{v_2})+(s_1\vec{v_2}+s_2\vec{v_1}+\vec{v_1}\times\vec{v_2})
\end{split}
\tag{1}
$$

四元数的模：
$$
\mid\vec q\mid =\sqrt{w^2+x^2+y^2+z^2}\\\mid\vec q_1\vec q_2\mid=\mid \vec q_1\mid*\mid\vec q_2\mid
$$
四元数点积的性质：**四元数的点积等于两个旋转夹角的余弦值**。
$$
p\cdot q=s_1s_2+\vec v_1\cdot\vec v_2=\cos<p,q>
$$
共轭四元数：
$$
q = s+\vec{v},\, \overline{q} = s - \vec{v}\\
q\overline{q} = \overline{q}q=q\cdot\overline{q}=\mid q\mid^2=q^2
$$
四元数的逆：
$$
q^{-1}=\frac{\overline{q}}{q^2}\\
qq^{-1}=q^{-1}q=1\\(q^{-1})^{-1}=q
$$
旋转可以被理解为一个函数Φ：R<sup>3</sup>->R<sup>3</sup>(从R<sup>3</sup>到R<sup>3</sup>的映射)。**P**,**P<sub>1</sub>**,**P<sub>2</sub>**∈R<sup>3</sup>，Φ要想表达一个旋转，必须在旋转过程中保证向量长度、向量夹角和手性不变，即：
$$
\mid \Phi(\vec{P})\mid = \mid\vec{P}\mid\tag{2}
$$

$$
\Phi(\vec{P_1})\cdot\Phi(\vec{P_2})=\vec{P_1}\cdot \vec{P_2}\tag{3}
$$

$$
\Phi(\vec{P_1})\times \Phi(\vec{P_2})=\Phi(\vec{P_1}\times \vec{P_2})\tag{4}
$$

扩展Φ为一个H->H的映射，要求Φ(s + **V**) = s + Φ(**V**)，于是我们可以重写公式(3)：
$$
\Phi(\vec{P_1})\cdot \Phi(\vec{P_2})=\Phi(\vec{P_1}\cdot \vec{P_2})\tag{5}
$$
把P<sub>1</sub>和P<sub>2</sub>当作标量部分为0的四元数，使P<sub>1</sub>,P<sub>2</sub>,Φ(P<sub>1</sub>),Φ(P<sub>2</sub>)∈H，将s<sub>1</sub>=s<sub>2</sub>=0代入公式(1)得：
$$
\Phi(\vec{P_1})\Phi(\vec{P_2})=-\Phi(\vec{P_1})\cdot \Phi(\vec{P_2})+\Phi(\vec{P_1})\times \Phi(\vec{P_2})\\
\vec{P_1}\vec{P_2}=-\vec{P_1}\cdot \vec{P_2}+\vec{P_1}\times \vec{P_2}
$$
因此我们可以(4)和(5)合成成新的公式：
$$
\begin{split}
\Phi(\vec{P_1})\Phi(\vec{P_2})&=-\Phi(\vec{P_1})\cdot\Phi(\vec{P_2})+\Phi(\vec{P_1})\times\Phi(\vec{P_2})
\\&=-\vec{P_1}\cdot \vec{P_2}+\Phi(\vec{P_1} \times \vec{P_2})
\\&=\Phi(-\vec{P_1}\cdot \vec{P_2}+\vec{P_1}\times \vec{P_2})
\\&=\Phi(\vec{P_1}\vec{P_2})
\\\end{split}
\tag{6}
$$
我们给出一个满足(2)和(6)的函数：
$$
\Phi_\vec{q}(\vec{P})=\vec{q}\vec{P}\vec{q}^{-1},\,\,\vec{q}\in H\tag{7}
$$
先证明(7)满足(2)：
$$
\mid\Phi_\vec{q}(\vec{P})\mid=\mid \vec{q}\vec{P}\vec{q}^{-1}\mid=\mid \vec{q}\mid*\mid \vec{P}\mid*\mid \vec{q}^{-1}\mid=\mid \vec{P}\mid
\frac{\mid \vec{q}\mid  \mid \overline{\vec{q}}\mid}{\vec{q}^2}=\mid \vec{P}|
$$
再证明(7)满足(6)：
$$
\Phi_\vec{q}(\vec{P_1})\Phi_\vec{q}(\vec{P_2})=\vec{q}\vec{P_1}\vec{q}^{-1}\vec{q}\vec{P_2}\vec{q}^{-1}=\vec{q}\vec{P_1}\vec{P_2}\vec{q}^{-1}=\Phi_\vec{q}(\vec{P_1}\vec{P_2})
$$
易证明对任意a∈R(a≠0)，Φ<sub>aq</sub>=Φ<sub>q</sub>，所以不妨设q为单位四元数，则：
$$
\vec{q}^{-1} =\overline{\vec{q}}=s-\vec{v}\tag{8}
$$
在此我们补充一个定理，对于任意**P**,**Q**∈R<sup>3</sup>，有：
$$
\vec{P}\times (\vec{Q} \times \vec{P}) = \vec{P}\times \vec{Q}\times \vec{P}=\vec{P}^2\vec{Q}-(\vec{P}\cdot \vec{Q})\vec{P}\tag 9
$$
于是我们得到：
$$
\begin{split}\vec{q}\vec{P}\vec{q}^{-1}&=(s+\vec{v})\vec{P}(s-\vec{v})\\
&=(-\vec{v}\cdot \vec{P}+s\vec{P}+\vec{v}\times \vec{P})(s-\vec v)\\
&=-s\vec v\cdot \vec{P}+s^2\vec{P}+s\vec v\times \vec{P}+(\vec v\cdot \vec{P})\vec v-s\vec{P}\vec v-(\vec v\times \vec{P})\times\vec v\\
&=s^2\vec{P}+2s\vec v\times \vec{P}+(\vec v\cdot \vec{P})\vec v-\vec v\times \vec{P}\times \vec v\\
&=(s^2-\vec v^2)\vec{P}+2s\vec v\times \vec{P}+2(\vec v\cdot \vec{P})\vec v
\end{split}\tag{10}
$$
设**v**=t**A**，其中t是非0常数，**A**是旋转轴所在的单位向量，重新(10)为：
$$
\vec{q}\vec{P}\vec{q}^{-1}=(s^2-t^2)\vec{P}+2st\vec A\times \vec{P}+2t^2(\vec A\cdot \vec{P})\vec A\tag{11}
$$
我们引入向量**P**绕轴**A**旋转θ度的公式：
$$
\vec{P'} =\vec{P}cos\theta+(\vec{A}\times\vec{P})sin\theta+\vec{A}(\vec{A}\cdot\vec{P})(1-cos\theta)\tag{12}
$$
比较(11)与(12)可以得到：
$$
\left \{ \begin{array}{c}
s^2-t^2=cos\theta\\2st=sin\theta\\2t^2 =1-cos\theta
\end{array}\right.\tag{13}
$$
解方程组得：
$$
\left \{ \begin{array}{c}
t=sin\frac{\theta}{2}\\s=cos\frac{\theta}{2}
\end{array}\right.\tag{14}
$$
于是我们解得：
$$
\begin{split} \vec{q}&=s+\vec v\\
&=s+t\vec{A}\\&=cos\frac{\theta}{2}+\vec{A}sin\frac{\theta}{2}
\end{split}
$$
在此我们可以形象的理解四元数$\vec q=cos\frac{\theta}{2}+\vec{A}sin\frac{\theta}{2}$，即**绕着轴$\vec A$旋转角度$\theta$的旋转**。通常来说，使用欧拉角向量为$(0,0,0)$的旋转状态为默认状态，则在默认状态上叠加一个旋转，就可以表达一个物体的唯一旋转状态。同时，根据这一特性，我们也可以推导得知，**四元数q和它的相反数-q表达同一个旋转**。

运用四元数实现旋转比旋转矩阵更便捷，两个四元数相乘只需要16次乘加，而两个4×4的矩阵需要64次乘加。

接下来求解四元数**q**=w+xi+yj+zk对应的旋转矩阵，首先将(11)用矩阵形式表现：
$$
\vec q\vec P\vec {q}^{-1}=\begin{bmatrix}s^2-t^2&0&0\\0&s^2-t^2&0\\0&0&s^2-t^2
\end{bmatrix}\vec P+\begin{bmatrix}0&-2stA_z&2stA_y\\2stA_z&0&-2stA_x\\-2stA_y&2stA_x&0
\end{bmatrix}\vec P\\+\begin{bmatrix}2t^2A_x^2&2t^2A_xA_y&2t^2A_xA_z\\2t^2A_xA_y&2t^2A_y^2&2t^2A_yA_z\\2t^2A_xA_z&2t^2A_yA_z&2t^2A_z^2
\end{bmatrix}\vec P
$$
用x、y、z、w来重现上面的矩阵：
$$
\vec q\vec P\vec {q}^{-1}=\begin{bmatrix}w^2-x^2-y^2-z^2&0&0\\0&w^2-x^2-y^2-z^2&0\\0&0&w^2-x^2-y^2-z^2
\end{bmatrix}\vec P\\
+\begin{bmatrix}0&-2wz&2wy\\2wz&0&-2wx\\-2wy&2wx&0
\end{bmatrix}\vec P+\begin{bmatrix}2x^2&2xy&2xz\\2xy&2y^2&2yz\\2xz&2yz&2z^2
\end{bmatrix}\vec P
$$
由于**q**是单位四元数，可得：
$$
w^2-x^2-y^2-z^2=(1-x^2-y^2-z^2)-x^2-y^2-z^2=1-2x^2-2y^2-2z^2
$$
得到**q**对应的旋转矩阵R<sub>q</sub>为：
$$
R_q=\begin{bmatrix}1-2y^2-2z^2&2xy-2wz&2xz+2wy\\2xy+2wz&1-2x^2-2z^2&2yz-2wx\\
2xz-2wy&2yz+2wx&1-2x^2-2y^2
\end{bmatrix}
$$

由欧拉角转化成四元数的公式如下：
$$
q=\begin{bmatrix}w\\x\\y\\z\end{bmatrix}=
\begin{bmatrix}
\cos(\phi/2)\cos(\theta/2)\cos(\psi/2)+\sin(\phi/2)\sin(\theta/2)\sin(\psi/2)\\
\sin(\phi/2)\cos(\theta/2)\cos(\psi/2)-\cos(\phi/2)\sin(\theta/2)\sin(\psi/2)\\
\cos(\phi/2)\sin(\theta/2)\cos(\psi/2)+\sin(\phi/2)\cos(\theta/2)\sin(\psi/2)\\
\cos(\phi/2)\cos(\theta/2)\sin(\psi/2)-\sin(\phi/2)\sin(\theta/2)\cos(\psi/2)\\
\end{bmatrix}
$$
由四元数转化成欧拉角的公式如下：
$$
\begin{bmatrix}\phi\\\theta\\\psi\end{bmatrix}=\begin{bmatrix}
\arctan\frac{2(wx+yz)}{1-2(x^2+y^2)}\\\arcsin(2(wy-zx))\\\arctan(\frac{2(wz+yz)}{1-2(y^2+z^2)})
\end{bmatrix}
$$

##### 如何对旋转进行插值?

利用四元数可以对旋转进行线性插值。

四元数的插值方法主要有三种，分别记作**Lerp(线性插值)**、**SLerp(球面线性插值)**和**Squad(球状三次样条插值)**。

Lerp是对四元数套用四元矢量的线性插值方法。Lerp方法相比传统的线性插值，需要额外进行一次归一化：
$$
q_{Lerp}=Lerp(q_A,q_b,t)=\frac{(1-t)q_A+t q_B}{|(1-t)q_A+t q_B|}
$$
Lerp的问题在于，他没有考虑四元数作为四维超球面(hypershpere)的点的特性。Lerp是沿超球的弦插值的，而不是在球面上插值，这样会导致当β以恒定速度改变时，旋转动画不能以恒定角度进行。旋转在两端看起来较慢，而在动画中间就会较快。

解决此问题的方法是，引入正弦函数以在四维超球面的球上进行插值，即SLerp：
$$
q_{SLerp}=\frac{q_A\sin((1-t)\theta)+q_B\sin(t\theta)}{\sin\theta}
$$
其中$\theta$是四元数$q_A$和$q_B$的夹角，可以通过点乘来获得：
$$
\cos\theta=q_A\cdot q_B
$$

在球面插值的过程中需要注意两个特殊情况：

+ 如果$q_A\cdot q_B<0$，则有$\theta > \frac\pi2$。由于$q$和$-q$实际上表达同一旋转，所以需要考虑是否有必要对$q_A$或$q_B$取反，使其进行插值的差值最小化，避免不必要的自旋转。
+ 如果$|q_A\cdot q_B|\approx1$，则可以使用线性差值来进行优化，因为此时$\sin\theta\approx\theta$。可以在进行球面插值前先运算$q_A\cdot q_B$以判断能否使用线性插值进行优化。

**球状三次样条插值可以用于平滑关键帧动画当中的旋转**，这个插值算法通过预测四元数序列的切线方向，形成了四个控制点，并对其进行了三词样条插值。这可以避免在动画中在控制点处产生角度的突变：
$$
f_{squad}(q_i,q_{i+1},t)=f_{slerp}(f_{slerp}(q_i,q_{i+1},t),f_{slerp}(a_i,a_{i+1},t), 2t(1-t))\\
a_i=q_i\exp[-\frac{\log(q_i^{-1}q_{i-1})+\log(q_i^{-1}q_{i+1})}{4}]
$$


#### 空间变换

##### 什么是世界坐标空间？

某点或某向量基于某坐标空间，就等于某点或某向量基于某坐标系。

图形学中会存在大量不同的坐标空间，它们各自有各自的作用。在不同的处理中，我们往往需要运用不同的坐标系。虽然对计算机而言，额外的坐标系确实增加了性能消耗，但通过转换坐标空间使顶点等图形学信息能更容易的被开发者理解，减少开发者的脑力负担。

首先，图形学中一般会存在一个**世界坐标空间(World Coordinate)**或**根坐标空间(Root Coordinate)**。坐标系之间形成了一个树形结构，如果A坐标系是通过计算在B坐标系中的坐标确定位置与旋转的，那么B坐标系就是A坐标系的父坐标系，而世界坐标就是这个坐标树的根节点。

##### 什么是坐标空间变换？

顶点所在的坐标系一般被称为**模型坐标空间(Model Coordinate)**或**本地坐标空间(Local Coordinate)**。在渲染过程中，流水线部件并不能识别模型坐标，我们必须计算出顶点在指定坐标空间(也就是**NDC**)的坐标才能将顶点交付给流水线使用。为了完成这个过程，我们要先不断的求出顶点在父坐标系下的坐标，直到求出它在世界坐标空间下的坐标，然后再将它向目标坐标空间转换。为此，我们必须知道坐标是如何在父子坐标空间中互相转化的。

**空间变换与几何变换的区别在于，几何变换中物体的位置是在改变的；而空间变换中，物体顶点的位置是没有改变的，只是表示方式发生了改变。**

将问题简化，假设有父坐标P和子坐标C，我们已知子坐标C在父坐标P中的坐标P<sub>C</sub>，子坐标C在父坐标P中的旋转R<sub>C</sub>(R<sub>x</sub>,R<sub>y</sub>,R<sub>z</sub>)(欧拉角)，子坐标C在父坐标P中的缩放S<sub>C</sub>(S<sub>x</sub>,S<sub>y</sub>,S<sub>z</sub>)，试求子坐标C中一坐标A<sub>C</sub>所表示的点，转换到父坐标P中之后的坐标A<sub>P</sub>，并反过来求父坐标P中坐标B<sub>P</sub>所表示的点，转换到子坐标C中之后的坐标B<sub>C</sub>。我们知道，矩阵具有变换的功能，所以我们列出下列的式子：
$$
A_p=M_{c-p}A_c
$$

$$
B_c=M_{p-c}B_p
$$

由于父子坐标空间变换是一对反向变换，所以显然矩阵M<sub>c-p</sub>和矩阵M<sub>p-c</sub>是一对逆矩阵，这样我们只需要解出其中一个矩阵即可，现在我们来求解由子坐标向父坐标转换的变换矩阵M<sub>c-p</sub>。我们设A<sub>c</sub>=(a,b,c)。

在子坐标系C中，C的原点坐标为O(0,0,0)，C的三条基向量分别为**V<sub>cx</sub>**、**V<sub>cy</sub>**、**V<sub>cz</sub>**，则：
$$
\vec{A_C}=\vec{O}+a\vec{V_{cx}}+\vec{bV_{cy}}+\vec{cV_{cz}}
$$
将其中的所有坐标和矢量转换到父坐标P下，设在父坐标系P中，子坐标系C的三条基向量分别为**V<sub>px</sub>**、**V<sub>py</sub>**、**V<sub>pz</sub>**，则：
$$
\vec{A_P} = \vec{P_c}+a\vec{V_{px}}+b\vec{V_{py}}+c\vec{V_{pz}}
$$
将这个算式写成矩阵的格式，就会变成这样：
$$
A_P=\vec{P_c} + \begin{bmatrix}\mid&\mid&\mid\\
\vec{V_{px}}&\vec{V_{py}}&\vec{V_{pz}}\\
\mid&\mid&\mid\end{bmatrix}\begin{bmatrix}a\\b\\c\end{bmatrix}
$$
将这个算式扩展到齐次坐标空间中，就会变成：
$$
A_P=\begin{bmatrix}\mid&\mid&\mid&\mid\\
\vec{V_{px}}&\vec{V_{py}}&\vec{V_{pz}}&\vec{P_c}\\
\mid&\mid&\mid&\mid\\0&0&0&1
\end{bmatrix}\begin{bmatrix}a\\b\\c\\1\end{bmatrix}
$$
接下来我们考虑**V<sub>px</sub>**、**V<sub>py</sub>**、**V<sub>pz</sub>**的求法：我们已经知道子坐标系C在父坐标系P中的旋转R<sub>C</sub>(欧拉角)和C在P中的缩放S<sub>C</sub>，对C的基向量**V<sub>cx</sub>**在P中的表示**V<sub>px</sub>**来说，**V<sub>px</sub>**相当于父坐标系P的x轴基向量先经过缩放S<sub>x</sub>再经过旋转R<sub>C</sub>的结果，以此类推的分析子坐标系C的另外两个基向量，可以得到：
$$
A_p=\begin{bmatrix}1&0&0&\mid\\0&1&0&\vec{P_c}\\0&0&1&\mid\\0&0&0&1\end{bmatrix}M_{Eular}(R_C)M_S(S_C)\begin{bmatrix}
1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&0&1
\end{bmatrix}\begin{bmatrix}a\\b\\c\\1\end{bmatrix}
$$

$$
A_p=\begin{bmatrix}1&0&0&\mid\\0&1&0&\vec{P_c}\\0&0&1&\mid\\0&0&0&1\end{bmatrix}M_{Eular}(R_C)
\begin{bmatrix}
S_x&0&0&0\\0&S_y&0&0\\0&0&S_z&0\\0&0&0&1
\end{bmatrix}\begin{bmatrix}a\\b\\c\\1\end{bmatrix}
$$

所以：
$$
M_{c-p}=\begin{bmatrix}\mid&\mid&\mid&\mid\\
\vec{V_{px}}&\vec{V_{py}}&\vec{V_{pz}}&\vec{P_c}\\
\mid&\mid&\mid&\mid\\0&0&0&1
\end{bmatrix}=\begin{bmatrix}1&0&0&\mid\\0&1&0&\vec{P_c}\\0&0&1&\mid\\0&0&0&1\end{bmatrix}M_{Eular}(R_C)
\begin{bmatrix}
S_x&0&0&0\\0&S_y&0&0\\0&0&S_z&0\\0&0&0&1
\end{bmatrix}
$$
其中M<sub>Eular</sub>(R<sub>C</sub>)是R<sub>C</sub>的欧拉变换矩阵。

简单来说，**一个从空间B转换到空间A的空间变换矩阵，相当于将空间A的坐标系通过几何变换转移到空间B的几何变换矩阵**。

##### 什么是观察空间变换？

观察空间变换将顶点的世界坐标转换到摄像机的模型坐标空间，摄像机的模型坐标空间又被称为**观察空间(View Space)**。根据前面的公式我们已经得到了M<sub>c-p</sub>，现在我们只需要摄像机的坐标、旋转和缩放代入M<sub>c-p</sub>，即可得到从像机模型坐标到世界坐标的变换矩阵(假设摄像机没有父坐标系)，然后再求它的逆矩阵，就可以得到M<sub>p-c</sub>，即世界坐标到像机模型坐标的变换矩阵。

在线性代数中我们知道：
$$
(M_1M_2)^{-1}=M_2^{-1}M_1^{-1}
$$
所以我们得到了通常版本的M<sub>p-c</sub>：
$$
M_{p-c}=M_{c-p}^{-1}=\begin{bmatrix}\frac1{S_x}&0&0&0\\0&\frac1{S_y}&0&0\\0&0&\frac1{S_z}&0\\0&0&0&1\end{bmatrix}M_{Eular}(-R_C)\begin{bmatrix}1&0&0&\mid\\0&1&0&\vec{-P_c}\\0&0&1&\mid\\0&0&0&1\end{bmatrix}
$$
在Unity中，**观察坐标空间是右手坐标系，而世界坐标空间和其它模型坐标空间是左手坐标系**，于是我们需要对手性取反，所以在Unity中，在观察空间变化时的M<sub>p-c</sub>改为：
$$
M_{p-c}=M_{c-p}^{-1}=\begin{bmatrix}\frac1{S_x}&0&0&0\\0&\frac1{S_y}&0&0\\0&0&\frac1{S_z}&0\\0&0&0&1\end{bmatrix}M_{Eular}(-R_C)\begin{bmatrix}1&0&0&\mid\\0&1&0&\vec{-P_c}\\0&0&-1&\mid\\0&0&0&1\end{bmatrix}
$$

当前最主流的正交和透视投影算法都源于对针孔摄像机的数学建模(**针孔像机模型**)。根据小孔成像原理，投影在成像屏幕上的图像是所拍摄景物的倒像，这也是导致在渲染引擎中观察空间与其它坐标空间手性相反的历史原因：

<img src="Textures\针孔像机.png" alt="针孔像机" style="zoom:33%;" />

为了方便理解，后来的投影变换将成像平面从针孔平面的后方转移到了针孔平面的前方，这个转换后的成像平面没有任何物理意义，但在数学上不会对变换矩阵的构造产生本质影响，降低了学习成本：

<img src="Textures\逆针孔模型.png" alt="逆针孔模型" style="zoom:33%;" />

##### 什么是投影空间变换？

我们已经求得了顶点在观察空间中的坐标，接下来我们并不是直接通过三角形相似公式将顶点变换到屏幕空间，**因为透视视锥体是一个梯形，不方便裁剪，所以要先将它们转换到立方体形的齐次裁剪空间(Clip Space)中**，在经过裁剪后再通过透视除法变换到屏幕空间。位于**齐次裁剪空间(Clip Space)**内的图元会被保留，而位于裁剪空间外的图元会被剔除。

裁剪空间是由**视锥体(view frustum，又称视见体)**决定的，视锥体由六个**裁剪平面(Clip Planes)**决定。在3D渲染中最常使用的是两种视锥体：**正交投影(Orthographic Projection)**视锥体和**透视投影(Perspective Projection)**视锥体。

裁剪空间变换对所有顶点的x、y、z分量进行了缩放，并对z分量进行了平移。

正交投影视锥体本身是一个立方体，这个立方体有两个面与摄像机的法线方向垂直，我们将这两个面中靠近摄像机的称为**近裁剪平面(Near Clip Plane)**，将它到摄像机的距离记为Near，远离摄像机的称为**远裁剪平面(Far Clip Plane)**，将它到摄像机的距离记为Far，将Far-Near记为Depth。视锥体中在视线上方和下方的两个平面称为上裁剪平面和下裁剪平面，它们到摄像机法线的距离相等，记为Size。视锥体中在实现左方和右方的两个平面称为左裁剪平面和右裁剪平面，它们到摄像机法线的距离相等，将摄像机的横纵比记为Aspect，即：$Aspect=\frac{nearClipPlaneWidth}{nearClipPlaneHeight}$，所以在正交投影中有$nearClipPlaneWidth=farClipPlaneWidth=Aspect*Size$。

正交投影矩阵尝试将正交视锥体中的所有点整体映射到范围为$[-1,1]\times[-1,1]\times[-1,1]$的立方体中方便裁剪。

对正交投影视锥体的变化就相当于对立方体的平移和缩放，即先通过平移矩阵在Z轴上平移$\frac{Far+Near}2$(注意，观察空间中$z$是负数)将分布在$[-Far,-Near]$的深度映射到$[-\frac{Far+Near}2,\frac{Far+Near}2]$，然后通过缩放矩阵将视锥体的长度和宽度压缩到$[-1,1]$，将分布在深度$[-\frac{Far+Near}2,\frac{Far+Near}2]$的顶点变换到$[1,-1]$，也就是原来在$-Near$那一侧的顶点被映射到$-1$一侧，得到：
$$
M_{ortho}=\begin{bmatrix}
\frac1{Aspect*Size}&0&0&0\\0&\frac1{Size}&0&0\\0&0&-\frac2{Far-Near}&0\\0&0&0&1
\end{bmatrix}\begin{bmatrix}
1&0&0&0\\0&1&0&0\\0&0&1&\frac{Far+Near}2\\0&0&0&1
\end{bmatrix}=
\begin{bmatrix}\frac1{Aspect*Size}&0&0&0\\0&\frac1{Size}&0&0\\
0&0&-\frac2{Far-Near}&-\frac{Far+Near}{Far-Near}\\0&0&0&1\end{bmatrix}\\
$$
可见，**经过正交投影变换后的顶点事实上已经处于NDC**，这是因为正交投影不需要经过透视除法。并且，在Near平面和Far平面的翻转过程中，已经隐性的实现了手性取反。

接下来分析透视投影矩阵的构造：

透视投影视锥体是一个四棱台，将四棱台补全为四棱锥，则四棱锥的侧楞交于观察空间原点(摄像机的坐标)，视锥体上裁剪平面和下裁剪平面的夹角记为**FOV(Field of Vied，视野)**，即：
$$
nearClipPlaneHeight=2\cdot Near\cdot\tan\frac{FOV}2\\
farClipPlaneHeight=2\cdot Far\cdot\tan\frac{FOV}2
$$
摄像机的四棱台相互平行的两个面与摄像机法线方向垂直，我们将这两个面中靠近摄像机的称为**近裁剪平面(Near Clip Plane)**，它到摄像机的距离记为Near，远离摄像机的称为**远裁剪平面(Far Clip Plane)**，它到摄像机的距离记为Far，Far-Near记为Depth。将摄像机的横纵比Aspect记为Aspect。

<img src="Textures\透视视锥体.png" alt="透视视锥体" style="zoom:50%;" />



透视投影矩阵的最终目的是将透视视锥体中的顶点映射到范围为$[-1,1]\times[-1,1]\times[-1,1]$的立方体中方便裁剪。

然而，由于透视视锥体是棱台形的，在进行这样映射时需要对不同深度的矩形进行不同程度的缩放，所以我们不妨先将每个深度为$z$的矩形映射到范围为$[-z,z]\times[-z,z]$的正方形中，然后再对所有点的坐标一起除以$z$(透视除法)将它们变换到$[-1,1]\times[-1,1]$的正方形中，这样可以**避免不同点使用不同的变换矩阵，高效的利用了GPU的并行结构和常量缓冲**。

将深度为$z$的矩形映射到范围为$[-z,z]\times[-z,z]$的正方形可以用这样的矩阵实现：
$$
M_{frustum\_xy}=\begin{bmatrix}
\frac{cot\frac{FOV}{2}}{Aspect}&0&0&0\\
0&cot\frac{FOV}2&0&0\\
0&0&1&0\\0&0&0&1
\end{bmatrix}
$$
到目前为止我们只考虑了顶点的x和y分量，接下来我们讨论z分量的变换。为了将范围为$[-Far,-Near]$的$z$分量线性映射到$[1,-1]$，也就是将$-Near$那一侧的顶点被映射到$-1$一侧，我们设一个映射$f(z)=az+b$，为了解出函数中的参数不妨设二元一次方程组：
$$
\left \{ \begin{array}{c}
a*-Near+b=1\\a*-Far+b=-1
\end{array}\right.
$$
在解方程组前注意，由于前面我们决定了使用透视除法，所以需要保留顶点的原深度，而我们的$z$分量即将被映射到$[-1,1]$从而丢失，所以我们可以利用起顶点的$w$分量，在$w$分量中保存原来的$z$值。

另外，为了方便进行向量除法，我们让$z$分量也不再映射到$[-1,1]$，而是映射到$[-z,z]$这一步本身没有任何几何或物理意义，所以难以理解，但我们要做的只是稍微修正上面的方程组：
$$
\left \{ \begin{array}{c}
a*-Near+b=-Near\\a*-Far+b=Far
\end{array}\right.
$$
解得：
$$
a=-\frac{Far+Near}{Far-Near}\\b=-\frac{2*Far*Near}{Far-Near}
$$
构造一个合适的矩阵，考虑到Unity观察空间的手性，我们在w分量中储存z时顺便对其取反，注意，变换矩阵的手性取反已经隐含在了Near和Far的计算公式中，对w进行取反是因为我们选择了Z轴对变换矩阵取反，而w中恰好又需要储存Z轴的深度信息，所以跟着一起取反：
$$
M_{frustum}=\begin{bmatrix}
\frac{cot\frac{FOV}{2}}{Aspect}&0&0&0\\
0&cot\frac{FOV}2&0&0\\
0&0&a&b\\0&0&-1&0
\end{bmatrix}=
\begin{bmatrix}
\frac{cot\frac{FOV}{2}}{Aspect}&0&0&0\\
0&cot\frac{FOV}2&0&0\\
0&0&-\frac{Far+Near}{Far-Near}&-\frac{2*Near*Far}{Far-Near}\\
0&0&-1&0
\end{bmatrix}
$$
我们来检验一下这个矩阵的作用：
$$
P_{clip}=M_{frustum}P_{view}=
\begin{bmatrix}
\frac{cot\frac{FOV}{2}}{Aspect}&0&0&0\\
0&cot\frac{FOV}2&0&0\\
0&0&-\frac{Far+Near}{Far-Near}&-\frac{2*Near*Far}{Far-Near}\\
0&0&-1&0
\end{bmatrix}
\begin{bmatrix}
x\\y\\z\\1
\end{bmatrix}=
\begin{bmatrix}
x*\frac zx\\y*\frac zy\\az+b\\-z
\end{bmatrix}=
\begin{bmatrix}
x\frac{cot\frac{FOV}2}{Aspect}\\ycot\frac{FOV}2\\-z\frac{Far+Near}{Far-Near}-\frac{2*Near*Far}{Far-Near}\\-z
\end{bmatrix}
$$
这里保留w分量，一是为了保证顶点仍处于齐次空间中，可以使用线性差值进行裁剪；二是为了随后可以通过透视除法将顶点转换到齐次裁剪空间坐标转换为NDC，届时顶点将从齐次空间投影到三维空间，顶点间不再具有线性关系。

> 这里得到的两个裁剪空间坐标是基于Unity的，在其它平台上对裁剪空间坐标的定义不同，就会得到不同的投影变换矩阵。

注意，透视投影在Z方向上的压缩实际上是非线性的，我们可以实验性质的将$(0,0,-\frac{Far+Near}2,1)$左乘上面的矩阵，得到的齐次裁剪空间下的顶点是$(0,0,\frac{Far-Near}2, \frac{Far+Near}2)$，在做过透视除法后的顶点坐标变为$(0,0,\frac{Far-Near}{Far+Near})$，为一个正数，可以证明**在透视除法后顶点在靠近$Far$平面的部分分布更密集，精度也更低，在靠近$Near$平面的部分分布更稀疏，精度也更高**。

透视投影的类型随观察物体的视角变换：当物体有两个轴与摄像机法线垂直时，渲染出的是一点透视；当物体仅有一个轴与摄像机法线垂直时，渲染出的是两点透视；当物体没有轴与摄像机法线垂直时，渲染出的是三点透视。

##### 什么是屏幕空间变换？

我们需要将视锥体投影到**屏幕空间(Screen Space)**中并获得像素坐标。

根据齐次裁剪空间的性质，位于视锥体内的顶点都应满足**$-w\le x,y,z\le w，near\le w\le far$**，否则就应该进行剔除。显然，由于裁剪阶段在屏幕映射阶段之前，屏幕空间变换中作为输入的顶点都应满足上述的式子。

我们知道，在世界空间和观察空间中，一个点的坐标是$(X,Y,Z,1)$，经过透视投影变换后的齐次裁剪空间坐标变为$(X',Y',Z',-Z)$。为了方便在之后的流水线中记录片元的深度，我们用**齐次坐标系的W分量除X、Y、Z、W四个分量**，得到的坐标$(\frac{X'}Z,\frac{Y'}Z,\frac{Z'}Z,1)$被称为**归一化设备坐标(Normalized Device Coordinates，NDC)**。这一步从裁剪空间转移到NDC的运算被称为**透视除法或齐次除法**。

<img src="Textures\NDC.jpg" alt="NDC." style="zoom:50%;" />

NDC是一个X和Y坐标范围都为$[-1,1]$的立方体，在OpenGL中其Z的取值范围是$[-1,1]$，而在DirectX中其Z的取值范围是$[0,1]$。

在这之后我们还需要从NDC转化到屏幕空间：

在OpenGL中，屏幕空间是左手坐标系，其前截面z=0，后截面z=1；而齐次裁剪空间中前截面z=1，后截面z=-1。为了实现这个映射，首先获得$z'=(1-z)/2$。随后，对于xoy平面，我们要做到：

+ 平移，将齐次裁剪空间中的左下角移到屏幕空间的坐标原点。
+ 进行比例变换，将观察窗口变成屏幕分辨率大小。
+ 将视口图形反向平移到原来的位置。

在NDC中已经被归一化的Z值就是片元的深度值，将在之后的流水线中被用于深度剔除。而根据上面列出的要求，将NDC中的X和Y坐标转换为像素坐标的公式如下：
$$
screen_x=\frac{clip_x*pixelWidth}{2*clip_w}+\frac{pixelWidth}2\\
screen_y=\frac{clip_y*pixelHeight}{2*clip_w}+\frac{pixelHeight}2
$$

##### 什么是法线变换？

一般来说，点和绝大部分方向矢量都可以使用同一个4×4或3×3的变换矩阵$M_{A\rightarrow B}$把其从坐标空间A变换到坐标空间B中。但在变换法线的时候(非统一缩放时)，如果使用同一个变换矩阵，可能就无法确保法线的垂直性。

由于切线是由两个顶点之间的差值计算得到的，因此我们可以直接使用用于变换顶点的变换矩阵来变换切线，也就是$M_{A\rightarrow B}$：
$$
\vec {T_B}=M_{A->B}\vec{T_A}
$$
其中$\vec {T_A}$和$\vec {T_B}$分别表示在坐标空间A和B中的顶点切线方向。

我们知道，切线和法线应当垂直，即$\vec{T_A}\cdot\vec{N_A}=0$。给定变换矩阵$M_{A\rightarrow B}$，我们已经知道，$\vec{T_B}=M_{A\rightarrow B}\vec{T_A}$现求矩阵G来变换法线N<sub>A</sub>使其满足：$\vec{T_B}\cdot\vec{N_B}=0$
$$
\vec{T_B}\cdot\vec{N_B}=(M_{A->B}\vec{T_A})\cdot(G\vec{N_A})=0
$$
对上式进行推导后得：
$$
(M_{A->B}\vec{T_A})\cdot(G\vec{N_A})=(M_{A->B}\vec{T_A})^T(G\vec{N_A})=T_A^TM_{A->B}^TGN_A=T_A^T(M_{A->B}^TG)N_A=0
$$
由于$\vec{T_A}\cdot\vec{N_A}=\vec 0$，因此如果$M^T_{A->B}G=E$，那么上式即可成立。也就是说：
$$
G=(M_{A->B}^T)^{-1}=(M_{A->B}^{-1})^T=M_{B->A}^T
$$
**法线变换所用的矩阵是普通空间变换所用矩阵的逆转置矩阵。**

值得注意的是，如果原变换矩阵M<sub>A->B</sub>是正交矩阵，那么G=M<sub>A->B</sub>。如果变换只包含旋转，那么这个矩阵就是正交矩阵。若只包含旋转和统一缩放，统一缩放系数为k，可以得到：
$$
G=\frac1kM_{A->B}
$$



<div STYLE="page-break-after:always;"></div>

### 计算几何

#### 几何数据结构

##### 什么是包围体?

**包围盒(Bounding Box)**是一种**包围体**。包围体是一个简单的几何体，用于近似的表达一个具有复杂形状的集合体。为物体添加包围盒，可以加快物体间、物体与射线间的射线检测，在物理计算、光学计算以及剔除计算等阶段有很大的意义。

常见的包围体有**球型包围体(包围球)**和**立方体型包围体(包围盒)**。其中，包围盒又分为**轴对齐包围盒(Axis-Aligned Bouding Box, AABB)**和**有向包围盒(Oriented Bounding Box, OBB)**。它们的关系如下图所示：

<img src="Textures\包围体.jpg" alt="包围体" style="zoom: 80%;" />

+ **包围球**

包围球在几何体和相交测试中都很简单，但紧密性太差，在绝大多数场合中都会留下大量的空隙，使得预处理时间大大加长。且当物体变形后，包围球树还要重新计算。因此，包围球是一种极少用到的包围体。包围球的优势是，当对象发生旋转时，包围球不需要任何更新，所以在几何对象频繁旋转时、几何体本身形态逼近球体、或场景中物体分布相对均匀的环境中，采用包围球能得到较好的效果。

表示包围球的方式很简单，只需知道球的球心和半径。这里我们关心包围球球心的求法：

一种朴素的算法是均值法，求出几何体所有顶点的均值，即将所有顶点相加，然后除以顶点数，以此得到一个球心坐标。这个算法的问题在于，球心的坐标受到几何体不同部分顶点密度的影响，这将导致包围球的紧密性进一步降低。

另一种算法由Jack Ritter在1990年提出，首先遍历几何体的所有顶点，求出其中x坐标最大/最小点$x_{max}$/$x_{min}$，y坐标最大/最小点$y_{max}$/$y_{min}$，z坐标最大/最小点$z_{max}$/$z_{min}$，在这三对点中取出距离最大的一对，以其二点连线中点为球心$C_0$，距离的一半为初始半径$R_0$。然后再次遍历原几何体的点集，逐一测试其它点是否在生成的包围球$B_k$中。如果点$P_k$位于包围球$B_k$中，则继续测试下一个点$P_{k+1}$。如果点$P_k$不位于包围球$B_k$中，则作$C_k$与$P_k$的连线，并反向延长至与当前包围球$B_k$相交，交点为$Q_k$，以$Q_k$和$P_k$的中点为新的球心$C_{k+1}$，其距离的一半为新的半径$R_{k+1}$，构成新的包围球$B_{k+1}$。重复这个过程直至遍历完所有顶点。

显然，$B_{k+1}$的半径总比$B_k$的半径大，且这两个包围球相切与$Q_k$。这保证了位于$B_k$内的顶点一定也位于$B_{k+1}$内，确保了算法的准确性。

+ **AABB盒**

一个AABB盒是一个简单的六面体，且每一条边都平行于一个坐标平面。

AABB内的点满足$X_{min}\le x\le X_{max},Y_{min}\le y\le Y_{max},Z_{min}\le z\le Z_{max}$。因此，只需要找到顶点$P_{min}=[X_{min},Y_{min},Z_{min}]$和顶点$P_{max}=[X_{max},Y_{max},Z_{max}]$。

当添加一个顶点到包围盒时，需要先与这两个顶点进行比较，然后进行修正。

~~~C#
void add(Vector3 p) { 
   if (p.x < min.x) min.x = p.x;
   if (p.x > max.x) max.x = p.x;
   if (p.y < min.y) min.y = p.y;
   if (p.y > max.y) max.y = p.y;
   if (p.z < min.z) min.z = p.z;
   if (p.z > max.z) max.z = p.z;
}
~~~

AABB盒最大的问题在于，在处理旋转时的不方便。

显然，我们不可能每次变换都对所有顶点求出新的坐标，然后重新构建AABB。一般来说，我们只需要构建一次AABB，然后在每次变换时由旧AABB的八个顶点求出新的AABB。

一种朴素的算法是对旧AABB的八个顶点全部应用变换矩阵，得到八个新顶点，然后由这八个顶点得到新AABB的边界，这需要8次矩阵乘法(32次向量点乘)和24次比较。这个算法的问题在于，随着每次旋转，AABB盒的大小会不断的变大，并直到大到失去了其加速相交测试的作用。

一种更高级的算法不必对八个顶点全都应用矩阵乘法。为了理解这个算法我们先回顾矩阵变换中的数学运算，这里先设一个未知变换矩阵M：
$$
M=\begin{bmatrix}
M_{11}&M_{12}&M_{13}&M_{14}\\M_{21}&M_{22}&M_{23}&M_{24}\\
M_{31}&M_{32}&M_{33}&M_{34}\\0&0&0&1
\end{bmatrix}
$$
接下来我们将AABB的八个顶点用$P_i=[x_i,y_i,z_i,1]$表示，其中$x_i\in\{max.x,min.x\},y_i\in\{max.y,min.y\},z_i\in\{max.z,min.z\}$，于是在之前的算法中我们将矩阵与顶点进行乘法，得到八个点$P'_i$。我们的目标是得到$P_i'$中每个分量的最大值和最小值：
$$
MP_i=\begin{bmatrix}
M_{11}&M_{12}&M_{13}&M_{14}\\M_{21}&M_{22}&M_{23}&M_{24}\\
M_{31}&M_{32}&M_{33}&M_{34}\\0&0&0&1
\end{bmatrix}\begin{bmatrix}
x_i\\y_i\\z_i\\1
\end{bmatrix}=\begin{bmatrix}
M_{11}x_i+M_{12}y_i+M_{13}z_i+M_{14}\\
M_{21}x_i+M_{22}y_i+M_{23}z_i+M_{24}\\
M_{31}x_i+M_{32}y_i+M_{33}z_i+M_{34}\\
M_{41}x_i+M_{42}y_i+M_{43}z_i+M_{44}
\end{bmatrix}
$$


接下来我们先关注$P_i'$的X分量：可知$P_i'.x=M_{11}x_i+M_{12}y_i+M_{13}z_i+M_{14}$。如果我们想知道$P_i'.x$的最小值，由于$x_i、y_i、z_i$的取值分别只有两个，我们只需要在这八个顶点提供的八种取值中选择最小的那一种即可，而不需要真的对八个顶点的所有分量进行变换。

观察这个式子$P_i'.x=M_{11}x_i+M_{12}y_i+M_{13}z_i+M_{14}$，由于$M_{14}$是固定值，要使$P_i'.x$最小就要使前三个项分别最小。看第一个乘积$M_{11}x_i$，为了最小化这个项，必须决定令$x_i$等于$max.x$还是$min.x$。显然，如果$M_{11}>0$，则$min.x$能得到最小的乘积，如果$M_{11}<0$，则$max.x$能得到最小的乘积。

以此类推，得到最小化和最大化$P_i'.x$的每一项的方法，进而得到最小化和最大化$P'_i.x$的方法。再更进一步，也能得到最小化和最大化$P_i'.y$和$P_i'.z$的算法，以此，我们只需要进行12次比较和6次向量点乘就可以得到新的AABB盒。

+ **OBB盒**

OBB可以被理解为一个可以旋转的AABB，所以它的构造方法和AABB的构造方法没有区别。

由于OBB不再是由一个平行于各轴的立方体构成，所以OBB在表示上不再是使用一对顶点$P_{min}$和$P_{max}$，而是常用盒体中心$C$与其半边长$B$来表达。由于OBB盒与几何体共用变换矩阵，所以OBB盒的$C$和$B$一般是定义在几何体的模型空间中的。

OBB由于具有可旋转性，在几何体进行变换时不再需要对OBB进行重构，在这一点上相对于AABB具有性能和精度的双重优势。但OBB相对于AABB的问题在于更难以高效的对其进行相交测试。对OBB进行相交判定的算法详见**【计算几何-相交测试-OBB与OBB的关系】**。

##### 什么是四叉树？

四叉树是一颗包含树根的树，它的每个内部结点都有四个子结点，用于在二维空间中将一个大正方形分割成四个小正方形。树中的每个结点都对应一个正方形。如果结点v有子结点，那么它们对应的四个子正方形，可以用NE、NW、SW、SE四个象限来表示。四叉树的几何意义与逻辑意义的对应关系可以用下图表达：

<img src="Textures\四叉树.png" alt="四叉树" style="zoom: 33%;" />

+ **顶点四叉树的构造**

四叉树可以储存多种数据，例如，我们可以根据一组平面上的点，通过递归的进行方块分割，直到在任意一个正方形中只存在一个点。通过这个方法，我们可以将所有点以一颗四叉树的形式储存起来，其中每个叶子节点代表一个顶点。递归构造这样的顶点四叉树的成本，以及这棵四叉树的复杂性取决于树的深度，而树的深度与点之间的距离有关。平面中的一组点P构造的四叉树的深度，最多为$\log(\frac sc)+\frac32$。其中，$c$是P中任意两个点之间的最小距离，s是初始正方形的边长。同时，储存了一组n个点、深度为d的四叉树，具有$O((d+1)n)$个结点，并可以在$O((d+1)n)$时间内构造。

将四叉树扩展到八叉树的逻辑很简单，八叉树的内部结点有8个子结点，其几何意义为一个立方体而非正方形。

将四叉树/八叉树的每个结点命名为$v$，结点当中需要定义结点所代表的正方形$Q$，以及其它业务逻辑(如在顶点四叉树中的顶点信息)。接下来，我们用$Q(v)$表示一个结点所对应的正方形。

+ **使用四叉树进行导航算法**

使用四叉树的场景有很多，其中最常见的是**导航(Navigation)**，即给定结点$v$和东、西、南或北方向，找到一个结点$v'$，使得$Q(v)$与$Q(v')$相邻。但有时，结点在指定方向上可能具有不止一个相邻结点，如下图中，Q的西方向就具有不止一个结点可供选择：

<img src="Textures\相邻四叉树.png" alt="相邻四叉树" style="zoom: 33%;" />

为方便起见，我们在使用导航算法时，应该经可能返回与给定结点$v$相同深度的结点，如果没有这样的结点，则返回其正方形相邻的最深结点。该算法的工作原理如下：假设想要找到$v$的北方邻居，如果$v$恰好是其父节点的SE或SW方向子结点，则其北方邻居很容易找到——分别是其父节点的NE或NW方向子结点。若$v$是其父节点的NE或NW方向子结点，则递归的找到$v$的父结点$\mu$的北方邻居$\gamma$，如果$\gamma$是内部结点，则$v$的北方邻居是$\gamma$的SE或SW子结点；如果$\gamma$是叶子结点，则要寻找的北方邻居是$\gamma$本身。类似的算法也适用于八叉树。

整个导航过程可以在$O(d+1)$时间内运行完毕，大部分游戏引擎都会使用预先生成的四叉树网格来加速其自动寻路算法，因为四叉树网格能有效的将离散的难以遍历的地图网格抽象为一个可遍历的图，大幅降低A^*^算法的复杂度。

##### 什么是4-8网格？

四叉树的又一个应用场景在于大地型的渲染。如果用户从指向地平线的低视角观察一个地形，则地形的一小部分离用户十分接近，而大部分可见地形的距离十分远。出于性能考虑，地形中比较靠近的部分应该以高细节度渲染，而远处部分则应该以非常小的细节渲染，以保持较高的帧速率。

一般的想法是，在网格上构建一个四叉树，然后从上到下遍历这个四叉树以渲染他。对于每一个结点，都可以通过计算确定渲染提供的细节是否足够，以决定是直接渲染一个较浅层的结点，还是继续遍历以生成四个较深层的结点。

采用四叉树来实现地形的一个问题是，结点之间并不是完全独立的。假设在某下地形上构建了一个四叉树，假设不同深度的结点按照原样渲染，由于在细分时产生了许多的四边面，所以在不同深度地形的相交处将产生间隙。我们将这些导致间隙的顶点成为T顶点，如下图所示：

<img src="Textures\T顶点示例.png" alt="T顶点示例" style="zoom: 33%;" />

所以，在细分时，我们需要设计一个新的算法，来避免生成四边面，为此我们引入了**4-8网格**。

我们定义三角形化规则网络，代替传统四叉树用正方形进行细分的方式，保证在细分时产生的所有面都是三角面，其递归描述如下：从一个正方形细分为两个直角三角形开始，对于每个递归步骤，可以细分任意直角三角形中最长的边，由直角顶点向这条最长边做垂线，可以将直角三角形细分成两个更小的直角三角形。这一方案也被称为**最长边二分法**，这样生成的网格之所以被称为为**4-8网格**，是因为其中所有顶点(除了边框顶点)不是4度点就是8度点，如图所示：

<img src="Textures\4-8网格生成.png" alt="4-8网格生成" style="zoom: 33%;" />

我们将在细分过程中的直角顶点称为父结点，将细分时新生成的结点称为子结点。显然，由于每一条斜边的两侧各有一个直角三角形，当细分层数足够深时，任意内部结点应该具有两个父结点。

为了既保证对尽可能少的顶点进行细分，又避免产生任何一个四边面，我们必须遵守以下的细分规则：

如果需要生成某个顶点，则必须还生成其的所有祖先结点(注意，一个顶点有两个父节点)。换句话说，当我们将一个直角三角形分成两半时，必须将对位的另一个直角三角形同时分成两半，这个规则也被称为**对位规则**。

下图中我们对一个4-8网格的细分过程进行了展示，其中的点表示该次迭代试图生成的顶点，实线为直接细分连成的线段，而虚线代表在细分时为了满足对位规则，递归的生成的其它线段：

<img src="Textures\4-8网格迭代.png" alt="4-8网格迭代" style="zoom: 25%;" />

观察细分的最终形态，我们可以将4-8网格拆分成正和斜两个四叉树，如下图所示：

<img src="Textures\4-8网格的拆分.png" alt="4-8网格的拆分" style="zoom: 25%;" />

这种给4-8网格带来了独特的性质，即一个4-8网格不仅可以被理解为两个四叉树的组合，还可以被理解为不同层级的点阵的叠加。如下图所示。利用这个性质，我们可以将不同层级的顶点数据紧密地储存在几个矩阵当中，这种储存方式的读取效率比简单的使用树形结构快十倍左右：

<img src="Textures\4-8网格的二重特性.png" alt="4-8网格的二重特性" style="zoom: 25%;" />

请注意，在矩阵式的储存时，斜四叉树包含网格之外的结点，这些结点又被称为幽灵结点。我们可以将部分正四叉树的结点数据储存在这些幽灵结点中，这可以将最终线性阵列中未使用元素的数量减少到33%。

##### 什么是八叉树？

+ **八叉树的构造**

在场景中，我们要处理比顶点复杂的多的对象。为了更好的逼近场景的几何形状，相比于四叉树，我们需要修正八叉树构造过程中的叶结点规则：不断细分，直到单元中没有对象，或达到最大深度时停止。

+ ##### 沿射线用线参数法遍历八叉树

与四叉树不同的是，八叉树通常具有两种遍历方式：自底向上的方法和自顶向下的方法。

自底向上方法从包含射线原点的八叉树叶子开始，它将从该位置开始试图找到被射线穿刺的相邻单元，这一方法与四叉树的导航类似。

自顶向下方法从八叉树的根开始，通过射线参数来决定需要向下递归到哪些被刺入的结点和叶子。接下来我们详解这个算法：

首先，由于射线穿过结点的方式实际上是关于坐标轴对称的，所以我们只需要考虑其中的一种情况，即方向向量$d_x>0,d_y>0,d_z>0$的情况，然后通过对称来得出在其它情况下的递归方案。

如图所示，下图中我们展示了八叉树中的一个x-y平面的投影图，在这个投影面中我们关注射线与x-z平面和y-z平面的交点：

<img src="Textures\递归线区间.png" alt="递归线区间" style="zoom: 67%;" />

在上图中，x-z边界限制了结点的y值范围，y-z边界限制了结点的x值范围。将结点Q的上下两个x-z边界的y值代入射线方程，可以很容易得到射线与结点Q的交点，将y值较小的交点命名为$P_y^l$，较大的命名为$P_y^h$。类似的，我们得到了射线与结点Q在y-z边界的交点$P_x^l$和$P_x^h$。

假设**射线由$\vec x(t) =\vec p+t\vec d$定义**，显然，通过交点$P_x^l,P_x^h,p_y^l,P_y^h$得到它们在射线参数方程中的参数值$t_x^l,t_x^h,t_y^l,t_y^h$，我们将这些参数值命名为**线参数**。显然，当且仅当$\max\{t_i^l\}<\min\{t_j^h\}$时，射线与该结点相交。

通过中点公式，我们可以得到射线与Q的子结点的交点$P_x^m=\frac12(P_x^l+P_x^h),P_y^m=\frac12(P_y^l+P_y^h)$。经过显而易见的转换，我们可以可以得到这些交点的线参数形式：$t_x^m=\frac12(t_x^l+t_x^h),t_y^m=\frac12(t_y^l+t_y^h)$。在得到子结点边界的线参数后，我们就可以递归的搜索射线经过了哪些子结点，并计算得出其交点了。

扩展到三维中，在八叉树当中我们可以将结点Q的边框分为x-y平面、y-z平面和x-z平面，他们分别限制了结点的z、x和y值范围。将它们带入射线方程，容易得到射线与结点Q的边框的三对交点，并计算出其线参数。将它们命名为$t_k^l,(k\in \{x,y,z\})$和$t_k^h,(k\in \{x,y,z\})$。于是，我们可以得到射线与Q的子结点的交点线参数$t_k^m=\frac12(t_k^l+t_k^h),(k\in\{x,y,z\})$。

在得知射线经过哪些子结点后，我们需要确定遍历子结点的顺序。显然，我们应该按照射线穿过子结点的顺序来遍历子结点。首先，我们需要通过查表确定光线在哪一侧入射当前单元：

| $\max\{t_i^l\},i\in \{x,y,z\}$ | 入射侧 |
| :----------------------------: | :----: |
|            $t_x^l$             |   YZ   |
|            $t_y^l$             |   XZ   |
|            $t_z^l$             |   XY   |

接下来，我们需要给结点的八个子结点编号：

<img src="Textures\八叉树结点编号.png" alt="八叉树结点编号" style="zoom:33%;" />

采用这种编号的原因是，当射线的三个分量并不都大于零时，只需要将对应位取反，就可以得到镜像的新编号方案。

接下来，我们在已知入射面的基础上，通过查表确定满足指定条件的子单元的编号，这个子单元就是射线将访问的第一个子单元：

|  侧  |               条件               | 索引位 |
| :--: | :------------------------------: | :----: |
|  XY  | $t_z^m\ge t_x^l$  &&  $t_y^m\ge t_x^l$ |   0   |
|      | $t_z^m\lt t_x^l$  &&  $t_y^m\ge t_x^l$ |   1   |
|      | $t_z^m\ge t_x^l$  &&  $t_y^m\lt t_x^l$ |   2   |
|      | $t_z^m\lt t_x^l$  &&  $t_y^m\lt t_x^l$ |   3   |
|  XZ  | $t_x^m\ge t_y^l$  &&  $t_z^m\ge t_y^l$ |   0   |
|      | $t_x^m\lt t_y^l$  &&  $t_z^m\ge t_y^l$ |   1   |
|      | $t_x^m\ge t_y^l$  &&  $t_z^m\lt t_y^l$ |   4   |
|      | $t_x^m\lt t_y^l$  &&  $t_z^m\lt t_y^l$ |   5   |
|  YZ  | $t_y^m\ge t_x^l$  &&  $t_z^m\ge t_x^l$ |   0   |
|      | $t_y^m\le t_x^l$  &&  $t_z^m\ge t_x^l$ |   2   |
|      | $t_y^m\ge t_x^l$  &&  $t_z^m\le t_x^l$ |   4   |
|      | $t_y^m\le t_x^l$  &&  $t_z^m\le t_x^l$ |   6   |

由于我们可以递归的确定更细一级的入射单元的遍历顺序，所以我们不再需要考虑更低细节单元的情况。接下来我们需要考虑的是射线从单元出射后进入的下一个单元编号。与入射侧的确定方法类似，我们可以通过查表快速判断出射侧：

| $\min\{t_i^h\},i\in \{x,y,z\}$ | 出射侧 |
| :----------------------------: | :----: |
|            $t_x^h$             |   YZ   |
|            $t_y^h$             |   XZ   |
|            $t_z^h$             |   XY   |

从一个单元的视角，如果射线穿出它的子单元的出射侧确定，那么就可以通过查表找到射线穿入的下一个子单元的编号，其中ex表示穿出了当前单元，需要回退到父结点继续遍历：

| 穿出的子单元 | 穿出子单元的YZ侧 | 穿出子单元的XZ侧 | 穿出子单元的XY侧 |
| :----------: | :--------------: | :--------------: | :--------------: |
|      0       |        4         |        2         |        1         |
|      1       |        5         |        3         |        ex        |
|      2       |        6         |        ex        |        3         |
|      3       |        7         |        ex        |        ex        |
|      4       |        ex        |        6         |        5         |
|      5       |        ex        |        7         |        ex        |
|      6       |        ex        |        ex        |        7         |
|      7       |        ex        |        ex        |        ex        |

##### 什么是线段树?

KD树是用于高效回答多维**穿刺查询(Stabbing Query)**和多维**截窗查询(Windowing)**问题的数据结构。对于线段集S，穿刺查询将报告由单个查询先$l$穿刺的所有线段。对于点集S，截窗查询将报告查询框B内的属于S的所有点。

我们从一些简单的查询和数据结构开始，逐步推进到KD树。所以，我们考虑一维穿刺查询问题，并引入**线段树**来解答这个问题：

> 输入：在一维空间中的闭合区间集S。
>
> 查询：单个值$x_q\in R$。
>
> 输出：所有$I\in S$，其中$x_q\in I$。

例如，在下图中存在7个区间$s_1.s_2,……,s_7$和一个查询值$x_q$。这个穿刺查询应该返回线段$s_2、s_5$和$s_6$。可以假设线段$s_i$分别由其左端点$l_i$和右端点$r_i$的坐标x表示。

<img src="Textures\一维穿刺查询.png" alt="一维穿刺查询" style="zoom: 50%;" />

接下来我们对这些n条线段端点的x坐标，即$\{l_i,r_i\}$，进行升序排序，将结果记作$E=\{e_1,e_2,...e_{2n}\}$，使得对任意$e_i,e_j\in E$，$i<j$，有$e_i<e_j$。我们将E分成2n+1个原子区间：
$$
[-\infin,e_1],[e_1,e_2],...,[e_{1n-1},e_{2n}],[e_{2n},\infin]
$$
通过分割区间，我们可以将空间存入一个二叉树当中，这棵二叉树被称为线段树(Sagment Tree)，线段树是一种特殊的区间树(Interval Tree)。如下图所示：

<img src="Textures\线段树.png" alt="线段树" style="zoom:50%;" />

这颗线段树的构造算法大致可以这样表达：

输入由给定n个段点的x值组成的有序序列表S

+ 为树的根分配一根带有两个子结点的节点v
+ 选择S的元素$x_m$，其中$m=\lceil\frac n2\rceil$，在v节点中插入分支键$x_m$，在v处插入区间$I=[x_1,x_n]$
+ 递归的计算$x_1,...x_m$的一维搜索树$v_l$，并计算$x_{m+1},...x_n$的一维搜索树$v_r$
+ 将$v_l$作为$v$的左子结点，$v_r$作为$v$的右子结点，并返回$v$

在上述的算法中，查找树的每个节点都可表示一段区间$v.I$。为了快速的对穿刺进行响应，我们还需要将相关线段也存入对应的节点中。对于一条线段$s$，它可以由一组连续的**基本区间**表示，为了优化储存结构，在将线段存入相应节点的过程中，我们要尽可能将信息储存在接近根节点的节点。于是对于每个线段$s=(x_i,x_j)$和节点$v$，当且仅当$v.I\in s$且$v.father\notin s$，将s存入v的线段集$v.L$中。

在一个线段树中存入线段的算法大致可以这样表达：

输入给定线段树的根节点v和新插入的线段s

+ 如果区间$v.I$已经是s的子集，则将s插入v的分线段集$v.L$中并停止
+ 如果区间$v.left.I$与s部分重合，则递归的对$v.left$进行插入
+ 如果区间$v.right.I$与s部分重合，则递归的对$v.right$进行插入

接下来我们尝试将这个算法扩展到多维空间中，构建一棵多层线段树。

> 输入：n维空间中的AABB盒集S
>
> 查询：空间中的点$P_q\in R^n$
>
> 输出：所有$I \in S$，其中$P_q\in I$

一个D维的AABB盒$B_i$可以由一组共D个轴平行线段定义，即$B_i:{S_{i1}, S_{i2},...,S_(id)}$，每个线段的两个端点记录了AABB盒在该轴方向上的边界。

我们使用递归的方法构造一个D维的多层线段树：



##### 什么是KD树?



##### 什么是BSP树？



##### 什么是BVH树？



#### 相交测试

##### 如何判断线段与线段的关系？

这里我们关系在二维空间中两个线段或两组线段的相交情况。显然，线段与线段存在三种几何关系——重合、相交、不相交。

首先，我们对两个线段进行**快速排斥实验**，对不可能相交或重合的线段进行高速的排查，这将极大的提升在批量判断线段相交时的效率：

<img src="Textures\线段碰撞检测.png" alt="线段碰撞检测.png" style="zoom: 50%;" />

设以线段$P_1P_2$为对角线的矩形为$R$，以线段$Q_1Q_2$为对角线的矩形为T，若R和T不相交，则两线段必然不相交，即：

~~~C++
if(max(p1.x, p2.x) <= min(q1.x, q2.x) ||
   max(q1.x, q2.x) <= min(p1.x, p2.x) || 
   max(p1.y, p2.y) <= min(q1.y, q2.y) ||
   max(q1.y, q2.y) <= min(p1.y, p2.y)) return false;
~~~

通过快速排斥实验后，我们要对线段进行**跨立实验**。即，如果两线段相交，则两直线必然互相跨立，也就是说矢量$\vec {Q_1P_1}$和矢量$\vec{Q_1P_2}$必然位于矢量$\vec{Q_1Q_2}$的两侧，同时，矢量$\vec {P_1Q_1}$和矢量$\vec{P_1Q_2}$必然位于矢量$\vec{P_1P_2}$的两侧。使用数学语言表达，即：
$$
(\vec{Q_1P_1} \times\vec{Q_1Q_2})*(\vec{Q_1P_2}\times\vec{Q_1Q_2}) <0 \\
(\vec{P_1Q_1} \times\vec{P_1P_2})*(\vec{P_1Q_2}\times\vec{P_1P_2}) <0 
$$
如果两个条件都通过，则我们可以判断两条线段有交点。接下来我们继续用数学方法求线段交点：

设交点为$(x_0,y_0)$，$P_1(x_1,y_1),P_2(x_2,y_2),Q_1(x_3,y_3),Q_2(x_4,y_4)$，则下列方程组成立：
$$
\left \{ 
\begin{array}{c}
x_0-x_1=k_1(x_2-x_1)\\
y_0-y_1=k_1(y_2-y_1)\\
x_0-x_3=k_2(x_4-x_3)\\
x_0-y_3=k_2(y_4-y_3)\\
\end{array}
\right.
$$

联立方程组消去参数$k_1、k_2$，得到：
$$
\begin{equation}\begin{split}
&\left \{ 
\begin{array}{c}
&k_1=\frac{y_0-y_1}{y_2-y_1}\\
&k_2=\frac{y_0-y_3}{y_4-y_3}\\\end{array}\right.
\\\Rightarrow &\left \{ \begin{array}{c}
&x_0(y_2-y_1)-x_1(y_2-y_1)=y_0(x_2-x_1)-y_1(x_2-x_1)\\
&x_0(y_4-y_3)-x_3(y_4-y_3)=y_0(x_4-x_3)-y_3(x_4-x_3)\\\end{array}\right.\\
设&\left \{ \begin{array}{c}
&b_1=(y_2-y_1)x_1-(x_2-x_1)y_1\\
&b_2=(y_4-y_3)x_3-(x_4-x_3)y_3\\\end{array}\right.\\
\Rightarrow&\left \{ \begin{array}{c}
&b_1=(y_2-y_1)x_0-(x_2-x_1)y_0\\
&b_2=(y_4-y_3)x_0-(x_4-x_3)y_0\end{array}\right.\\
\end{split}\end{equation}
$$
先尝试消去$y_0$：
$$
\begin{equation}\begin{split}
&\left \{ \begin{array}{c}
&(x_4-x_3)b_1=(y_2-y_1)(x_4-x_3)x_0-(x_2-x_1)(x_4-x_3)y_0\\
&(x_2-x_1)b_2=(y_4-y_3)(x_2-x_1)x_0-(x_2-x_1)(x_4-x_3)y_0
\end{array}\right.\\
\Rightarrow&x_0=\frac{(x_4-x_3)b_1-(x_2-x_1)b_2}{(y_2-y_1)(x_4-x_3)-(y_4-y_3)(x_2-x_1)}\\
设&D=(y_2-y_1)(x_4-x_3)-(y_4-y_3)(x_2-x_1)\\
&D_1=(x_4-x_3)b_1-(x_2-x_1)b_2\\
得&x_0=\frac{D_1}D\\
同理有&D_2=(y_4-y_3)b_1-(y_2-y_1)b_2\\
得&y_0=\frac{D_2}D
\end{split}\end{equation}
$$

##### 如何判断一组线段的交点(Bentley-Ottmann算法)？

前面介绍的朴素交点算法具有$O(n^2)$的复杂度，因为在求一组线段的交点时，两两匹配运算的效率很低。接下来我们介绍一个效率高得多的Bentley-Ottmann算法，以及它的一种变体。

BO算法要解决的问题是，给定任意一组线段的所有端点坐标，输出这组线段中存在的所有交点。BO算法的核心思路是，按照端点X轴坐标升序对端点进行排序，然后按照端点从左到右的顺序扫描这些线段。对于任意一个线段，如果它的左端点和右端点间还排入了其它线段，那么这两条线段就可能存在交点，这筛除了大量不可能通过快速排斥实验的线段对。当出现可能存在交点的线段对后，再对这两个线段使用朴素交点算法求交点，具体流程如下：

+ **初始化事件队列和扫描队列**

  按照顶点X轴坐标升序，将所有顶点存入**事件队列(priority queue)**，事件队列中储存的顶点包括线段左端点、右端点和稍后将求出来的线段交点，它们将始终按X轴坐标升序排列。在初始化完成后，不断从事件队列中取出最左边的顶点进行遍历，扫描线将沿着事件队列从左往右移动。

  然后初始化**扫描线链表**为空，扫描线链表中将储存与扫描线相交的所有线段，如果严格按照下面的算法运行，那么扫描线链表的顺序将始终按各线段与扫描线的交点Y轴坐标升序排列。

+ **若取出的顶点是线段左端点**

  将该顶点所属的线段的插入扫描线链表中，然后按照顶点Y轴坐标升序对扫描队列中所有元素重新排序(插入排序)。然后分别计算此条线段与其在扫描队列中上下相邻线段的相交情况。若有交点，则将交点按照X轴坐标升序插入到事件队列中。如果是垂直于X轴的线段，默认其下端点为“左”端点。

  <img src="Textures\左端BO.png" alt="左端BO.png" style="zoom: 67%;" />

+ **若取出的顶点是线段右端点**

  将此顶点所属的线段从扫描线链表中删除，然后计算此时与此条线段上下相邻的两条线段互相相交的情况，如果有交点，但在事件队列中还不存在，则将交点按照X轴坐标升序插入到事件队列中。

  <img src="Textures\右端BO.png" alt="右端BO" style="zoom: 67%;" />

+ **若取出的顶点为交点**

  将交点增加到最后要输出的结果集合中，然后将这个交点从属的两条线段在扫描线链表中的位置交换，这一操作可以从几何上看作通过顶点后，两条线段的上下位置关系交换。然后让它们与新的紧邻线段求交点，如果有交点，并在事件队列中还不存在，则将交点按照X轴坐标升序插入到事件队列中。如果有交点，而在事件队列中已经存在，说明出现了多条线段相交于同一点的情况，这时我们不能直接对线段在扫描线链表中的位置进行两两互换，而是要对这几条相交于同一点的线段进行反序。

  <img src="Textures\交点BO.png" alt="交点BO" style="zoom: 67%;" />

##### 如何判断射线与三角形的关系(Moller-Trumbore算法)？

射线与三角形是计算几何中碰撞关系的基础。我们给出射线的参数方差$P=O+Dt$，也就是给定起点坐标和射线方向向量，并给出三角形的三个顶点坐标$V_0、V_1、V_2$。

我们也用参数方程来表示一个三角形：$(1-u-v)V_0+uV_1+vV_2，u\ge0,v\ge0,u+v\le1$，其几何含义如下图所示。

<img src="Textures\三角形方程.png" alt="三角形方程.png" style="zoom: 33%;" />

一种朴素的解法是，先判断射线是否与平面相交，求出射线与平面的交点，然后判断点是否在三角形里：

现在我们需要得到平面的参数方程。平面由法向量和平面上的一个点定义，即平面上的点$P_0$和平面法向量$\vec n$，有参数方程$(P-P_0)\cdot \vec n=0$。则我们可以根据三角形的参数得到平面的定义：
$$
\vec n=(V_1-V_0)\times(V_2-V_0),P_0=V_1
$$
将两个参数方程联立，得：
$$
\left\{\begin{array}{c}
P=O+Dt\\\vec n\cdot(P-P_0)=0
\end{array}\right.
$$
整理得到$t=\frac{\vec n\cdot O-\vec n\cdot P_0}{\vec n\cdot \vec D}$，若$t\ge0$，则射线与平面相交，且交点为$P_0+Dt$，若$t\le0$则不相交。

将得到的交点P与三角形的参数方程联立，有：
$$
\left\{\begin{array}{c}
P=P_0+Dt\\P=(1-u-v)V_0+uV_1+vV_2
\end{array}\right.
$$
我们设$V_1-V_0=\vec E_1, V_2-V_0=\vec E_2, P-V_0=\vec E_0$便于计算，则：
$$
u = \frac{(\vec E_1\cdot\vec E_1)(\vec E_2\cdot\vec E_0)-(\vec E_1\cdot\vec E_0)(\vec E_2\cdot\vec E_1)}{(\vec E_0\cdot\vec E_0)(\vec E_1\cdot\vec E_1) - (\vec E_0\cdot\vec E_1)(\vec E_1\cdot\vec E_0)}
\\
v = \frac{(\vec E_0\cdot\vec E_0)(\vec E_2\cdot\vec E_1)-(\vec E_0\cdot\vec E_1)(\vec E_2\cdot\vec E_0)}{(\vec E_0\cdot\vec E_0)(\vec E_1\cdot\vec E_1) - (\vec E_0\cdot\vec E_1)(\vec E_1\cdot\vec E_0)}
$$
若满足$u\ge0,v\ge0,u+v\le1$，则点位于三角形内。

上述算法效率不高，接下来我们介绍Moller-Trumbore算法用于计算射线与三角形的交点，直接将射线与三角形的参数方程联立：
$$
O+Dt=(1-u-v)V_0+uV_1+vV_2\\
O-V_0=-Dt+(V_1-V_0)u+(V_2-V_0)v
$$
其中$t,u,v$是未知数，将它们提取出来后可以得到线性方程组：
$$
\begin{bmatrix}|&|&|\\-D&V_1-V0&V_2-V_0\\|&|&|\end{bmatrix}
\begin{bmatrix}t\\u\\v\end{bmatrix}=\begin{bmatrix}|\\O-V_0\\|\end{bmatrix}
$$
为了表达方便我们记$E_1=V_1-V_0,E_2=V_2-V_0,T=O-V_0$，将上式改写为：
$$
\begin{bmatrix}|&|&|\\-D&E_1&E_2\\|&|&|\end{bmatrix}
\begin{bmatrix}t\\u\\v\end{bmatrix}=\begin{bmatrix}|\\T\\|\end{bmatrix}
$$
根据克莱姆法则，可得到$t,u,v$的解为：
$$
t=\frac1{\begin{vmatrix}-D&E_1&E_2\end{vmatrix}}\begin{vmatrix}T&E_1&E_2\end{vmatrix}\\
u=\frac1{\begin{vmatrix}-D&E_1&E_2\end{vmatrix}}\begin{vmatrix}-D&T&E_2\end{vmatrix}\\
v=\frac1{\begin{vmatrix}-D&E_1&E_2\end{vmatrix}}\begin{vmatrix}-D&E_1&T\end{vmatrix}\\
$$

根据混合积公式$\begin{vmatrix}a&b&c\end{vmatrix}=a\times b\cdot c=-a\times c\cdot b$，将上式改写为：
$$
t = \frac{T\times E_1\cdot E_2}{D\times E_2\cdot E_1}\\
u = \frac{D\times E_2\cdot T}{D\times E_2\cdot E_1}\\
v = \frac{T\times E_1\cdot D}{D\times E_2\cdot E_1}
$$
令$P=D\times E_2$，$Q=T\times E_1$，最终得到：
$$
t=\frac{Q\cdot E_2}{P\cdot E_1}\\
u=\frac{P\cdot T}{P\cdot E_1}\\
v=\frac{Q\cdot D}{P\cdot E_1}
$$
最终判断，若满足$u\ge0,v\ge0,u+v\le1$，则点位于三角形内，此时可根据t、u、v求出交点坐标。

##### 如何判断三角形与三角形的关系?

本节我们要处理的问题是，给定两个三角形，判断它们是否相交。我们先对问题进行简化：设两个三角形为$T_1、T_2$，$T_1$的3个顶点分别是$A_1、B_1$和$C_1$，所在平面为$\pi_1$，其法向量为$\vec n_1$；同理的，$T_2$的3个顶点分别是$A_2、B_2$和$C_2$，所在平面为$\pi_2$，其法向量为$\vec n_2$。我们将$T_2$与$\pi_1$或$T_1$与$\pi_2$的相交状态分为以下3种情况：

+ $T_2$不与$\pi_1$相交或$T_1$不与$\pi_2$相交，则$T_1、T_2$显然不相交。

  判断$T_2$的三个顶点是否在平面$\pi_1$同侧，建立表达式：
  $$
  \left\{\begin{array}{c}
  t_1=\vec{A_1A_2}\cdot\vec n_1\\
  t_2=\vec{A_1B_2}\cdot\vec n_1\\
  t_3=\vec{A_1C_2}\cdot\vec n_1\\
  \end{array}\right.
  $$
  若$t_1、t_2、t_3$同号且均不为0，则$T_2$的三个顶点均在平面$\pi_1$同侧，则$T_1、T_2$不相交。

  类似的，再判断$T_1$与$\pi_2$的关系：
  $$
  \left\{\begin{array}{c}
  s_1=\vec{A_2A_1}\cdot\vec n_2\\
  s_2=\vec{A_2B_1}\cdot\vec n_2\\
  s_3=\vec{A_2C_1}\cdot\vec n_2\\
  \end{array}\right.
  $$

+ $T_2$在平面$\pi_1$上，或$T_1$在平面$\pi_2$上，则问题简化为二维平面中三角形相交问题。

  计算$\vec f=\vec n_1\times \vec n_2$，若$\vec f=\vec 0$，说明$T_1、T_2$平行或共面。若$t_1、t_2、t_3、s_1、s_2、s_3$为0，则两个三角形共面，接下来要判断两个共面三角形是否相交。首先判断$A_1、B_1、C_1$是否在三角形$T_2$内，以及$A_2、B_2、C_2$是否在三角形$T_1$内。

  以判断点$A_2$是否位于三角形$T_1$中为例，首先建立表达式：
  $$
  \left\{\begin{array}{c}
  k_1=[\vec{A_1A_2}\cdot (\vec{A_1B_1}\times n_1)]\cdot[\vec{A_1C_1}\cdot(\vec{A_1B_1}\times n_1)]\\
  k_2=[\vec{A_1A_2}\cdot (\vec{A_1B_1}\times n_1)]\cdot[\vec{B_1A_1}\cdot(\vec{B_1C_1}\times n_1)]\\
  k_3=[\vec{A_1A_2}\cdot (\vec{A_1B_1}\times n_1)]\cdot[\vec{C_1B_1}\cdot(\vec{C_1A_1}\times n_1)]\\
  \end{array}\right.
  $$

  如果$k_1、k_2、k_3$均为非负，则$A_2$在三角形$T_1$内，否则在三角形外。以$k_1$的表达式为例，这个式子的含义如图所示：

<img src="Textures\平面三角形求交.png" alt="平面三角形求交.png" style="zoom: 33%;" />

​		这个表达式计算了$C_1$和$A_2$是否在线段$A_1B_1$的两侧。

 	   如果$A_1、B_1、C_1$不在三角形$T_2$内，且$A_2、B_2、C_2$也不在三角形$T_1$内，两个三角形仍有可能相交，相交的情况如下图所示：

<img src="Textures\平面三角形相交.png" alt="平面三角形相交" style="zoom: 33%;" />

  		在这种情况下，$T_2$的每个顶点，位于$T_1$的每条边异侧。建立表达式：
$$
\left\{\begin{array}{c}
h_1= \vec{A_1A_2}\cdot (\vec{A_1B_1}\times \vec n_1)\\
h_2= \vec{A_1B_2}\cdot (\vec{A_1B_1}\times \vec n_1)\\
h_3= \vec{A_1C_2}\cdot (\vec{A_1B_1}\times \vec n_1)
\end{array}\right.
$$
​		  如果$h_1、h_2、h_3$不同时为正或不同时为负，则$A_2、B_2、C_2$位于$A_1B_1$异侧。

+ $T_1$与$\pi_2$相交或$T_2$与$\pi_1$相交，则判断$T_2$中是否有边与$T_1$相交，或$T_1$中是否有边与$T_2$相交。

  当$\vec f \ne\vec0$，则三角形$T_1$和$T_2$不共面。则我们需要进行6次线段与三角形的求交，以判断边$A_2B_2$与三角形$T_1$是否相交为例，建立表达式：
  $$
  \vec S_1=\vec{A_1A_2}\times \vec n_1\\
  \vec S_2=\vec{A_1B_2}\times \vec n_1
  $$
  当$\vec S_1=\vec S_2=\vec 0$时，线段$A_2B_2$与$T_1$共面，否则异面。先考虑异面的情况，计算$\vec S_1\cdot\vec S_2$，若为正，则$A_2B_2$在$\pi_1$同侧，线段与三角形必不相交；若不为正，则两点在$\pi_1$异侧，且至少有一点不在平面上。假设$A_2$不在$\pi_1$上，建立表达式：
  $$
  g_1=[\vec{A_1B_2}\cdot(\vec{A_1B_1}\times\vec{A_1A_2})]\cdot[\vec{A_1C_1}\cdot(\vec{A_1B_1}\times\vec{A_1A_2})]\\
  g_2=[\vec{B_1B_2}\cdot(\vec{B_1C_1}\times\vec{B_1A_2})]\cdot[\vec{B_1A_1}\cdot(\vec{B_1C_1}\times\vec{B_1A_2})]\\
  g_3=[\vec{C_1B_2}\cdot(\vec{C_1A_1}\times\vec{C_1A_2})]\cdot[\vec{C_1B_1}\cdot(\vec{C_1A_1}\times\vec{C_1A_2})]
  $$
  如果$g_1、g_2、g_3$均为非负，则线段$A_2B_2$和三角形$T_1$相交。否则不相交。

  如果$A_2B_2$与$\pi_1$共面，判断$A_2、B_2$是否位于三角形$T_1$内，如果是，则三角形必相交。如果两点都不位于三角形内，则$A_2B_2$必与$T_1$中的两条线段相交，这两条线段与$A_2B_2$异面相交。由于我们需要遍历六条边进行相交判断，所以我们可以在$A_2B_2$不位于$T_1$内时暂时丢弃结果，相交结果将在遍历其它边时被检测出来。

##### 如何判断点与网格的关系(Ray-crossing算法)？

我们假设多边形是由三角形组成的封闭网格，那么要判断一个点是否在网格内，就需要由该点朝任意方向构造一条射线，如果该射线穿入的三角形面数与穿出的三角形面数之和为奇数，那么可以确定该点在网格内部，否则在外部。

思路很简单，我们需要解决三个难点：判断射线和三角形面相交(这里使用Moller-Trumbore算法，不再赘述)；判断相交方向是否在三角形正面；如何快速抛除大量无关三角形。

首先快速判断射线方向与三角形正面的关系。通过叉乘可以得到三角形的正面，然后用点乘判断射线方向和三角形正面的关系：
$$
t = \vec{V_0V_1}\times\vec{V_0V_2}\cdot \vec D
$$
若$t\gt0$，则射线射出三角形；若$t\lt0$，则三角形射入三角形；若$t=0$，则射线和三角形平行，认为没有射入也没有射出三角形。

接下来我们来对三角形进行快速剔除。

由于我们已知射线的原点$O$和方向$\vec D$，不妨假设$\vec D$的三个分量都大于零，在这种情况下，容易发现，射线不可能穿过任意一个分量的值小于$O$的顶点。显然，我们通过原点$O$将空间分为了8个区域，其中射线$\vec D$确定了这八个区域中的一个。所以我们遍历所有三角形，对于一个三角形，如果三个顶点都不位于射线所在的那个区域中，就可以将三角形抛弃。这样我们以$O(n)$的时间复杂度，平均剔除了$\frac78$的三角形。

接下来，我们可以对剩余的三角形进行2D投影，简单的排除z轴坐标，保留x、y轴坐标来实现投影。然后，我们对三角形求出其AABB(轴对齐外包边界框)，并按AABB的某个顶点(与$\vec D$的方向有关，如果$\vec D$的x、y分量都大于0，则选取左上角，以此类推)的x轴坐标对三角形进行散列。接下来按照$\vec D$的方向遍历散列表，抛弃AABB与射线不相交的三角形。这一轮我们以$O(2n)$的时间复杂度平均剔除了$\frac23$的三角形。

紧接着我们改变坐标轴，对x、y轴再次投影，重复上面的步骤，最终我们只剩下有限的一些三角形，对这些三角形进行Bentley-Ottman算法求交，判断射线与三角形是否相交以及如何相交，最后对交点进行统计即可。

这个思路扩展到尔维当中，也很容易理解点和封闭线框的碰撞判定逻辑，向一个方向发射射线，统计与所有线段的相交次数即可。

##### 如何判断OBB与OBB的关系？

在图形学领域对六面体进行相交测试是一个经典问题，它的主要应用领域为OBB的碰撞检测。由于AABB实际上可以认为是具有特定朝向的OBB，所以也可以包含在这类问题当中。

OBB的碰撞算法基于**分离轴定律(Seperating Axis Theorem, SAT)**。这个定律描述了这样的碰撞检测思想：对于任意两个凸多边形，如果我们能找到一个轴，使得两个物体在该轴上的投影互不重叠，则这两个物体间必不碰撞。其中，这个轴被称为**分离轴(Seperating Axis)**。一般来说，我们只检测垂直于多边形每条边的那些轴(在三维空间中，则是垂直于每个面的那些轴)。这么做是因为，多边形在垂直于它的边的轴上的投影是长度最短的投影，如果在这些轴上的投影都相交，那么在剩下没有检测的轴上自然也相交。

我们从二维矩形的相交测试开始考虑，对于一个矩形来说，它具有四条边，可以生成4条检测轴。但由于矩形的2个轴是重复的，所以我们实际上需要检测的只有两个轴，即矩形的两条互相垂直边所在的轴：

<img src="Textures\轴分离定律.png" alt="轴分离定律" style="zoom:33%;" />

把每个矩形的4个顶点投影到一个轴上，这样算出4个顶点最长的连线距离。同样的方法对待第二个矩形，将其投影到同一个轴上，然后判断两个矩形在这个轴上的投影是否相交。

在投影上判断相交可以使用这样的理解方式：由于我们只关心两个矩形之间的关系而不关心它们在场景中的位置和方向，所以我们以正在讨论的轴为x轴，以产生该轴的矩形的中心为原点重新构造坐标系，以此简化所要讨论的问题：

<img src="Textures\投影半径.png" alt="投影半径" style="zoom:33%;" />

显然，要比较两个矩形在新生成的X轴上的投影是否相交，可以等价于比较两点连线在X轴上的投影长度，与两个矩形的对角线向量的一半(图中的蓝色向量)在X轴上的投影的长度和的大小关系。但是，对作为坐标轴的矩形来说，选取哪个对角线对投影长度是没有影响的，因为对角线在轴上投影的长度始终等于矩形的宽度。但对于另一个矩形来说，更换对角线会影响其在X轴上的投影长度。为此，我们可以将对角线在X轴上的投影改为这个矩形两条互相垂直的边在轴上的投影的长度的和，这可以保证我们取的始终是更宽的那一条对角线。

最终判断两个OBB在指定轴上的投影是否相交的算法可以这样表示：

~~~C#
bool OBBInteract(OBB A, OBB B, Vector3 axis)
{
    Vector3 cc = B.c - A.c;
    float pa = Mathf.Abs(Vector3.Dot(A.w, axis)) + Mathf.Abs(Vector3.Dot(A.h, axis));
    float pb = Mathf.Abs(Vector3.Dot(B.w, axis)) + Mathf.Abs(Vector3.Dot(B.h, axis));
    float pc = Mathf.Abs(Vector3.Dot(cc, axis));
    return pa + pb >= pc * 2;
}

// OBB.c为OBB的中心坐标，OBB.w和OBB.h分别是它们的长向量和宽向量。
~~~

##### 如何判断OBB与视锥体的关系？

视锥体的剔除则发生在应用阶段，是渲染控制的一部分。一般来说，我们需要判断场景中物体的AABB或OBB是否与视锥体相交，以此判断是否有必要为这个物体生成一个Batch。

视锥体剔除的过程需要BVH的辅助。

##### 如何判断多边形与多边形的关系？



<div STYLE="page-break-after:always;"></div>

### 色彩学原理

#### 物理光学

##### 光具有哪些几何学性质？

光学具有三重领域，按照其诞生的历史顺序分为几何光学、波动光学和量子光学。而在计算机图形学领域，我们仅关注光的几何性质。在几何光学中，我们仅考虑可视光，而不考虑其它电磁波。可视光的波长特别小，几乎可以忽略其波动特性，而仅研究其粒子性。在这个背景下，光可以在宏观视角下由几何学描述，光可以认为沿直线传播。

虽然在物理仿真程序中，计算机确实可以模拟出完整的几何光学和部分的波动光学，但对于追求视觉正确而非本质正确的图形学领域来说几何光学仍然过于复杂。相比标准的几何光学，图形学领域对光做了如下简化：

+ 物体表面是绝对光滑的
+ 光只可以被发射，反射和传播
+ 光速无穷大

几何光学中定义，光入射到物体表面时会同时发生反射和折射，无论物体是否透明：

反射定理证明，光射到一个界面上时，其入射光线和反射光线与表面法线的夹角相等。

折射定理证明，折射光线位于入射光线和界面法线所决定的平面内，且位于法线的两侧。斯涅尔公式证明，入射角θ<sub>1</sub>的正弦值和折射角θ<sub>2</sub>的正弦值得比例，对折射率一定得两种介质来说是一个常数。
$$
n_1sin\theta_1=n_2sin\theta_2
$$
对于金属，折射进表面的光线的能量会立即被金属中的自由电子吸收，转换成电子的能量，不再可见。对于非金属，它们往往不是由单一成分构成，而可以认为其中包含了很多折射率不同的微粒，光线遇到这些粒子后发生反射折射，在物质内部不断传播，散射到不同方向，其中一部分会再次穿过表面被观察到，这种现象称为次表面散射，也有一部分在传输过程中被吸收。

由于光会在界面处同时发生折射和反射，入射光的能量自然会被分为两部分，菲涅尔公式论述了光在发生折射和反射时的能量分配问题，根据此我们可以得知在入射角为θ，入射介质反射率为n<sub>1</sub>，折射介质反射率为n<sub>2</sub>，入射光强度为L<sub>1</sub>的状态下，反射光的强度L<sub>2</sub>。由于我们不需要了解物理学中的推导过程，这里直接提供结论：
$$
L_2=(1-R_F(\theta))\frac{n_2^2}{n_1^2}L_1
$$
其中，R<sub>F</sub>(θ)是在入射角为θ时，物体的反射系数。但反射系数与物体的多种物理特性有关，这对图形学领域来说依然太复杂了，所以我们有了Schlick菲涅尔近似等式，这个近似公式与原公式的误差不超过1%：
$$
F_{Schlick}(\theta,F_0)\approx [F_0+(1-F_0)(1-\cos\theta)^5]L_1=lerp(F_0,1,(1-\cos\theta)^5)L_1
$$
其中$F_0\in [0,1]$为物体的基础反射率，一般是材质的固有属性，与材质的导电性等有关。

另外一种菲涅尔公式的拟合公式如下，这个公式是虚幻引擎使用的版本：
$$
F(\theta)=F_0+(1-F_0)\times2^{(-5.55473\cos\theta-6.98346)\cos\theta}
$$
**Microfacet模型**认为，由于物体表面并不是光滑的，真实物体表面是凹凸不平的，光所照到的物体表面的法线和理论上的法线并不相同，这导致光照到物体上时可能被反射到更多的方向上，这就是漫反射。而在假设光滑的平面下光的反射就被称为镜面反射。Microfacet模型就主要用于处理镜面反射，也就是高光反射。它与Lambert模型结合，再加上环境光和自发光效果，组成了最基础的各向同性的**基于物理的光照模型(PBR)**。

<img src="https://i0.hdslb.com/bfs/article/1a2ca0120f9eaf0280aa789911c646f77748a273.png@1320w_620h.png" alt="img" style="zoom: 25%;" />

人观察到的物体光照，基本上就是漫反射和镜面反射的总和。在某些偏复杂的光照环境中，我们也要考虑包括次表面散射和折射在内的更多光照特征。

##### 光具有哪些辐射度学(能量)性质？

**辐射度学(Radiology)**是一门以电磁波的电磁辐射能测量为研究对象的科学。计算机图形学中设计的辐射度学，一般集中于整个电磁波段中的可见光谱段的辐射能的计算，这个波段的波长范围是$0.38\mu m$~$0.76\mu m$。

辐射度学研究的辐射能是客观独立的，与观察者无关。辐射度学建立在几何光学的基础上，认为辐射是沿直线传播的，而不考虑干涉和衍射等性质。根据波粒二象性，若将光视为一束粒子流，则构成光的粒子被称为光子。在真空中，光子的速度为常量$c=299792458m/s$。每个光子的能量$E=hf$，其中$h$为普拉克常量，$f$为光子的频率。

**辐射通量(Radiation flux，又称辐射功率)**是为了研究在单位时间内通过某表面的各个光子所携带的能量之和的大小引入的概念。它定义为以辐射的形式发射、传输或者接受的功率，即单位时间内的辐射能，单位是$W$。不同波长的光引起人对不同颜色的感知，而不同辐射通量的光则引起人对光不同亮度的感知。

在给定空间V中，辐射通量在空间中的分布保持一个常量——只要光源恒定，在被光源所照亮的空间中的某部分不会自发地变得忽明忽暗。又由于光速非常快，对于考察光照效果的观察者而言，空间中的某个光源突然产生时，光的能量几乎立即被分布到空间中的每个角度，所以一切看起来都是保持恒定的。

在空间中，光子的流动和传播遵循能量守恒定律。在给定空间V中，流入空间的光子总能量一定等于从本空间中流出的光子能量与被本空间内物体所吸收的光子能量的总和。某一空间中光有两种来源：从外部空间中进入的光$\Phi_i$，从本空间中的物体发出的光$\Phi_e$。空间中的光有三种去向：不经过本空间内任何物体干扰直接向外流出的光$\Phi_s\ $，被本空间内物体完全反射后流出的光$\Phi_o$，被被本空间内的物体吸收的光$\Phi_a$。可以列出给定空间V中辐射通量守恒方程：$\Phi_e+\Phi_i=\Phi_s+\Phi_o+\Phi_a$。

在给定空间V中，如果已知任意一点$p$在任意一个方向$\omega$上的辐射通量值$\Phi(p,\omega)$，就能得到计算机图形学中光照问题的完全解决方案。使用复杂的积分公式实时求解光照是不可能的，在计算机图形学中主要研究$\Phi(p,\omega)$的近似解。

在此引入**球面度(立体角)**的概念：以半径为R的球的球心为顶点作一个圆锥，圆锥底面圆形的圆周全部位于球面上。由圆锥的底面圆所截取的那一部分球面的面积为A，则立体角$\Omega=\frac A{R^2}$。它的微分形式定义为$\mathrm{d}\Omega=\frac{\mathrm{d}A}{R^2}$。完整球面的立体角为$\int_0^{2\pi}\mathrm{d}\phi\int_0^\pi\sin\theta\mathrm{d}\theta=4\pi$。立体角的单位是$sr$。

如果在传输介质中没有发生反射、散射和吸收，那么在给定方向上的某一个立体角内，无论辐射距离有多远，其辐射通量是不变的。

**辐射强度(Radiation intensity)**是描述在给定方向上，单位立体角内光源发出的辐射通量的物理量，其单位为瓦/球面度。令辐射通量为$\Phi$，立体角为$\Omega$，辐射强度为$I$，则以微分形式定义的辐射强度公式为$I=\frac{\mathrm{d}\Phi}{\mathrm{d}\Omega}$。这个物理量常被用在点光源等常见光源的物理模拟中。

**辐射亮度(Radiance)**定义为辐射表面在其单位投影面积的单位立体角内发出的辐射通量，它描述了光源表面单位面积朝单位立体角发出的辐射强度，其单位为瓦/(球面度*米^2^)。令光源表面的面积微元为$\mathrm{d}A$，光线方向与面积微元$\mathrm{d}A$的法线的夹角为$\theta$，则面积微元的投影面积为$\mathrm{d}A'=\mathrm{d}A\cos\theta$。$\mathrm{d}A'$乘以立体角值$\Omega$得到截取的投影球面的表面积。令辐射亮度为$L$，则辐射亮度的微分形式定义为$L=\frac{\mathrm{d}I}{\mathrm{d}\Omega\mathrm{d}A\cos\theta}$。这个物理量常用于描述非点光源的线光源、面光源等辐射体。

**辐射出射度(Radiant exitance)**定义为离开光源表面的单位面积的辐射通量，令辐射出射度为$M$，则以微分形式定义辐射出射度的公式为$M=\frac{\mathrm{d}\Phi}{\mathrm{d}A}$。其中，面积微元$\mathrm{d}A$表示的是光源辐射平面上的半球空间，例如太阳表面的辐射出射度指太阳的单位表面积向外部空间发射的辐射通量。

**辐射入射度(Irradiance)**定义为单位面积被照射的辐射通量，令辐射入射度为$E$，则以微分形式定义辐射入射度的公式为$E=\frac{\mathrm{d}\Phi}{\mathrm{d}A}$，这个公式是求解光照效果的重要公式，如给定一个点光源和给定辐射通量$\Phi$，则它在一个半径为R的球面某一面积微元上的辐射入射度为$E=\frac{\Phi}{4\pi R^2}$。

##### 光具有哪些电磁学性质？

光具有波粒二象性，因此光可以视为一种电磁波，故而广播具有电磁波的所有性质，这些性质可以通过麦克斯韦方程组推导出来：
$$
\left \{ \begin{array}{c}
\Delta\cdot\vec D=\rho\\
\Delta\cdot\vec B=0\\
\Delta\times\vec E=-\frac{\partial\vec B}{\partial t}\\
\Delta\times\vec H = J+\frac{\partial\vec D}{\partial t}
\end{array}\right.
$$
其中，矢量$\vec D、\vec B、\vec E、\vec H$分别为电感应强度、磁感应强度、电场强度、磁场强度；标量$\rho$自由电荷密度；$J$为传导电流密度；$\Delta$算子称为哈密顿子，在三维直角坐标系中的展开式为：$\Delta=\frac\part{\part x}\vec i+\frac\part{\part y}\vec j+\frac\part{\part z}\vec k$。

麦克斯韦方程组描述了电磁现象的变化规律，指出了任何时间变化的电场将在周围空间产生变化的磁场，任何时间变换的磁场将在周围空间产生变化的电场，这些电场和磁场互相关联，互相激发，并以一定速度向周围空间传播。所以，对于在各向同性的均匀介质中传播的电磁波，可推导出由波速、波的磁场强度、波的电场强度三者建立的波动方程：
$$
\left \{ \begin{array}{c}
v=\frac1{\sqrt{\mu\epsilon}}\\
\Delta^2\vec E-\frac1{v^2}\frac{\part^2\vec E}{\part t^2}=0\\
\Delta^2\vec H-\frac1{v^2}\frac{\part^2\vec H}{\part t^2}=0
\end{array}\right.
$$
其中，$\Delta^2$为拉普拉斯算子，在三维直角坐标系中的展开式为：$\Delta^2=\frac{\part^2}{\part x^2}\vec i + \frac{\part^2}{\part y^2}\vec j+\frac{\part^2}{\part z^2}\vec k$。

从上式中可知，电磁波的波动方程就是电场强度和磁场强度关于时间的二阶偏导数。光波中包含电场矢量和磁场矢量，从波的传播特征来看，它们应处于同样的地位，但从光与传播介质的相互作用来看其作用不同。在大部分应用场景下电场的作用远超过磁场：**使相片底片感光的是电场，对人视网膜起作用的也是电场。因此，通常把广波中的电场强度矢量$\vec E$称为光矢量**，由于电场强度和磁场强度在空间中正交地交替变化，如果我们单纯观察电场强度的传播过程，可以解释为何光是一种横波。在物理学中，将电磁波的电场强度和磁场强度统一用场强$f$表示为一般形式：
$$
\left \{ \begin{array}{c}
v=\frac1{\sqrt{\mu\epsilon}}\\
\Delta^2f-\frac1{v^2}\frac{\part^2f}{\part t^2}=0
\end{array}\right.
$$
一般来说，在图形学中我们使用球面坐标系求解波动方程。设径向为$r$、天顶角为$\theta$、方位角为$\phi$。在球面坐标系上求解一个点的场强$f$就相当于求解人眼感受到的辐射强度，而$f$又是一个以$r、\theta、\phi$为自变量的函数，根据直角坐标系和球面坐标系的换算关系可以将拉普拉斯方程$\Delta^2\vec f=0$改写为：
$$
\frac1{r^2}\frac{\part}{\part r}(r^2\frac{\part f}{\part r})+\frac1{r^2\sin\theta}\frac{\part}{\part\theta}(\sin\theta\frac{\part f}{\part\theta})+\frac1{r^2\sin^2\theta}\frac{\part^2 f}{\part \phi^2}=0\tag{1}
$$
上式是一个偏微分方程，函数$f$具有3个自变量，无法直接求解，所以需要利用分离变量法将该式分解为多个常微分方程，分别进行求解：

将$f(r,\theta,\phi)=R(r)Y(\theta,\phi)$代入公式(1)，得到：
$$
\frac{Y}{r^2}(r^2\frac{dR}{dr})+\frac{R}{r^2\sin\theta}\frac{\part}{\part\theta}(\sin\theta\frac{\part Y}{\part\theta})+\frac{R}{r^2\sin^2\theta}\frac{\part^2Y}{\part\phi^2}=0\tag{2}
$$
方程两边同时乘以$\frac{r^2}{YR}$，经整理可得：
$$
\frac1R\frac{\mathrm{d}}{\mathrm{d}r}(r^2\frac{\mathrm{d}R}{\mathrm{d}r})=-\frac1{Y\sin\theta}\frac{\part}{\part\theta}(\sin\theta\frac{\part Y}{\part\theta})-\frac1{Y\sin^2\theta}\frac{\part^2Y}{\part\phi^2}=l(l+1)\tag{3}
$$
其中$l$为常数。于是可以将球面坐标系拉普拉斯方程(1)分离为如下两个式子：
$$
\frac{\mathrm{d}}{\mathrm{d}r}(r^2\frac{\mathrm{d}R}{\mathrm{d}r})-l(l+1)R=0\tag{4}
$$

$$
\frac1{\sin\theta}\frac{\part}{\part \theta}(\sin\theta\frac{\part Y}{\part \theta})+\frac1{\sin^2\theta}\frac{\part^2Y}{\part\phi^2}+l(l+1)Y=0\tag{5}
$$

其中，式(4)为欧拉方程，是一个常微分方程，可以直接求解。而式(5)为球谐函数方程，依然是偏微分方程，所以需要进一步分离变量才能求解。再令$Y(\theta,\phi)=\Theta(\theta)\Phi(\phi)=0$，代入式(5)中，可得到：
$$
\frac{\Phi}{\sin\theta}\frac{\mathrm{d}}{\mathrm{d}\theta}(\sin\theta\frac{\mathrm{d}\Theta}{\mathrm{d}\theta})+\frac{\Theta}{\sin^2\theta}\frac{\mathrm{d}^2\Phi}{\mathrm{d}\phi^2}+l(l+1)\Theta\Phi=0\tag{6}
$$
式(6)两边同时乘以$\frac{\sin^2\theta}{\Theta\Phi}$，经整理可得：
$$
\frac{\sin\theta}{\Theta}\frac{\mathrm{d}}{\mathrm{d}\theta}(\sin\theta\frac{\mathrm{d}\Theta}{\mathrm{d}\theta})+l(l+1)\sin^2\theta=-\frac1{\Phi}\frac{\mathrm{d}^2\Phi}{\mathrm{d}\phi^2}=m^2\tag{7}
$$
其中$m$为常数。于是可以将式(5)分解为两个常微分方程：
$$
\frac{\mathrm{d}^2\Phi}{\mathrm{d}\phi^2}+m^2\Phi=0\tag{8}
$$

$$
\sin\theta\frac{\mathrm{d}}{\mathrm{d}\theta}(\sin\theta\frac{\mathrm{d}\Theta}{\mathrm{d}\theta})+[l(l+1)\sin^2\theta-m^2]\Theta=0\tag{9}
$$

综上，球面坐标系下拉普拉斯偏微分方程(1)可以通过分离变量法分解成3个常微分方程(4)、(8)、(9)，通过解这三个方程便可以得到球面坐标系下的拉普拉斯偏微分方程的解$R(r)、\Theta(\theta)、\Phi(\phi)$：

$$
R(r)=Cr'+\frac D{r^{l+1}},l=0,1,2,...,\ C、D为常数
$$

$$
\Phi(\phi)=A_m\cos(m\phi)+B_m\sin(m\phi)=e^{im\phi}，m=0,1,2...
$$





##### 光具有哪些光度学(亮度)性质？

**光度学(Photometry)**的研究范畴是光对人眼产生的目视刺激的强度。

光度量和辐射度量的定义是一一对应的，如下表所示：

| 辐射度量   | 光度量   | 符号   | 方程                                                         | 辐射度量单位名称和符号              | 光度量单位名称和符号   |
| ---------- | -------- | ------ | ------------------------------------------------------------ | ----------------------------------- | ---------------------- |
| 辐射量     | 光量     | $Q$    |                                                              | 焦，J                               | 流秒，$lm\cdot s$      |
| 辐射通量   | 光通量   | $\Phi$ | $\Phi=\mathrm{d}Q/\mathrm{d}r$                               | 瓦(焦/秒)，W(J/s)                   | 流明，$lm$             |
| 辐射强度   | 发光强度 | $I$    | $I=\mathrm{d}\Phi/\mathrm{d}\Omega$                          | 瓦每球面度，$W/sr$                  | 坎德拉，$cd$           |
| 辐射亮度   | 光亮度   | $L$    | $L=\mathrm{d}^2\Phi/(\mathrm{d}\Omega\mathrm{d}A\cos\theta)$ | 瓦每球面度平方米，$W/(sr\cdot m^2)$ | 坎每平方米，$cd/m^2$   |
| 辐射出射度 | 光出射度 | $M$    | $M=\mathrm{d}\Phi/\mathrm{d}A$                               | 瓦每平方米，$W/m^2$                 | 流每平方米，$lm/m^2$   |
| 辐射入射度 | 光照度   | $E$    | $E=\mathrm{d}\Phi/\mathrm{d}A$                               | 瓦每平方米，$W/m^2$                 | 勒克斯，$lx$($lm/m^2$) |

眼睛受限于可查觉的最小亮度和最小分辨率。视力正常的人能在中等亮度和中等对比度的情况下观察静止图像，并分别出$1'$~$1.5'$的最小视角。如果显示器像素可显示的亮度等级小于人眼可察觉的最小亮度差，或者其像素对人眼所张视角为眼睛最小角度分辨率的1/10，那么该显示器的复杂性就超出了必要。

人眼可以辨识亮度范围可达$10^9:1$。人眼总视觉范围很宽，但不能在同一时间感受到这么大的亮度范围。当平均亮度适中时亮度范围为$100:1$，平均亮度较高或较低时亮度范围只有$10:1$。电影银幕亮度范围大致为$100:1$，CRT显像管亮度范围约为$30:1$。人眼对景物亮度的主观感觉不仅取决于场景的实际亮度，而且与周围环境的平均亮度高度相关。人沿的明暗感觉是相对的，在不同的环境亮度下，对同一亮度的主管感觉会不同。

光通量和辐射通量可以用以下公式进行转换：
$$
\Phi_v(\lambda)=K_mV(\lambda)\Phi_e(\lambda)
$$
其中，$\Phi_v$是光通量，$\Phi_e$是辐射通量，$\lambda$是光的波长。$V(\lambda)$是由国际照明委员会(CIE)推荐的平均人沿光谱光视效率，即**视见函数**，$K_m$是最大光谱光视效能值，它是一个常熟。下图展示勒人眼在明视觉和暗视觉下的视见函数：

<img src="Textures\视见函数.jpg" alt="视见函数" style="zoom:67%;" />

明视觉指的是人眼在光亮度超过3cd/m^2^的环境下产生的视觉，此时视觉主要由视锥细胞起作用。与明视觉相对，暗视觉指人眼在环境亮度低于$10^{-3}$cd/m^2^的环境下产生的视觉，此时视杆细胞是主要的感光细胞。

眼睛中的视觉细胞检测到光后触发视觉系统反应；粗略的说，到达视觉细胞的光每增强一倍，引发的刺激响应将增加固定的强度，这一定律也被称为**韦伯-费希纳定律**。如果光源B与光源A相比较，主管感受中光源B的亮度只有光源A的一半，则光源B发出的实际能量经过实验测量，大约只有光源A的18%。

功率为50%的灰色，人眼实际感知亮度为：
$$
\sqrt[2.2]{0.5}\times 100\% = 72.97\%
$$
而人眼认为的50%中灰色，实际功率为：
$$
0.5^{2.2}\times 100\%=21.76\%
$$
这意味着视觉系统的局部适应性意味着亮度的变化往往比其绝对值更受关注，在比较两幅图像时，其对应像素**亮度的比值比两者的差值更重要**。所以在储存与亮度相关的数据时，应优先考虑图像的梯度而不是亮度的绝对值。

在早期电子显示器使用8位的灰度通道显示图像。如果不针对性的处理这些色彩，比中灰色暗的像素就会被记录在0.2以下并储存，只能分配到50个灰阶，而比中灰色亮的像素却被分配了200多个灰阶，显得过于冗余。

考虑到早期图形设备储存空间的限制，图形学家为此提出了**伽马校正**这一概念：
$$
V_{out}=AV_{in}^\gamma
$$
其中A为常量，通常为1，而γ分为**解码伽马值**和**加码伽马值**，解码伽马值通常为2.2，加码伽马值通常为2.2的倒数，也就是0.45。一般来说，早期电子摄像机可以通过伽马加码，将实际亮度储存成更亮的值来保存暗部的灰阶，然后在显示时通过伽马解码来恢复成正确的物理功率，这也是加码和解码的含义。这样，人眼中的50%灰就可以被加码为0.5储存在数据结构中，而在显示器显示时又被解码为0.2176的实际物理功耗。

<img src="Textures\伽马变换.png" alt="伽马变换" style="zoom: 50%;" />

即使在现代图形设备中，这种储存空间限制依然存在。在目前主流的RGBA32设备中，每个通道依旧只有8个不同的亮度阶，这使得伽马校正在目前的图形学领域中依然是重要的知识点。

在讨论伽马校正时，要对颜色采集段和颜色数据还原段同时讨论。如果解码伽马值和编码伽马值得乘积为1，理论上可以让还原端精确重现实际场景得视觉感受。但是，由于人眼对亮度的感受与周围的环境有关，真实场景中未被采集设备记录的部分对人眼视觉的影响被忽视，使得加码伽马值和解码伽马值的乘积为1反而不一定能精确还原实际场景的视觉观感。

为了使还原端尽可能重现采集端采集的实际场景的视觉观感，在电影院这种漆黑的环境中加码伽马值和解码伽马值的乘积一般为1.5，而在明亮的室内乘积为1.125。

##### 伽马工作流和线性工作流有什么区别？

伽马工作流和线性工作流是渲染引擎提供的一个可配置选项，可以通过设置渲染工作流来定义引擎在对图像进行采样时的行为。

长期以来，伽马工作流都是标准的开发用颜色空间。但随着硬件的进步，线性颜色空间能得到更精确的渲染效果。

在线性工作流中，储存颜色数据的纹理无论是在线性空间还是伽马空间中创建的，都可以正常工作。基于历史原因，图片文件中保存的颜色多半已经转换到了伽马空间。一般来说，引擎计算光照并烘培光照贴图的过程都是在线性空间中完成的，而计算结果则被变换到了伽马空间储存在贴图文件中。无论使用伽马工作流还是线性工作流，贴图数据都是位于伽马空间的。

在线性工作流中渲染时，渲染器则需要使用基于线性空间的颜色数据来保证计算结果的正确性。Unity提供的解决方式是，使用**sRGB采样器(这是一种硬件结构)**代替线性RGB采样器，以此将伽马空间中生成的图片采样到线性空间，然后在渲染计算和混合时，使用线性空间中的数据，并将运算结果通过sRGB写入器转换回伽马空间写入缓冲区。特殊的，如果使用了HDR，那么通过sRGB写入器将颜色转换到伽马空间的过程将被推迟到屏幕渲染结束后。

相对的，在伽马工作流中渲染时，渲染器在伽马空间中计算光照效果，不需要使用sRGB采样器来转换色彩空间，在整个运算流程中不存在线性空间和伽马空间的转换。

换句话说，**在线性工作流中，硬件处理了在纹理采样和纹理储存中的空间变换；而在伽马工作流中，硬件完全不关心空间变换，如果需要进行空间变换则应手动进行。**

在线性工作流中需要注意输入的纹理文件的性质，如果纹理中的颜色数据本就是基于线性空间的，需要手动设置纹理文件禁用sRGB采样，避免对色彩进行了多重伽马解码，如果多重伽马解码会导致图片异常偏暗。除此之外，法线贴图，或被用于数据查询的贴图也应该禁用sRGB。而在伽马工作流中，由于不存在线性空间和伽马空间的转换，所以是否禁用sRGB没有任何区别。

虽然线性工作流更准确，但有些平台硬件只支持伽马工作流，也就是不支持sRGB采样器。这使得即使纹理数据是储存在伽马空间中的，着色器仍然将其以计算线性空间颜色的方式去计算。由于伽马加码会使图像亮度值变亮，所以随着光强度的增加，待绘制表面将以非线性的方式过度曝光。

在伽马工作流下，可以根据传入的纹理的实际情况，在着色器中手动实现伽马加码和解码函数以缓解这样的问题，即在采样后对采样得到的色彩进行伽马解码，在将色彩提交到颜色缓冲区前进行伽马加码。但注意，在这种情况下，由于对帧缓存的混合操作仍发生在非线性空间中，可能仍会导致混合的结果异常**变暗(透明度混合)**或**变亮(线性叠加)**：

变暗的逻辑可以这样理解：假设有线性空间中的两种颜色$(0.2,0,0)$和$(0,0.2,0)$，它们一比一混合后得到$(0.1,0.1,0)$，而将它们对应到伽马空间中为$(0.48,0,0)$和$(0,0.48,0)$，它们一比一混合后得到伽马空间中的$(0.24,0.24,0)$，转换回线性空间中只有$(0.043,0.043,0)$。

<img src="Textures\伽马叠加.png" alt="伽马叠加" style="zoom:67%;" />

为了避免混合发生在非线性空间中导致的异常效果，我们可以在提交色彩到颜色缓冲区时不进行伽马加码，而是在渲染结束后使用一次额外的屏幕后处理来对整个场景补充伽马加码。又或者，从头到尾不进行任何空间变换，保证纹理储存在线性空间中。

##### 光具有哪些色度学性质？

物理学中的颜色严格依照光的波长划分。光子具有固定的波长，所以将3M个550nm的光子和3M个750nm的光子混合，它们并不会混合成6M个650nm的光子。但人眼并不能准确的识别出3M个550nm的光子和3M个750nm的光子，对人眼来说，3M个550nm的光子和3M个750nm的光子的混合光束，与6M个650nm的光子组成的光束差别不大。

人眼中有三种视锥细胞，分别称为S、M、L，它们大都集中在视网膜中央，每个视网膜上有700万余个视锥细胞。其中，S细胞对红色敏感、M细胞对绿色敏感、L细胞对蓝色敏感，这也是计算机领域通用的三基色。**三基色**(红绿蓝)与**三原色**(红黄青)不同，三原色是通过减色法进行混合的，它模拟的是吸收不同波长光的物体混合后的结果，所以三原色的物体混合后将吸收所有可见波长的光，呈现黑色；而三基色模拟的是不同波长光混合后的结果，所以三基色混合后将呈现白色。

除了视锥细胞外，视杆细胞对光线的强度更敏感，很微弱的光线就能激发它对光的感应。但视杆细胞无法针对光线的波长感受到对应的颜色。

人眼对三基色的识别在光谱上依旧是非线性的，由于太阳光中绿色光的含量最丰沛，人眼的视觉细胞也对绿色最为敏感：

<img src="Textures\人眼视觉曲线.jpg" alt="img" style="zoom:50%;" />

图中，人眼将S曲线全部识别为蓝色，M曲线全部识别为绿色，L曲线全部识别为红色。

由于人眼识别RGB三个分量也是非对称的，所以计算机也必须适应人类的这一特征，在根据人类视觉计算物理特性时必须考虑到三个分量的比例，比如在计算明度和灰度时对三个分量必须使用不同的系数。

但人眼对混合光的颜色的感知却是线性的，人感知的颜色是蓝绿红三种视锥细胞的**神经强度**的线性叠加，这一特性被称为**格拉斯曼定律**，即如果视锥细胞对两束光$C_1$和$C_2$的刺激反应分别是$r_1$和$r_2$，则将它们按照一定比例混合得到的颜色$C_3=\alpha C_1+\beta C_2$，被视锥细胞感受后的刺击反应值是$r_3=\alpha r_2+\beta r_2$。

格拉斯曼定律方便了显示器的制造，对显示器来说，对某个像素输入一个RGB参数，只需要线性的改变红绿蓝三个子像素的亮度即可。这三个子像素的波长一般是三种视觉细胞最敏感的波长，比如450nm、550nm和580nm的组合。注意，亮度在伽马校正阶段已经处理好了，所以显示器不需要考虑人眼识别亮度的非线性特性。

视锥细胞的感应光子的能力存在一个范围，也就是到达某个强度或者低于某个强度就超过了它的能力范围。因此，人眼视锥细胞的响应也存在饱和以及最低响应值：蓝、绿和红不可能无限的亮，人类的感知只是这个三维色度空间里的一个有限空间区域。这个区域被称为**CIE色表**，它囊括了所有人眼能看到的色彩。

<img src="Textures\CIE图.jpg" alt="http://www.boss-int.com/Develop/sheet/CIE1931_fig2.jpg" style="zoom: 67%;" />

在测量某个光子的“颜色”时，只需要把那个波长处的三种视锥细胞的响应曲线的响应率分别乘以此波段的光的强度，得到三维色度空间里的坐标(s, m, l)。在光强度固定的状态下，380nm~780nm光谱会被映射到**SML空间**中的一条曲线上，这条曲线正是CIE色表中的边缘曲线，CIE色表边缘上标注的波长信息表面该颜色可以近似代表自然界中对应波长的单色光的“颜色”。

注意到，表示紫色的部分波长记为-500nm，这是因为人眼实际上并不能识别紫色波段的光。人眼看到的紫色实际上是大脑对红色和蓝色的线性混合。

得到一个RGB输入后，在CIE色表中截取一个三角形，然后将RGB坐标映射到这个三角形中，每个RGB值可以对应SML坐标中的一个颜色，这个三角形范围就被称为**RGB色彩标准**，映射使用的变换称为**色彩空间变换**，常见的色彩空间变换矩阵如下：

<img src="https://imgsa.baidu.com/baike/pic/item/c8177f3e6709c93d89f1876c963df8dcd100542e.jpg" alt="img" style="zoom: 50%;" />

印刷行业主流的色彩标准称作**sRGB**，它反映的是真实世界中伽马值为2.2的CRT显示器的效果。这个标准保证了同一份印刷产品能在不同品种的标准打印机下表现相同的颜色，而Adobe指定的**ARGB**标准移动了三角形在绿色区域的顶点，获得了更大的色彩范围。但由于显示器复杂的工艺，大部分显示器甚至还没办法达到sRGB的标准。显示器行业中没有准确的色彩标准，这导致不同显示器厂商面对相同的RGB输入可能产生不同的显示效果。

通常来说，我们会使用**L2距离**公式来计算两个颜色的相近程度：
$$
L2=\sqrt{\sum_{i=1}^n\mid C_1-C_2\mid ^2}
$$
#### 色彩空间

##### 什么是色彩空间?

色彩空间可以被理解成是一个n维的线性空间。当然，从光学角度上讲，我们最常用的色彩空间RGB中的三个分量r、g、b实际上是非正交的，因为人眼的三种感光细胞实际上对所有可视光频段的光都存在一定程度的感知。

RGB色彩空间是最好理解的色彩空间，我们在三维空间中取一组标准正交基构建三维笛卡尔坐标系，将RGB分量分别映射在XYZ轴上，每个对应点是RGB分量的线性混合，可以轻易得到**RGB立方体**，如图a)。不同厂商具有不同的RGB输出标准，所以在不同屏幕上观察这幅图可能看到不同的色域，而RGB立方体的顶点处的色彩一般就是该显示器设定的三基色。

![色彩空间](Textures\色彩空间.png)

CMYK色彩空间是印刷行业常用的色彩空间。利用色料的三原色混色原理，加上黑色油墨，共计四种颜色混合叠加，形成所谓“全彩印刷”。将RGB模型取反就可以轻易的获得**CMY立方体**，如图b)。四种标准颜色是：

- **C**： Cyan = 青色
- **M**： Magenta = 洋红色
- **Y**： Yellow = 黄色
- **K**： blacK = 黑色

理想的CMY三原色就可以混合出包括黑色在内的所有色彩，但现实世界里的彩色印刷使用的CMY三色色料并不是物理学意义上的标准三原色，这使得三层CMY套印只能表现出深灰色或深褐色。为了节省成本并增强油印效果，即使黑色并非原色，也成为彩印的必须色之一。

理想的CMY与RGB的转换：
$$
\begin{bmatrix}C\\M\\Y\end{bmatrix}+\begin{bmatrix}R\\G\\B\end{bmatrix}=\begin{bmatrix}1\\1\\1\end{bmatrix}
$$
HSV(HSB)色彩空间是基于柱坐标系的色彩空间，将色调、饱和度和明度分别映射在φ、r、z分量上，可以得到**HSV(HSB)柱体**，如图c)。**柱体最外围一圈顶点的色彩一般是该显示器的**RGB色彩标准边界上的色彩。

**色调(色相)H**用角度度量，取值范围为0°～360°，从红色开始按逆时针方向计算，红色为0°(360°)，黄色为60°，绿色为120°，青色为180°，蓝色为240°，紫色为300°。

**饱和度S**表示颜色接近光谱色的程度。一种颜色，可以看成是某种光谱色与白色混合的结果。其中光谱色所占的比例愈大，颜色接近光谱色的程度就愈高，颜色的饱和度也就愈高。饱和度高，颜色则深而艳。光谱色的白光成分为0，饱和度达到最高。通常取值范围为0%～100%，值越大，颜色越饱和。

**明度V(B)**表示颜色明亮的程度，对于光源色，明度值与发光体的光亮度有关；对于物体色，此值和物体的透射比或反射比有关。通常取值范围为0%（黑）到100%（白）。

RGB到HSV(HSB)的转换：
$$
m = \max(r,g,b)\\
n = \min(r,g,b)\\
h=\left \{ \begin{array}{c}
0°&m=n\\
60°\times \frac{g-b}{m-n}+0°&m=r \land g\ge b\\
60°\times \frac{g-b}{m-n}+360°&m=r \land g\lt b\\
60°\times \frac{b-r}{m-n}+120°&m=g\\
60°\times \frac{r-g}{m-n}+240°&m=b\\
\end{array}\right.
\\s=\min(0,\frac{m-n}m)\\v=m
$$
HSV(HSB)到RGB的转换：
$$
h_i=\lfloor\frac h{60°}\rfloor \mod 6\\
f=\frac h{60°}-h_i\\
p=v\times(1-s)\\
q=v\times(1-f\times s)\\
t=v\times(1-(1-f)\times s)\\
(r,g,b)=\left \{ \begin{array}{c}
(v,t,p)&h_i=0\\
(q,v,p)&h_i=1\\
(p,v,t)&h_i=2\\
(p,q,v)&h_i=3\\
(t,p,v)&h_i=4\\
(v,p,q)&h_i=5
\end{array}\right.
$$

HSL和HSV(HSB)类似，它的H分量具有一样的算法，但S和L不同：
$$
m = \max(r,g,b)\\
n = \min(r,g,b)\\
h=\left \{ \begin{array}{c}
0°&m=n\\
60°\times \frac{g-b}{m-n}+0°&m=r \land g\ge b\\
60°\times \frac{g-b}{m-n}+360°&m=r \land g\lt b\\
60°\times \frac{b-r}{m-n}+120°&m=g\\
60°\times \frac{r-g}{m-n}+240°&m=b\\
\end{array}\right.\\
l=\frac12 (m+n)\\
s=\left \{ \begin{array}{c}
\frac{m-n}{2l},&l\lt \frac{1}{2}\\
\frac{m-n}{2-2l},&l\ge \frac12
\end{array}\right.
$$

HSL转化为RGB：
$$
q=\left \{ \begin{array}{c}
l\times (1+s),&l\lt\frac12\\
l+s-(l\times s),&l\ge \frac12
\end{array}\right.\\
p=2\times l-q\\
h_k=\frac h{360}\\
t_R=\left \{ \begin{array}{c}h_k+\frac13,&h_k\lt\frac23\\h_k-\frac23,&h_k\ge\frac23\end{array}\right.\\
t_G=h_k\\
t_B=\left \{ \begin{array}{c}h_k+\frac23,&h_k\lt\frac13\\h_k-\frac13,&h_k\ge\frac13\end{array}\right.\\
Color_C=\left \{ \begin{array}{c}
p+((q-p)\times 6\times t),&t_C\lt\frac16\\
q,&\frac16\le t_C\lt\frac12\\
p+((q-p)\times 6\times(\frac23-t_C)),&\frac12\le t_C\lt\frac23\\
p,&t_C\ge\frac23
\end{array}\right.,C\in \{R,G,B\}\\Color=(Color_R,Color_G,Color_B)
$$

RGB是面向硬件的色彩空间，而HSV(HSB)和HSL是面向用户的色彩空间，符合人类直觉，在许多图像编辑工具中应用比较广泛，但这也决定了它不适合使用在光照模型中，许多光线混合运算、光强运算等都无法直接使用HSV(HSB)来实现。

除了常见的色彩空间外，我们还可以将颜色转移到灰度空间。灰度可以表现物体的亮度，通过**灰度函数**我们可以将三通道的色彩混合为单通道。一般的自然景物平均灰度约为18%。

灰度的计算公式包括：

+ 除数法：

$$
Gray=\frac{30\cdot R+59\cdot G+11\cdot B}{100}
$$

+ 移位法：

$$
Gray = (77\cdot R+151\cdot G+28\cdot B)>>8
$$

+ 伽马校正法：

$$
Gray = \sqrt[2.2]{\frac{R^{2.2}+(1.5G)^{2.2}+(0.6B)^{2.2}}{1+1.5^{2.2}+0.6^{2.2}}}
$$

鉴于精确度的要求，在高质量图片处理中最好使用伽马校正灰度公式进行计算，以保证准确度。

##### 什么是拮抗原理？

在三基色理论之外，黑林(Ewald Hering)提出了另一种颜色理论——**拮抗原理(Opponent-Process Theory)**，简称四基色学说。黑林认为人眼对光产生视觉反应的过程是由三对互相拮抗的组织主导的，它们分别为红与绿、黄与蓝和黑与白。当一对拮抗组织中的一个受激兴奋时，与它同组的另一类组织就会受抑制。如黄与蓝，当人眼对蓝产生兴奋时，对黄色的感知就会受到抑制。

拮抗原理也能解释颜色互补现象。如果产生两种颜色的光波相混合，结果出现灰色，则两种颜色互补，或称为互补色。互补色在色环上的位置相对，红与绿、蓝与黄是互补色。根据拮抗原理，互补现象是由于两个互补色以相反方向刺激一对拮抗视觉单元，两种色彩相互抵消造成的。这三对相互拮抗的活动已得到研究证明。它们不是在视网膜上，而是在视神经通路坐功图的神经结细胞发生的。

使用HSV(HSB)或HSL调整色相时，具有人眼视觉感知亮度随之变化的问题。例如，纯蓝与纯红在HSV中具有相同的V值，但人类对其的感知显然是蓝色更亮(当然，伽马校正可以有效的改善这一点)。拮抗原理在计算机图形学中的应用恰好可以针对性的解决这个问题。

**YCbCr色差空间(有时也被称为YUV，但实际上是YUV的改进)**是另一种常见的拮抗色彩空间，其中Y指图像的亮度，Cb指蓝绿色差，Cr指红绿色差(白色色差为0)。这种色彩空间通常用于影片中的影像连续处理，或是数字摄影系统中，由于记录通道间的差值而非各通道的实际亮度，可以提升储存的精度，方便进行压缩。在自动白平衡算法中有重要的作用，JPEG就是基于YCbCr空间进行的图像压缩。RGB转YCbCr的公式为：
$$
Y=0.299*R+0.587*G+0.144*B\\
Cb=-0.1687*R-0.3313*G+0.5*B+128\\
Cr=0.5*R-0.4187*G-0.0813*B+128
$$
色差空间的特点是，它与RGB的转换是线性变换，能弄矩阵快速的实现转换。

**LUV(有时也被称为Lab，但实际上是与Lab类似的色彩空间)**色彩空间是一个不基于RGB色彩空间的色彩方案，它是由拮抗原理衍生出的一套色彩空间方案。在LUV色彩空间中，色彩被分配到了一个平面中，和纹理坐标中的UV类似，在L确定的情况下，LUV色彩平面是由一组亮度相同的颜色组成的。其中，U值定义了颜色蓝的程度，而V值定义了颜色红的程度。

![LUV](Textures\LUV.png)

LUV虽然解决了RGB亮度不均的问题，但没有提供很好的变换色相的方案。

由CIE给出的官方算法整理出RGB和LUV转换的过程，先将RGB转换到XYZ空间：
$$
XYZ=\begin{bmatrix}0.636958&0.144616&0.168881\\
0.262700&0.677998&0.059301\\0.000000&0.028072&1.060985
\end{bmatrix}\begin{bmatrix}R\\G\\B\end{bmatrix}\\RGB=\begin{bmatrix}
1.716651&-0.355670&-0.253366\\-0.666684&1.616481&0.015769\\
0.017639&-0.042771&0.942103
\end{bmatrix}\begin{bmatrix}X\\Y\\Z\end{bmatrix}
$$

> 注：浮点数保留小数点后六位，更精确的参数参见W3C的版本：
>
> https://www.w3.org/TR/css-color-4/#color-conversion-code

然后将XYZ转换到LUV：
$$
\epsilon=(\frac6{29})^3\\\kappa=(\frac{29}3)^3\\
xyz=(\frac X{0.96422},Y,\frac Z{0.82521})\\
f_a=a>\epsilon\ ?\sqrt[3]a\ :\ \frac{(\kappa*a +16)}{116}, a\in \{X,Y,Z\}
\\L=(116*f_Y)-16\\U=500*(f_X-f_Y)\\V=200*(f_Y-f_Z)
$$
将LUV转换为XYZ：
$$
f_Y=\frac{(L+16)}{116}\\f_X=\frac U{500}+f_Y\\f_Z=f_Y-\frac V{200}\\
X=(f_X^3>\epsilon\ ?\ f_X^3:\frac{116*f_X-16}{\kappa})*0.96422\\
Y=L>\epsilon*\kappa\ ?\ f_Y^3:\frac{L}{\kappa}\\
Z=(f_Z^3>\epsilon\ ?\ f_Z^3:\frac{116*f_X-16}{\kappa})*0.82521\
$$

**HCL (Hue-Chroma-Luminance)**色彩空间是由拮抗原理衍生出的一套色彩空间方案，旨在确保当色相改变而 L 通道保持恒定时，对人眼而言，色彩对比度不变。HCL由CIE提出，所以也被称作CIEHCLuv。HCL可以被理解为LUV的极坐标表示，如图所示：

![HCL](Textures\HCL.png)

Luminance作为极坐标的纵轴，将LUV中的UV看作向量，向量的模为Chroma，夹角为Hue。

由于HCL是基于拮抗原理的色彩空间，它并不能由RGB色彩空间经过线性变化得到，在RGB和HCL之间转换存在一定的失配问题，具体表现为：转换后颜色但色相、感知明度均存在，但颜色的色距不一定在 HCL 色彩空间范围内。

由LUV转换到HCL：
$$
L = L\\
H'=\arctan(\frac{V}{U})*\frac{180}{\pi}\\
H = H'>=0\ ?\ H':H'+360\\
C=\sqrt{U^2+V^2}
$$
反过来由HCL得到LUV：
$$
L=L\\U=C\cos(H*\frac\pi{180})\\
V=C\sin(H*\frac\pi{180})\\
$$

##### 什么是HDR？

HDR是**High Dynamic Range**的缩写，即**高动态范围**，HDR是一种颜色缓冲的实现技术。与之相对地是通常使用在显示设备中的**LDR缓冲**，LDR缓冲中每个通道的精度为8位，意味着我们只能使用256种亮度来表示真实世界中的亮度，因此，这个过程中一定会产生精度损失。而HDR技术使用更高的精度来记录每个通道的信息**(常用的HDR格式是每通道是0、12或16位，最高的有每通道32位)**，这使得我们可以表示范围远超过0~255的亮度值，从而更精确地反映真实地光照环境。

HDR可以更真实的反映光照。如果我们场景中有很多光源或光源强度非常大，那么一个物体在经过多次光照渲染叠加后最终得到的光照亮度很可能会超过1，如果没有使用HDR，那么这些超过的部分就会全部截取到1，使得场景丢失了很多亮度的细节。而开启HDR后，我们就可以保留这些细节，然后使用**色调映射(Tone Mapping)**技术将HDR转换到LDR，以最大程度的保留我们需要的亮度细节。

一般来说，HDR的实现遵循以下步骤：

+ 在一个大得多(每通道16或32位)的渲染目标(帧缓冲空间)上渲染场景
+ 使用HDR编码方式来节省所需的内存和带宽，常用的HDR编码方式有RGBM编码和LogLuv编码
+ 通过降采样计算场景亮度
+ 根据亮度值对场景做色调映射，并将映射结果输出到通道精度低的渲染目标上

由于有些硬件处理16或32位浮点数据的效率远低于处理8位浮点数据，所以我们需要对HDR渲染结果进行特殊的编码，以此在8位分量的渲染目标中储存更高精度的渲染结果。

RGBM是一种经典的HDR编码方式，其中M表示shared multiplier。一般来说，在伽马工作流中，M取值范围为$[0,5]$，在线性工作流中则是$[0,5^{2.2}]$。下面是使用RGBM编码对HDR进行加码和解码的函数，也是Unity所使用的函数。

~~~hlsl
fixed4 UnityEncodeRGBM (half3 color, float maxRGBM)
{
	//计算了maxRGBM的倒数
	float kOneOverRGBMMaxRange = 1.0 / maxRGBM;
	const float kMinMultiplier = 0.02;
	
	//将color的rgb值分别除以maxRGBM，然后取得商中的最大值alpha
	float3 rgb = color * kOneOverRGBMMaxRange;
	float alpha = max(max(rgb.r, rgb.g), max(rgb.b, kMinMultiplier));
	
	//取得大于alpha的能被1/255整除的最小数，其中alpha不小于0.02
	alpha = ceil(alpha * 255.0) / 255.0;
	alpha = max(alpha, kMinMultiplier);
	
	//通过rgb除以alpha，可以将rgb尽可能广地映射到[0,1]的范围内，以此尽可能地保存了精度
	return fixed4(rgb / alpha, alpha);
}

inline half3 DecodeHDR(fixed4 data, half4 decodeInstructions)
{
	//decodeInstruction的w分量表示是否考虑HDR纹理中的alpha，如过w为0，则alpha始终为1
	half alpha = decodeInstructions.w * (data.a - 1.0) + 1.0;
	
#if defined(UNITY_COLORSPACE_GAMMA)
	return (decodeInstructions.x * alpha) * data.rgb;
#else
	#if defined(UNITY_USE_NATIVE_HDR)
		return decodeInstructions.x * data.rgb;
	#else
		return (decodeInstructions.x * pow(alpha, decodeInstructions.y)) * data.rgb;
	#endif
#endif
}
~~~

一种适合移动平台的HDR格式被称为**双重低动态范围(double low dynamic range, dLDR)**，即将[0,2]范围内的亮度映射到[0,1]内，然后将大于2的亮度值截取到2。dLDR通过牺牲一半的精度，实现了在不增加缓冲区大小的情况下对bloom等效果的适配。在解码dLDR时，如果当前使用的是伽马工作流，则解码时将源纹理的RGB值乘以2；如果使用的是线性工作流，则在解码时将源纹理的RGB乘以$2^{2.2}$。

##### 什么是色调映射？

所谓色调映射，就是利用显示器有限的亮度范围来近似的显示HDR的亮度的技术。换句话说，**色调映射就是将亮度线性映射到[0,1]的范围，并压缩到每通道8位的渲染目标上**。

进行色调映射前，需要对整个图像的亮度进行统计，得到每个像素的亮度$L_w(x,y)$和整个图像的最亮像素亮度$L_{max}$。在HDR技术中，亮度值的大小可能超过1。

由于我们知道人眼对光的感知是非线性的，直观的归一化方法有**对数映射法**：
$$
L_d(x,y)=\frac{\lg(1+q*L_w(x,y))}{\lg(1+k*L_{max})}
$$
其中，$q,k\in [1,+\infin)$是使用者定义的参数。类似的还有**指数映射法**：
$$
L_d(x,y)=1-\exp(-\frac{q*L_w(x,y)}{k*L_{max}})
$$
可惜，这些算法过于简单粗暴，以致于在遇到动态幅度较大的图像时会丢失较多的细节，导致图像过于明亮或过于昏暗，使图像不自然Ferwerda提出了一种**视觉适应算法(Ferwerda算法)**，通过对人的视觉系统进行数学建模，构建了分段函数来实现对视感细胞$T_p$和视锥细胞$T_s$的仿真：
$$
\log_{10}T_p(x)=\left\{\begin{split}-0.72&\ \ \ \ \log_{10}x\le-2.6\\
(0.249\log_{10}x+0.65)^2-0.72&\ \ \ \ -2.6\lt\log_{10}x\lt1.9\\
\log_{10}x-1.255&\ \ \ \ \log_{10}x\ge1.9
\end{split}\right.\\\log_{10}T_s(x)=\left\{\begin{split}-0.86&\ \ \ \ \log_{10}x\le-3.94\\
(0.405\log_{10}x+1.6)^{2.18}-2.86&\ \ \ \ -3.94\lt\log_{10}x\lt-1.44\\
\log_{10}x-0.395&\ \ \ \ \log_{10}x\ge-1.44
\end{split}\right.
$$
接下来使用一个线性方程来对光照进行重建：
$$
\begin{bmatrix}R_d(x)\\G_d(x)\\B_d(x)\end{bmatrix}=m_c(L_{da},L_{wa})\begin{bmatrix}R_w(x)\\G_w(x)\\B_w(x)\end{bmatrix}+m_r(L_{da},L_{wa})\begin{bmatrix}L_w(x)\\L_w(x)\\L_w(x)\end{bmatrix}
$$
其中$L_{da}$表示显示器的亮度显示范围，$L_{wa}$表示原始图像的亮度范围。函数$m_c$和$m_r$为：
$$
m_r(L_{da},L_{wa})=\frac{T_p(L_{da})}{T_p(L_{wa})},m_c(L_{da},L_{wa})=\frac{T_s(L_{da})}{T_s(L_{wa})}
$$
Ferwerda算法提供了在暗环境中的优秀适应能力，但由于其动态范围是一个简单的线性缩放，所以无法达到更好的动态压缩范围。

前面介绍的色调映射算法都属于全局色调映射算法，即对全局的亮度进行统一的线性变换。为了实现更好的映射效果，接下来介绍一种局部色调映射算法，由于人眼的视觉具有局部性，通过提高相邻像素而非整个图像的对比度，可以有效的保留图像的细节。

一种较为先进的实现方案是Reinhard的**摄像学色调重现算法(Reinhard算法)**，这种算法模拟了摄影技术中使用了超过一个世纪的**Burning&Dodge效应 (减淡与加深效应，简称D&B)**，这个算法首先通过对数平均数$\overline L_w$主观地确认了场景的明亮程度：
$$
\overline L_w=exp(\frac{\sum_{x,y}\log(\epsilon+L_w(x,y))}{N})
$$
其中$\epsilon$是最小的正浮点数，它的作用是避免在计算黑色像素时遇到$\log0$的问题。

接下来选定一个目标曝光度$\alpha$，它决定了场景的整体明暗程度。使用下面的公式获得缩放因子$L_m$：
$$
L_m(x,y)=\frac\alpha{\overline L_w}L_w(x,y)
$$
为了尽可能保留某些场景的大曝光，Reinhard算法选择了一个相对柔和的函数来对超出高光范围的部分进行压缩：
$$
L_d(x,y)=\frac{L_m(x,y)(1+L_{white}^{-2}L_m(x,y))}{1+L_m(x,y)}
$$
其中$L_{white}$表示会被映射为白色的亮度阈值，默认情况下会将它设定为图片中的最亮像素亮度值，$L_{white}$作为截断值，会将亮度超过该值的像素都截断为$L_{white}$。

基于Reinhard算法的逻辑，我们可以定义一个局部色调映射算法。这个算法的关键在于，找到一个边界范围R，使得对任意像素(x,y)，在它附近范围R内的所有像素的亮度差异都足够小，以此避免在分别处理每个区域的对比度时产生传统局部色调映射算法的光晕效应。所以我们把对一个像素半径为$r$的高斯模糊得到的像素亮度称为$L_r^{blur}(x,y)$。接下来，我们通过比对，来寻找使得公式$V_r=\frac{|L_r^{blur}-L_{r+1}^{blur}|}{2^\phi\alpha r^{-2}+L_r^{blur}}$最小的参数$r_{max}$，其中$\alpha$是目标曝光度，$\phi$是锐化参数。随后得到Reinhard局部色调映射算法：
$$
L_d(x,y)=\frac{L_m(x,y)}{1+L_{r_{max}}^{blur}}
$$
HDR的使用可以允许我们在屏幕后处理中拥有更多控制权。以Bloom效果(见**屏幕后处理-如何实现Bloom效果？**)为例，我们需要通过一个阈值来截取屏幕中”偏亮“的像素点，如果使用LDR，我们就必须使用一个不大于1的定点数来描述亮度，这导致屏幕上一些偏白的普通区域也会被记入高光，但实际上我们需要的只是如车窗上对太阳的反光那样真正亮的区域，HDR的存在可以替我们解决这一问题。

HDR的问题在于，需要的显存空间会增加数倍，渲染速度明显变慢。很多硬件将HDR需要的额外缓冲区与MSAA或SSAA共用，这使得在开启HDR时硬件就会失去对硬件抗锯齿功能的支持。

#### 色彩运算

##### 如何对颜色进行修正？

+ **调整色相**

前面我们已经利用拮抗原理和HCL色彩空间为色相的调整做好了铺垫，将RGB转换到HCL色彩空间，然后调整H即可，这样可以得到最稳定的色相变化。

当然，在HSV空间中调整H也可以实现色相变化，但可能不是特别符合人眼直觉。

+ **调整亮度**

调整亮度即调整HSV中的V(或HSL中的L)，表现在伽马色彩空间中是对RGB增加一个变化值k：
$$
\begin{bmatrix}R'\\G'\\B'\end{bmatrix}=\begin{bmatrix}R+k\\G+k\\B+k\end{bmatrix}
$$
调整亮度会使整体层次感明显下降，这是因为亮度的变化没有符合人眼对亮度的非线性感知。

+ **调整曝光度**

调整曝光度时，饱和度和亮度同时变化，表现在伽马色彩空间中是对RGB的叠乘：
$$
\begin{bmatrix}R'\\G'\\B'\end{bmatrix}=2^k\begin{bmatrix}R\\G\\B\end{bmatrix}
$$
调整曝光度不会影响图像的层次感。

+ **调整饱和度**

饱和度是指色彩的鲜艳程度，也称色彩的纯度。美术色彩学的定义中，饱和度是颜色与灰色的混合度，向纯色中加入灰色则饱和度下降。感性理解的话，饱和度增加就是使RGB中较强的分量更加强，较弱的分量更加弱。满饱和度的图像指只由R、G、B、C、M、Y六色以及不同亮度的灰色组成的图像。

首先我们判断RGB中强的分量有哪些，为此我们必须求出一个阈值。阈值的求法可以使用算术平均数、几何平均数或平方平均数等，通常使用算术平均数，我们记这个阈值为μ，同时令饱和度变换率为k。则饱和度的计算公式为：
$$
out_C=in_C+k*(in_C-\mu),C\in\{R,G,B\}
$$
PhotoShop中的饱和度算法比起平均值公式更复杂，首先计算HSL中的L与S：
$$
l=\frac12 (max+min)\\
s=\left \{ \begin{array}{c}
\frac{max-min}{2l},&l\le \frac{1}{2}\\
\frac{max-min}{2-2l},&l\le \frac12
\end{array}\right.
$$
根据饱和度变化方向分类讨论，如果饱和度变化量k>=0，求出中间量α：
$$
\alpha=\left \{ \begin{array}{c}
\frac1S-1,&k+S\ge 1\\\frac1{1-k}-1,&k+S\lt 1
\end{array}\right.\\
out_C=in_C+(in_C-L)\times \alpha,\ C\in\{R,G,B\}
$$
如果饱和度变化量k<0：
$$
out_C=L+(in_C-L)\times(1+k),\ C\in\{R,G,B\}
$$

+ **色调分离**

色调分离就是通过降采样，将接近的颜色转化为同一颜色。这一效果经常用在屏幕后处理中实现非真实渲染效果。

最简单的降采样算法是利用整数除法运算的截断：

```c++
R = R / 16 * 16;
G = G / 16 * 16;
B = B / 16 * 16;
```

这样就可以轻易的将相近的颜色截取到一起。类似的，只需要修改除数的大小就可以修改色阶的大小。

##### 如何调整画面对比度？

对比度指的是一幅图像中明暗区域**最亮的白和最暗的黑**之间不同亮度层级的测量，差异范围越大代表对比越大，差异范围越小代表对比越小。反应到图像编辑上，调整对比度就是扩大或缩小亮的点和暗的点的差异。关于伽马空间的问题我们在上一部分已经谈过了，在此我们给出一个伽马色彩空间下的线性调整公式：
$$
Out = Average + k\cdot(In-Average)
$$
我们知道了亮度的调节公式，将它转换为对RGB的操作，得到：
$$
C'=\frac {Out}{In}C
$$
对提供给美术人员的对比度调节接口来说，这个算法已经足矣，因为美术人员对处理效率并不在意，并且很在意操作的自由度。

但是对计算机自动化处理，尤其是实时处理来说，计算平均亮度是得不偿失的，因为我们更注重的是通过提高对比度突显一些因为灰度相近而被隐藏的信息，而不是保证灰度的精确性，而计算平均灰度对GPU来说是及其不友好的。在精确性和效率上这个公式都不足够。

如果照顾GPU的并行特性，就必须去掉计算平均值这样的数据依赖，这里给出一个无数据依赖的调整公式。它的效果或许不够稳定，但效率足够高。其中Threshold是灰度阈值参数，通过估计图片平均灰度来计算对比度。
$$
Out = In + k \cdot (In-Threshold)
$$
而如果不考虑GPU的并行性，我们可以使用**直方图均衡化**实现效果更好且鲁棒性更强的对比度增强算法：

我们通过统计学的方法得到某图像的灰度分布直方图，在绝大部分自然图像中，灰度都聚集在一个很小的范围内，一般在[90,180]的范围左右，我们设原图A的灰度分布函数为H<sub>A</sub>(D)。我们希望对每个像素上的颜色进行变换，使其最终整个图像的灰度能在伽马色彩空间的[0,255]线性均匀分布，换句话说，就是进行一个f : R->R映射D<sub>B</sub>=f(D<sub>A</sub>)，对每个像素点施加一个变换，将图像A变为图像B，新的灰度分布函数为H<sub>B</sub>(D)。现在我们尝试求函数f：

由于D<sub>B</sub>与D<sub>A</sub>满足函数关系，显然有：
$$
D_B=f(D_A)\\D_B+\Delta D_B=f(D_A+\Delta D_A)
$$
为了使D<sub>B</sub>与D<sub>A</sub>在对应的区段上包含相同数量的像素，我们给出：
$$
\int_0^{D_A}H_A(D)dD=\int_0^{D_B}H_B(D)dD
$$
理想的H<sub>B</sub>(D)应使像素在不同灰度梯度线性均匀分布，所以有：
$$
H_B(D)=\frac {A_0}{L}
$$
其中A<sub>0</sub>为像素点总数，L为灰度梯度，一般为256。将H<sub>B</sub>(D)代入微分方程得：
$$
\int_0^{D_A}H_A(D)dD=\frac{A_0D_B}L=\frac{A_0f(D_A)}L
$$
轻易解得：
$$
f(D_A)=\frac L{A_0}\int_0^{D_A}H_A(D)dD
$$
将其改写成适合计算机得离散形式得：
$$
f(D_A)=\frac L{A_0}\sum_{i=0}^{D_A}H_A(i)
$$
这个公式已经足以适应绝大部分的图像，但问题也很明显——我们的推导是在H<sub>A</sub>(D)为连续分布函数的假设基础上进行的，而实际上H<sub>A</sub>(D)是一个离散函数。在大部分情况下，目标直方图会是近似均匀分布的，但假若A图像的灰度直方图变化剧烈，且某些灰度区间不存在像素点，这会造成B图像的直方图也有较大的不均匀性。

解决A直方图变化剧烈的方法之一是**削峰填谷**，设定一个阈值，假定直方图某个灰度级的像素数或占总像素比例超过了阈值，就对之进行裁剪，将超出阈值的部分平均分配到各个灰度级。这个思想可以用下图来理解：

![削峰填谷](Textures\削峰填谷.png)

将削峰填谷的思想公式化的表达如下：
$$
P(D_A)=\min(0,H_A(D_A)-Threshold)\\H_{A'}(D_A)=\max(H_A(D_A),Threshold)+\frac{\int_0^L P(D)dD}{L}\\f(D_A)=\frac L{A_0}\sum_{i=0}^{D_A}H_{A'}(i)
$$
另一种方法是根据灰度级进行均衡化，将灰度级划分为几个灰度区间，使每个区间内的像素数量大致相等，然后分别在每个区间做直方图均衡化，最后合并。

最后，**CLAHE插值法**还提出了将图像分割成多个窗口的局部直方图均衡化算法。首先，将图像分块，每块称为窗口，每个窗口计算一个直方图**累积分数函数**(指前文中的f(D<sub>A</sub>))。对于图像的每一个像素点，找到其邻近的四个窗口，分别计算四个窗口直方图累积分数函数的映射值，分别记作f<sub>ul</sub>、f<sub>ur</sub>、f<sub>bl</sub>、f<sub>br</sub>，进行双线性插值得到最终该像素点的映射值。**双线性插值(BiLinear)**公式为：
$$
f(D)=(1-\Delta y)((1-\Delta x)f_{ul}(D)+\Delta xf_{bl}(D))+\Delta y((1-\Delta x)f_{ur}(D)+\Delta xf_{br}(D))
$$

##### 如何调整画面白平衡？

白平衡是描述显示器中红、绿、蓝混合后产生的白色的精准度的指标。把一张白纸放在不同光线下会出现不同的色调，中午太阳光下会更白，而在黄昏下会偏黄。调整白平衡就是要将偏黄的白纸重新调整为白色。

调整白平衡由调整色温实现，色温就是光线在不同能量下，人眼感受到的能量变化。色温低时是红色，随着色温的逐渐增加变为橙色、黄色、白色、青色，而色温最高时表现为蓝色。换句话说，在偏红色调下人眼感受到的能量较低；在偏蓝色调下人眼感受到的能量较高。

主流白平衡算法有三种：**灰度世界法GW**、**全反射法PR**和**色温估计法**。

+ **灰度世界法GW**

  这个算法基于这样的一个假设：一副有足够多色彩变化的图像，其RGB三个通道的均值应该趋向于相等：
  $$
  \overline R=\overline G = \overline B
  $$
  所以在这个算法中我们先求出三个通道的均值的均值，然后调整整个图像使三个通道的均值相等，也就是满足上面的公式：
  $$
  K=\frac{\overline R+\overline G+\overline B}3\\
  R'=\frac K{\overline R}R\\G'=\frac K{\overline G}G\\B'=\frac K{\overline B}B
  $$

+ **全反射法PR**

  不再用通道的均值的均值作为目标色，而是直接对三个通道的分量进行预设比例的缩放：
  $$
  R'=\frac{R_W}{R_{max}}R\\G'=\frac{G_W}{G_{max}}G\\B'=\frac{B_W}{B_{max}}B
  $$
  其中$R_W、G_W、B_W$是预设的在调节完后每个通道最亮的值。

+ **GW和PR的正交组合算法QCGP**

  通过一个系数决定GW和PR两个算法的贡献，然后将它们叠加起来：
  $$
  K_{ave}=\frac{R_{ave}+G_{ave}+B_{ave}}3\\
  K_{max}=\frac{R_{max}+G_{ave}+B_{ave}}3\\
  u_rR^2_{ave}+v_rR_{ave}=K_{ave}\\
  u_rR^2_{max}+v_rR_{max}=K_{max}\\
  R'=u_rR^2+v_rR^2
  $$
  其中$u_r$和$v_r$可以通过前四条等式解出，然后带入到第五条等式得到输出的红色通道值。绿色和蓝色通道同理。

+ **色温估计法**

  这个算法需要将图像转换到YCbCr空间，复习RGB到YCbCr的转换公式：
  $$
  Y=0.299*R+0.587*G+0.144*B\\
  Cb=-0.1687*R-0.3313*G+0.5*B+128\\
  Cr=0.5*R-0.4187*G-0.0813*B+128
  $$
  第一步进行色温估计，这步希望找到一些像素将它们归为白色，判断依据是：
  $$
  -\gamma<Cb+Cr<\gamma\\
  Y-|Cb|-|Cr|>\phi
  $$
  这个公式筛选出了色差较小且亮度较高的像素，称它们为准白色像素。接下来我们遍历整个图像中的所有准白色像素，求出它们的Cb的绝对值的平均值$\overline {|Cb|}$和它们的Cr的绝对值的平均值$\overline {|Cr|}$，判断应该对整个图像调整红色通道还是蓝色通道：当$\overline {|Cb|}$较大且$\overline{Cb}$小于0时，调大整幅图像的R通道；当$\overline {|Cb|}$较大且$\overline{Cb}$大于0时，调小整幅图像的R通道；当$\overline {|Cr|}$较大且$\overline{Cr}$小于0时，调大整幅图像的B通道；当$\overline {|Cr|}$较大且$\overline{Cr}$大于0时，调小整幅图像的B通道。重复进行若干次校正，直到达到迭代次数或图像的平均色差足够少。

##### 有哪些颜色混合算法？

在本节中出现的A表示下层图层的像素每个通道值(遍历RGB三个通道)，B表示上层图层的像素每个通道值，C表示新图像的像素每个通道值。

+ **Darken 变暗：**

  对每个像素的每个通道比较上下两个图层，取通道数值较小的值作为混合像素该通道的值。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.3, 0.5, 0.3)

+ **Lighten 变亮：**

  对每个像素的每个通道比较上下两个图层，取通道数值较大的值作为混合像素该通道的值。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.5, 0.8, 0.8)

+ **Multiply 正片叠底：**

  C=AB。其效果可以形容成：两个幻灯片叠加在一起然后放映，透射光需要分别通过这两个幻灯片，从而被削弱了两次。由于符合美术中色彩混合的特点，是最常用的混合方式。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.15, 0.4, 0.24)

+ **Screen 滤色：**

  对两个RGB向量取反色，相乘，再取反色，即C=1-(1-A)(1-B)，其中1-X为X的反相。其效果可以形容成：两台投影机分别对两个图层进行投影后，然后投射到同一个屏幕上。常用于混合多个光照。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.65, 0.9, 0.86)

+ **Color Burn** **颜色加深：**

  C = A - (1-A)(1-B) / B。常用于实现阴影。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0, 0.375, 0.333)

+ **Color Dodge 颜色减淡：**

  C = A + (AB)/(1-B)。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.6, 1.0, 1.0)

+ **Liner Burn 线性加深：**

  C = A + B - 1。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0, 0.3, 0.1)

+ **Liner Dodge 线性减淡：**

  C = A + B。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.8, 1.0, 1.0)

+ **Overlay 叠加：**

  A<=0.5，则 C=2AB，否则 C=1-2(1-A)(1-B)。依据下层色彩值的差别，动态的决定使用Multiply或Screen，而上层决定了下层中间色调偏移的强度。如果上层为50%灰，则结果将完全为下层像素的值。如果上层比50%灰暗，则下层的中间色调的将向暗地方偏移，下层中间色调以下的色带变窄，中间色调以上的色带变宽。如果上层比50%灰亮，则下层的中间色调的将向亮地方偏移，下层中间色调以下的色带变宽，中间色调以上的色带变窄。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.15, 0.8, 0.72)

+ **Hard Light 强光：**

  B<=0.5，则 C = 2AB，否则 C = 1-2(1-A)(1-B)，相当于叠加模式上下两个图层次序交换的情况。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.3, 0.8, 0.24)

+ **Soft Light 柔光：**

  B<=0.5，则C = 2AB+(A^2)(1-2B)，否则C = 2A(1-B)+(A^0.5)(2B-1)。该模式将B作为遮罩，对A进行了Gamma值范围0.5到2.0的近似伽马校正，结果将是非常柔和的色彩组合。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.3, 0.544, 0.736)

+ **Vivid Light 亮光：**

  B<=0.5，则C=A-(1-A)(1-2B)/(2B)，否则C=A+A(2B-1)/(2(1-B))。该模式非常强烈的增加了对比度，特别是在高亮和阴暗处。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.3, 1.0, 0.667)

+ **Linear Light 线形光：**

  C=A+2B-1。和Linear Burn类似，只不过是加深了上层的影响力。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.3, 1.0, 0.4)

+ **Pin Light 点光：**

  B<=0.5，则C=min(A,2B)，否则C=min(A,2B-1)。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.3, 0.5, 0.6)

+ **Differences 差值：**

  C=|A-B|，用于得出两个像素的差异，差异越大越亮。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.2, 0.3, 0.2)

+ **Exclusion 排除：**

  C=A+B-2AB，亮的图片区域将导致另一层的反相，暗的区域导致另一层完全没有变化。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.5, 0.5, 0.62)

+ **Hue 色相：**

  H<sub>C</sub>S<sub>C</sub>V<sub>C</sub> =H<sub>B</sub>S<sub>A</sub>V<sub>A</sub>，输出图像的饱和度为上层，色调和亮度保持为下层。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.5, 0.8, 0.3)

+ **Color 颜色：**

  H<sub>C</sub>S<sub>C</sub>V<sub>C</sub> =H<sub>B</sub>S<sub>B</sub>V<sub>A</sub>，输出图像的亮度为下层，色调和饱和度保持为上层。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.5, 0.8, 0.3)

+ **Luminosity 亮度**

  H<sub>C</sub>S<sub>C</sub>V<sub>C</sub>=H<sub>A</sub>S<sub>A</sub>V<sub>B</sub>，输出图像的亮度为上层，色调和饱和度保持为下层。

  A(0.3, 0.5, 0.8) mix B(0.5, 0.8, 0.3) = C(0.3, 0.5, 0.8)

##### 有哪些透明度混合算法？

+ **简易透明度混合：**
  $$
  RGB =\left \{ \begin{array}{c}
   \frac{RGB1 + RGB2}2 &\alpha=0.5\\
   RGB1+\frac{RGB2-RGB1}\alpha&\alpha<0.5\\
   RGB2+\frac{RGB1-RGB2}\alpha&\alpha>0.5
  \end{array}\right.\tag{14}
  $$
  这个算法的特点是没有明确的上下层，α值更像是两个图层的混合参数而非某个图层的不透明度。

+ **加权透明度混合：**
  $$
  RGB = \frac{\alpha_1 *RGB1+\alpha_2*RGB2}{\alpha_1+\alpha_2}
  $$
  这个混合效果是将RGB1以α的程度混合到RGB2中，于透过RGB1看RGB2效果不同，更像是将两种颜色按比例α调和了起来，属于颜色混合的带参数扩展。

+ **双透明度混合：**
  $$
  RGB=(RGB1*\alpha_1+RGB2*\alpha_2*{(1-\alpha_1)})\\
  \alpha=1-(1-\alpha_1)*(1-\alpha_2)
  $$
  其中，RGB1、α1是前景颜色值，RGB2、α2是背景颜色值，α是新图片的不透明度。这个算法可以模拟两个半透明像素混合时生成新的半透明像素的效果。

+ **经典透明度混合：**
  $$
  RGB=\alpha*RGB1+(1-\alpha)*RGB2
  $$
  假设RGB1是透明图像，RGB2是不透明图像，α是RGB1某像素的不透明度，这个混合模式可以模拟透过RGB1观察RGB2的效果。由于渲染过程中，透明图像总是叠加在不透明图像前方，所以这个算法是最常用的透明度算法。

<div STYLE="page-break-after:always;"></div>

### 噪声原理

#### 随机数

##### 数列随机数是如何生成的？

随机数在计算机领域一直不可忽视，在图形学领域更是如此。我们在计算机领域用到的随机数一般被称为伪随机数，严格意义上是可预测的，但只需要其分布方式难以以寻常逻辑推理，也难以以第二台计算机进行预测，就可以被认为是随机数。

数列随机数是最早期的随机数算法之一，数列的首项被称为**种子(Seed)**，通过某种迭代算法，让数列中的两项之间的关系难以预测，得到随机数数列。在伪随机数算法中固定的种子会生成固定的随机数序列，如果希望序列本身也不可预测，可以将系统时间和内存使用量等用户难以捉摸的量作为种子。

目前的数列随机数生成算法一般以**线性同余法**为主，除此之外**平方取中法**是一种在早期常见伪随机数算法，在此介绍这两种经典的算法。

**平方取中法**是冯诺依曼在1946年提出的，其基本思想为：将数列中的第a~i~项平方(假设a~i~为m位二进制数)，得到2m位二进制数，去中间部分的m位数作为下一项a~i+1~，由此产生一个伪随机数列，即：
$$
a_{i+1}=(a_i^2>>\frac m2)\%(1>>m)
$$
平方取中法计算较快，但该方法容易产生周期性明显的数列，而且在某些情况下计算到一定步骤后始终产生相同的数，或者产生的数字越来越小直至产生零。这导致平方取中法产生伪随机数数列时不能只依靠公式，必须在计算过程中加入更多变化因素。

**线性同余法**是目前应用最广泛的伪随机数算法，其思想是通过对前一个数进行线性运算并取模从而得到下一个数，即：$a_{i+1} = (a_i*b+c)\mod m$。线性同余法的最大周期是m，但一般情况下会小于m。要使周期达到最大，应该满足以下条件：

+ c和m互质；
+ m的所有质因子的积能整除b-1；
+ 若m是4的倍数，则b-1也是；
+ b，c，种子a~0~都比m小；
+ b，c是正整数

C标准库中的rand()函数定义如下：

~~~c
static long do_rand(unsigned long *value)
{
    long quotient, remainder, t;

    quotient = *value / 127773L;
    remainder = *value % 127773L;
    t = 16807L * remainder - 2836L * quotient;
    if (t <= 0)
    t += 0x7FFFFFFFL;
    return ((*value = t) % ((unsigned long)RANDOM_MAX + 1));
}
~~~

VC中rand()函数定义如下：

~~~VC
int __cdecl rand (void) 
{
	return(((holdrand = holdrand * 214013L + 2531011L) >> 16) & 0x7fff);   
} 
~~~

##### 函数随机数是如何生成的？

有时候我们并不想通过迭代法产生随机数，我们希望输入一个源码，经过某种函数映射后，得到难以预测的结果，这就是函数随机数。

我们最常见的函数随机数是正弦分散法，其基本思想为提取sin函数波形的分数部分，然后通过提高sin函数的振幅来将其进一步打散，我们可以在下图中看到一些例子：

<img src="Textures\函数随机数.png" alt="函数随机数" style="zoom:50%;" />

在函数随机数中，我们一般将正弦函数的振幅作为随机数的种子，这个种子必须足够大才能获得理想的效果。

在随机图表中，在$\pm 1.5707$处有明显的不和谐，我们应当尽量避免这两个地方。

##### 如何对随机数进行修饰？

我们希望修改随机数生成函数的概率密度函数，以此控制随机数的状态。为此我们考察使用传统的数列随机数或函数随机数生成的随机数频率分布直方图，可见传统随机数生成函数得到的是均匀分布的随机数：

<img src="Textures\均匀分布.gif" alt="均匀分布" style="zoom: 50%;" />

将均匀分布的传统随机数生成函数命名为函数rand()，我们将利用概率论相关原理，通过多次使用随机均匀分布函数并进行简单的数学运算来得到新的随机函数，这个函数应当具有不同的概率密度函数。

如果我们把两个随机数乘起来rand()*rand()，我们就可以得到反比分布的图形，随机数的结果更倾向于0：

<img src="Textures\Sqrt分布.gif" alt="Sqrt分布" style="zoom:50%;" />

如果我们对一个随机数取根号，即sqrt(rand())，我们可以得到线性分布曲线，随机数结果倾向于1：

<img src="E:/Textures/图形学/squareRoot.gif" alt="squareRoot" style="zoom:50%;" />

如果我们取max(rand(),rand())，得到的结果和上面几乎一致：

<img src="Textures\maxOf2.gif" alt="maxOf2" style="zoom:50%;" />

我们再取max(rand(),rand(),rand())，发现分布曲线趋近于x^2^曲线：

<img src="Textures\maxOf3.gif" alt="maxOf3" style="zoom:50%;" />

可见，对n个随机数取最大值，可以得到近似于x^n-1^的曲线。

将两个随机数相加并不等于2*rand()，我们用0.5f * (rand()+rand())生成倾向于中心的随机分布：

<img src="Textures\Gaussian2.gif" alt="Gaussian2" style="zoom:50%;" />

在概率论中我们知道，多个均匀分布的叠加可以得到贴近高斯分布的性质，如使用三个随机数相加可以得到：

<img src="Textures\Gaussian3.gif" alt="Gaussian2" style="zoom:50%;" />

最后我们获取一个逆高斯分布：

~~~
r = random() - random() + random() - random();
r += (r<0.0) ? 4.0 : 0.0;
r *= 0.25;
~~~

<img src="E:/Textures/图形学/modifiedGaussian.gif" alt="modifiedGaussian" style="zoom:50%;" />

#### 静态噪声

##### 什么是白噪声？

白噪声就是对图片中的每个像素取一个随机数，如：

~~~glsl
float whitenoise(vec2 uv)
{
    return fract(sin(dot(uv, vec2(12.9898,78.233)))*43758.5453123);
}
~~~

<img src="Textures\WhiteNoise.png" alt="WhiteNoise" style="zoom:67%;" />

白噪声和老式电视信号差时的形态类似，这也是二维随机数图被称为噪声图的原因。

我们可以通过修改坐标的格式来获得不同密度的噪声，比如说我们将坐标的小数部分约去，可以获得不那么密集的白噪声：

<img src="Textures\WhiteNoisePixel.png" alt="WhiteNoisePixel" style="zoom:67%;" />

如果希望每次生成不同的噪声图，可以将时间纳入随机数：

~~~glsl
float whitenoise(vec2 uv)
{
    return fract(sin(dot(uv, vec2(12.9898,78.233) + _Time.y))*43758.5453123)
}
~~~

##### 什么是抖动算法(Dither)?



##### 如何生成更平滑的噪声图？

白噪声还是过于刺眼，我们需要一些更平滑的噪声图，我们在白噪声的基础上得到**插值噪声**。

最直观的方法是**线性插值**，将坐标分为整数部分和小数部分，通过整数部分获得随机数，通过小数部分进行插值，如图所示：

~~~glsl
float i = floor(x); 
float f = fract(x); 
y = lerp(rand(i), rand(i+1), f);
~~~

<img src="Textures\线性插值噪声.png" alt="线性插值噪声" style="zoom:67%;" />

在图形学中，我们将整数值点称为**晶格**，这种噪声生成的算法也被称为基于晶格的噪声算法，这一概念我们还会继续谈到。

在线性插值的基础上，把插值系数改为余弦值，就实现了更平滑的**余弦插值**噪声：

~~~glsl
float i = floor(x); 
float f = fract(x); 
f *= 3.1415926;
f = (1 - cos(f)) * 0.5;
y = lerp(rand(i), rand(i+1), f);
~~~

<img src="Textures\余弦插值噪声.png" alt="余弦插值噪声" style="zoom:67%;" />

如果我们考虑更多的顶点，可以获得平滑级别更高的曲线，如使用三次多项式进行插值的**Cubic插值噪声**：
$$
f(v_0,v_1,v_2,v_3,x)=(-\frac12v_0+\frac32v_1-\frac32v_2+\frac12v_3)x^3+(v_0-\frac52v_1+2v_2-\frac12v_3)x^2+(-\frac12v_0+\frac12v_2)x+v_1
$$
即：

~~~glsl
float v0=rand(i-1.);
float v1=rand(i);
float v2=rand(i+1.);
float v3=rand(i+2.);
y=v1+f*(0.5*(v2-v0)+f*(0.5*(2.*v0-5.*v1+4.0*v2-v3)+f*(0.5*(-v0+3.0*v1-3.*v2+v3))));
~~~

另外一个Cubic插值公式如下：
$$
f(v_0,v_1,v_2,v_3,x)=((v_3-v_2)-(v_0-v_1))x^3+(2(v_0-v_1)-(v_3-v_2))t^2+(v_2-v_0)x+v_1
$$
<img src="Textures\三次插值噪声.png" alt="三次插值噪声" style="zoom:67%;" />

如果我们将整数随机点作为埃尔米特曲线或贝塞尔曲线的控制点，则由得到了埃尔米特插值噪声或贝塞尔插值噪声，其中由于贝塞尔曲线中段不经过控制点，可能导致随机数向中央聚集，所以埃尔米特插值噪声更常用。

讲完一维的插值噪声再讨论二维和三维的，二维插值噪声需要考虑周围4个点的插值，三维需要考虑周围8个点，而插值的方式则是对不同维度分别进行考虑，如下：

~~~glsl
float mix4(float a, float b, float c, float d, float2 u)
{
	mix2(mix2(a,b,u.x), mix2(c,d,u.x), u.y);
}
float mix8(float a, float b, float c, float d, float e, float f, float g , float h, float3 u)
{
	mix2(mix4(a,b,c,d,u.xy), mix4(e,f,g,h,u.xy), u.z);
}
~~~

其中mix2是前面提到的某种插值算法，u为输入点的小数部分，a、b、c、d、e、f、g、h分别为输入点周围的整数点的随机数值。如：

~~~glsl
float ValueNoise(float p)
{
    float2 i = floor(p);
    float2 f = floor(p);
    float a = rand(i);
    float b = rand(i + float2(1,0));
    float c = rand(i + float2(0,1));
    float d = rand(i + float2(1,1));
    return mix4(a,b,c,d,f);
}
~~~

##### 什么是柏林噪声？

柏林噪声是一个复杂的概念，因为Ken Perlin先生开发了不止一种噪声生成算法，而这几种算法一直以来都被人叫做Perlin Noise，这使得柏林噪声的指向并不明确。在此我们只讨论柏林先生发明的其中一种最经典的噪声生成算法————**梯度噪声(Gradient Noise)**。

在梯度噪声算法中，我们对晶格随机数的产生方式进行了一步优化：我们通过随机取值获得的不再是晶格点的值，而是晶格点的梯度向量。对一个二维柏林噪声图来说，我们需要随机生成一个二维的梯度向量。为了获得这样的随机向量，我们需要使用两个种子不同的随机数获得二维向量，然后将其单位化来获得归一的梯度向量。

那么我们已经将晶格值换为随机梯度了，接下来该怎么办呢？柏林使用目标点周围四个晶格点的梯度，和晶格点到目标点的向量的点乘值来进行埃尔米特插值，如图所示，紫色和绿色箭头为得到的随机梯度，而蓝色箭头是晶格点到目标点的向量。

<img src="Textures\柏林噪声.jpg" alt="柏林噪声" style="zoom: 67%;" />

~~~glsl
float2 rand2(float2 p)
{
    return normalize(float2(rand_a(p),rand_b(p)));
}

float noise(float2 p)
{
    float2 i = floor(p);
    float2 f = fract(p);
    
    float2 u = f*f*(3.0-2.0*f);//三次Hermit插值函数
    
    return lerp(
    	       lerp(dot(rand2(i + float2(0,0)), f - float2(0,0)),
                    dot(rand2(i + float2(1,0)), f - float2(1,0)), u.x),
               lerp(dot(rand2(i + float2(0,1)), f - float2(0,1)),
                    dot(rand2(i + float2(1,1)), f - float2(1,1)), u.x),
               u.y);
}
~~~

2002年，柏林先生提出将插值方法从三次Hermit函数$3x^2-2x^3$改为四次Hermit函数$6x^5-15x^4+10x^3$，仅需要改变u的赋值语句即可：

~~~glsl
float u = f*f*f*(10-15*f+6*f*f);
~~~

<img src="Textures\柏林噪声.gif" alt="柏林噪声" style="zoom:67%;" />

在noise(x*p)中，x被称为频率，因为x定义了噪声的密度。

三次柏林噪声的算法由二维柏林噪声和三维插值噪声类比即可推出，不再赘述。

##### 什么是噪声调制?

我们可以通过噪声调制函数来优化一个噪声图的效果，噪声调制可以应用在任意噪声类型上，下面以柏林噪声为例介绍几种常见的调制函数。在调制示意图中，左上角为原始的柏林噪声，左下角为分型调制的柏林噪声，右下角为湍流调制的柏林噪声，右上角为条形调制的柏林噪声。

<img src="Textures\柏林噪声实现.jpg" alt="柏林噪声实现" style="zoom: 67%;" />

+ **柏林噪声：**

  如调制示意图左上角所示，表现的最分散且差异最大。

+ **分型噪声：**

  对不同频率域进行一个**FDM调制(分形布朗运动，Fractal Brownian Motion)**得到分形噪声。FDM调制是最常用的噪声调制手段，可以有效的增加噪声的细节。

  分形是一个几何概念，是对自然界当中局部与整体的关系的一种总结，其描述为：“一个粗糙或零碎的几何形状，可以分成数个部分，且每一部分都（至少近似地）是整体缩小后的形状”。这种现象频繁出现在自然界的纹理、地形和云雾等不规则物体上。而布朗运动是一个物理概念，它描述了一种随机的定向运动。分形布朗运动是对这两个现象的模拟，通过将噪声图缩放成不同的大小(分形)，沿不同方向和不同速度移动(布朗运动)，然后按照不同的权值进行叠加混合，得到近似于自然界的随机纹理。

  综上所述，可以将FDM调制理解为把不同比例位置的若干张噪声进行混合。如调制示意图左下角所示，公式如下：
  $$
  noise(p)+\frac12noise(2p)+\frac14noise(4p)+\frac18noise(8p)...
  $$
  即每次采样频率翻倍，振幅减半，然后将这些采样叠加起来。分型噪声看起来更平滑和稳定了一些。适合模拟石头、山脉等物体。

  ~~~glsl
  float fractalNoise(float2 p)
  {
      float f = 0;
      p *= 4;//4为分型采样基频率
      f += 1.0000 * noise(p); p *= 2;
      f += 0.5000 * noise(p); p *= 2;
      f += 0.2500 * noise(p); p *= 2;
      f += 0.1250 * noise(p); p *= 2;
      f += 0.0625 * noise(p);
      
      return f;
  }
  ~~~

  很多时候我们还可以插值噪声代替柏林噪声进行fdm叠加，进而产生消耗更低的分型噪声。

+ **湍流噪声：**

  在分形噪声的基础上，**湍流噪声(turbulence noise)**对每个频率域叠加了绝对值：
  $$
  |noise(p)|+|\frac12noise(2p)|+|\frac14noise(4p)|+|\frac18noise(8p)|+...
  $$
  如调制示意图右下角所示，这使得噪声看起来更尖锐，适合模拟火焰、云朵等气体，以及尖锐的山峰。

  ~~~glsl
  float turbulenceNoise(float2 p)
  {
      float f = 0;
      p *= 7;
      f += 1.0000 * abs(noise(p)*2-1); p *= 2;
      f += 0.5000 * abs(noise(p)*2-1); p *= 2;
      f += 0.2500 * abs(noise(p)*2-1); p *= 2;
      f += 0.1250 * abs(noise(p)*2-1); p *= 2;
      f += 0.0625 * abs(noise(p)*2-1);
      
      return f;
  }
  ~~~

+ **条状噪声**：

  在湍流噪声的基础上，对其使用了一个关于表面x分量的正弦函数:
  $$
  \sin(x+|noise(p)|+|\frac12noise(2p)|+|\frac14noise(4p)|+|\frac18noise(8p)|+...)
  $$
  如调制示意图右上角所示，这个噪声适合模拟大理石材质和木纹等物体。我们可以给x乘上一个系数来控制条纹疏密。

  ~~~glsl
  float stripeNoise(float2 p)
  {
      float f = 0;
      p *= 7;
      f += 1.0000 * abs(noise(p)*2-1); p *= 2;
      f += 0.5000 * abs(noise(p)*2-1); p *= 2;
      f += 0.2500 * abs(noise(p)*2-1); p *= 2;
      f += 0.1250 * abs(noise(p)*2-1); p *= 2;
      f += 0.0625 * abs(noise(p)*2-1);
      f = sin(f + p.x/32);//32为条纹密度
      
      return f;
  }
  ~~~

##### 什么是单形噪声？

**单形噪声(Simplex Noise)**是对柏林噪声在性能上的优化。它也是基于晶格的梯度噪声，但它不使用正方形或正方体晶胞，而是使用等边三角形或正四面体晶胞。这也是**单形(Simplex)**的含义————在N维线性空间中最紧凑的多边形。

单形噪声的过程和柏林噪声一致，所以我们更关心的时如何方便的获得顶点附近的晶格坐标。

<img src="Textures\单形噪声.jpg" alt="单形噪声" style="zoom: 50%;" />

获得晶格坐标的方法被称为**坐标偏斜法(Skewing)**，我们将在二维平面中由等边三角形组成的坐标系表示成如图中红色网格的样式，通过坐标倾斜将其变成后面黑色网格的样子：

![坐标偏斜](Textures\坐标偏斜.jpg)

公式如下：
$$
x'=x+(x+y+...)\cdot K_n\\
y'=y+(x+y+...)\cdot K_n\\......
$$
其中$K_n=\frac{\sqrt{n+1}-1}{n}$，则二维平面中$K_2=\frac{\sqrt3-1}2$。

这个变换函数的逆函数为：
$$
x=x'+(x'+y'+...)\cdot K'_n\\
y=y'+(x'+y'+...)\cdot K'_n\\
......
$$
其中$K'_n=\frac{\frac{1}{\sqrt{n+1}}-1}{n}$。

这样我们就可以将目标点坐标进行偏斜然后向下取整得到点所在的超立方体的坐标(在二维平面中则是正方向)。
$$
x_i=floor(x'),y_i=floor(y')...
$$
然后通过小数部分进一步获得其所在的单形晶胞，这个过程称为**单形分割**。

>  n维单形具有n+1个顶点，n维单形分割算法的描述为：先将经过偏斜后的点的n个分量按顺序排列，从零坐标(0,0,...,0)开始，依次取当前最大的分量，在该分量位置加1，直至添加所有分量。例如，对于二维空间来说，如果xf,yfxf,yf满足xf>yfxf>yf，那么对应的3个单形坐标为：首先找到(0, 0)，由于x分量比较大，因此下一个坐标是(1, 0)，接下来是y分量，坐标为(1, 1)；对于三维空间来说，如果xf,yf,zfxf,yf,zf满足xf>zf>yfxf>zf>yf，那么对应的4个单形坐标位：首先从(0, 0, 0)开始，接下来在x分量上加1得(1, 0, 0)，再在z分量上加1得(1, 0, 1)，最后在y分量上加1得(1, 1, 1)。这一步的算法复杂度即为排序复杂度O(n2)O(n2)。

找到对应单形后，我们找到该单形上各个顶点的伪随机梯度向量，然后按照和柏林噪声类似的算法计算目标点的噪声值。我们先把在超立方体坐标下的晶格顶点使用前面提到的逆函数变回到单形网格坐标中，然后与目标点坐标相减得到位移坐标。

插值算法中每个晶格顶点对目标点的贡献为：
$$
(r^2-|dist|^2)^4\times dot(dist,grad)
$$
其中grad为晶格的随机梯度向量，dist为晶格顶点到目标点的距离向量，而r^2^为系数，一般取0.5或0.6。

单形噪声实现的代码如下：

~~~glsl
float simplexNoise(float2 p)
{
    const float K1 = 0.366025404; // (sqrt(3)-1)/2;
    const float K2 = 0.211324865; // (3-sqrt(3))/6;

    float2 i = floor(p + (p.x + p.y) * K1);

    float2 a = p - (i - (i.x + i.y) * K2);
    float2 o = (a.x < a.y) ? float2(0.0, 1.0) : float2(1.0, 0.0);
    float2 b = a - o + K2;
    float2 c = a - 1.0 + 2.0 * K2;

    float3 h = max(0.5 - float3(dot(a, a), dot(b, b), dot(c, c)), 0.0);
    float3 n = h * h * h * h * float3(dot(a, hash22(i)), dot(b, hash22(i + o)), dot(c, hash22(i + 1.0)));

    return dot(vec3(70.0, 70.0, 70.0), n);
}

~~~

对单形噪声使用和柏林噪声一样的变形算法，可以获得如下示意图，由于调制算法没有变化所以这里没有给出代码：

<img src="Textures\单形噪声实现.jpg" alt="单形噪声实现" style="zoom:67%;" />

##### 什么是Voronoi噪声？

**Voronoi噪声**也被称为**Worley噪声**或**Cellular噪声**。Voronoi噪声有别于前面的几种噪声，它不是基于晶格的噪声生成算法。

Voronoi噪声随机生成特征点，然后每个像素点取它与最近的特征点之间的距离。

![Voronoi](Textures\Voronoi.png)

举一个简单一些的例子，加入我们只有四个特征点：

~~~glsl
float Voronoi(float2 p)
{
    float2 Rpoints[4];
	Rpoints[0] = float2(0.83, 0.75);
	Rpoints[1] = float2(0.60, 0.07);
	Rpoints[2] = float2(0.28, 0.64);
	Rpoints[3] = float2(0.31, 0.26);
	float m_dist = 1;
	for (int i = 0; i < 4;i++) 
    {
		m_dist=min(m_dist, distance(p, Rpoints[i]));
	}
	return m_dist;
}
~~~

根据这个逻辑我们理论上已经可以得到任意的Voronoi图了，可是通过遍历的算法求特征点在特征点密集度大的情况下就是灾难。Worley对此的解决方法是，把空间分割成网格，每个格子只生成一个特征点，那么像素就只需要在自己及相邻的八个格子内寻找特征点了，这样任意密度的Voronoi图都只需要对每个像素计算9次距离了。

![Worley](Textures\Worley.png)

##### Voironoi噪声有哪些变体？

我们使用基于欧几里得距离的算法获得了最小距离，这使得Voronoi噪声每一个胞体是一个偏圆滑的形态，但我们有时希望胞体为边缘更硬直的多边形，这种形态的Voronoi噪声称为**Boarder Voronoi**噪声。

一个简单的做法是不取最小距离而取第二小距离和第一小距离的差，这个做法得到的效果如下：

![BorderVoronoi](Textures\BorderVoronoi.png)

但这个做法虽然得到了边缘硬直的多边形，却导致多边形内部失去了均匀性。

我们希望噪声的强度与目标点距离胞体边界线的距离相关，为此我们分析两个胞体之间的边界：

![QuileVoronoi](Textures\QuileVoronoi.jpg)

图中x为目标点位置，a是离x最近的特征点，b是离x第二近的特征点，m是a与b的中点，过m做ab的垂线，这条线就是两个胞体之间的边界线。紫色的线的长度就是x离边界线的距离，可以通过取$\vec{mx}$在$\vec {ab}$上的投影获得，即：
$$
dist = (\vec x - \frac{a+b}{2})\cdot normalize(\vec b - \vec a)
$$

~~~glsl
float worleyNoise(float2 uv) 
{
	float2 i = floor(uv);
	float2 f = frac(uv);
	float m_dist = 8;
	float2 mp;
	float2 mr;
	//正常的求最短距离
	for (int x = -1; x < 2; x++)
		for (int y = -1; y < 2; y++) 
    	{
			float2 neighbor = float2(x, y);
			float2 neighborP = random(i + neighbor);
			float dist = distance(f, neighborP + neighbor);
			if (dist < m_dist.x) 
	        {
				//最短距离
				m_dist.x = dist;
				//最近的特征点网格，下面要用
				mp=neighbor;
				//最近点和像素点的距离向量，下面要用
				mr = neighborP + neighbor - f;
			}
		}
	//求边缘距离 寻找(距离(最近特征点)最近的特征点) 5x5
	float borderDis = 8;
	for (int x = -2; x < 3; x++)
	for (int y = -2; y < 3; y++) 
    {
		float2 neighbor = float2(x, y);
		/mr=最近点和像素点距离，r=第二近和像素点距离，mr+r=2mx r-mr=ab
		float2 neighborP = random(i+mp + neighbor);
		float2 r = mp + neighborP + neighbor - f;
		if (dot(mr - r, mr - r) > 0.00001) //排除xy=0的情况
        {
			borderDis = min(borderDis, dot(0.5*(mr + r), normalize(r - mr)));
		}
	}
	return borderDis;
}
~~~

可以看到效果比起前面好了不少。

![BorderVoronoiNew](Textures\BorderVoronoiNew.png)

#### 动态噪声

##### 什么是快速傅里叶变换(FFT)？

以时间作为参照来观察动态世界的方法被称为时域，任何一种波动都自然有其时域波形图，即横坐标为时间，纵坐标为振幅的波形图。而傅里叶告诉我们，**任何连续周期信号都可以理解为常数、与若干频率和振幅不同的正弦波和余弦波的叠加**。于是，我们可以将任何一个时域波形图转化成另一种形式，通过统计这些正弦波的频率和振幅的大小来整合出完整的波形，这个以频率为横轴，以振幅为纵轴的波形图可以被称为频域波形图。而傅里叶变换就是一个将波动从时域变换到频域的工具。

![傅里叶变换](Textures\傅里叶变换.jpg)

> 图出自知乎用户Heinrich，微博@花生油工人

考虑一个周期为$T$，频率为$f=\frac1T$的周期函数$s_N(x)$，由于正弦和余弦函数的周期都为$2\pi$，所以为了保证这些三角函数的叠加结果的频率为$f$，可以将函数$s_N(x)$表示为：
$$
s_N(x)=C+\sum_{i=1}^n(A_n\sin(2\pi nfx)+B_n\cos(2\pi nfx))
$$
则傅里叶变换就可以理解为在给定$s_N(x)$的情况下，对$A_n、B_n、C$进行求解。

为了实现傅里叶变换，需要引入欧拉公式$e^{ix}=\cos x+i\sin x$。这个公式的内涵是，将乘以虚数$i$可以被理解为在复平面上进行了一次90°的旋转，同时用$e^{ix}$代表了复平面上的一个夹角为$x$的单位向量。我们引入时间变量$t$，得到$e^{it}$，则随着时间$t$的流逝，这个向量就会开始旋转，且每$2\pi$秒完成一圈。也就是说，我们可以通过$e^{iwt}$来表示任意速率的旋转。如果我们取$e^{iwt}$的虚部$\mathrm{Im}(e^{iwt})$，就可以得到$\sin(wt)$的波形图。

举一个例子，对于周期函数$g(t)=\sin(t)+\sin(2t)$，它对应的是函数$g(t)=\mathrm{Im}(e^{it}+e^{2it})$。也就是说，我们将若干个正弦波混合而成的波形看成若干个旋转的向量的矢量和在一轴上的投影。

由于我们知道正弦函数和余弦函数可以用$e^{iwt}$表示，而$e^{iwt}$可以表示复平面上的一个向量，所以正弦函数和余弦函数的线性加和也应当能表示复平面上的一个向量。又因为正弦函数和余弦函数的线性加和能表示所有的周期函数，所以所有的周期函数都能表示为复平面上的一个向量，这些向量被称为函数向量。既然是向量，那就存在向量间的几何关系。我们定义函数向量的点积为：
$$
f(x)\cdot g(x)=\int_0^Tf(x)g(x)\mathrm{d}x
$$
对函数向量$g(t)=\sin(t)+\sin(2t)$，根据函数向量的点积，有$\sin(t)\cdot\sin(2t)=\int_0^{2\pi}\sin(t)\sin(2t)\mathrm{d}t=0$，可知这两个函数向量正交，可以作为一组正交基。如果我们把函数改写为$g(t)=1\cdot\sin(t)+1\cdot\sin(2t)$，则可以理解为$g(t)$在一对正交基$\sin(t)$和$\sin(2t)$下的坐标为$(1,1)$。

现在我们已经发现，可以通过求解函数向量在正交基下的坐标来解出波的振幅。



##### 什么是波动粒子(Wave Particles)噪声？





<div STYLE="page-break-after:always;"></div>

## 几何数据

### 参数建模

#### 参数曲线

##### 什么是埃尔米特曲线？

**埃尔米特曲线(Hermit)**是一种插值曲线，也是样条曲线家族中最好理解的一种曲线。

Hermit曲线给定曲线两个端点的位置矢量$P_0$、$P_1$和切线矢量$T_0$、$T_1$，通过插值获得曲线在定义域[0,1]上的三次参数方程：

<img src="E:/Textures/图形学/埃尔米特示例.png" alt="埃尔米特曲线" style="zoom: 25%;" />

我们设目标参数方程为$P(t)=at^3+bt^2+ct+d$，则有$P'(t)=3at^2+2bt+c$，将0和1代入t易得：
$$
\left \{ \begin{array}{c}
\ P(0)&=&d\\\ P(1)&=&a+b+c+d\\\ P'(0)&=&c\\\ P'(1)&=&3a+2b+c
\end{array}\right.\tag{14}
$$
现在我们设$h_0=P(0),h_1=P(1),h_2=P'(0),h_3=P'(1)$，则有：
$$
\begin{bmatrix} 
h_0\\h_1\\h_2\\h_3
\end{bmatrix}=\begin{bmatrix} 
0&0&0&1\\1&1&1&1\\0&0&1&0\\3&2&1&0
\end{bmatrix}\begin{bmatrix} 
a\\b\\c\\d
\end{bmatrix}\\\Rightarrow\begin{bmatrix} 
a\\b\\c\\d
\end{bmatrix}=\begin{bmatrix} 
2&-2&1&1\\-3&3&-2&-1\\0&0&1&0\\1&0&0&0
\end{bmatrix}\begin{bmatrix} 
h_0\\h_1\\h_2\\h_3
\end{bmatrix}
$$
我们将参数方程$P(t)=at^3+bt^2+ct+d$写成矩阵形式：
$$
P(t)=\begin{bmatrix} 
a&b&c&d
\end{bmatrix}\begin{bmatrix} 
t^3\\t^2\\t\\1
\end{bmatrix}
$$
将列向量abcd的表达式转置，代入上式，得到：
$$
P(t)=\begin{bmatrix} 
h_0&h_1&h_2&h_3
\end{bmatrix}\begin{bmatrix} 
2&-3&0&1\\-2&3&0&0\\1&-2&1&0\\1&-1&0&0
\end{bmatrix}\begin{bmatrix} 
t^3\\t^2\\t\\1
\end{bmatrix}
$$
将后两项乘起来可以得到：

<img src="E:/Textures/图形学/Hermit.png" alt="Hermit" style="zoom:33%;" />
$$
P(t)=\sum_{i=0}^3h_iH_i(t)\\
$$
其中$h_1=P(0)=P_0$，$h_2=P(1)=P_1$，$h_3=P'(0)=T_0$，0$h_4=P'(1)=T_1$。

##### 什么是贝塞尔曲线？

贝塞尔曲线B(t)也是参数格式的曲线，n阶贝塞尔曲线是n次参数方程，一个四阶贝塞尔曲线的参数t在不同取值下的原理图如下，将t从0到1时B(t)返回的点连接起来，就可以得到贝塞尔曲线的图像：

![贝塞尔曲线原理](Textures\贝塞尔曲线原理.png)

贝塞尔曲线完全由其控制点决定其形状，n个控制点对应着n-1阶的贝塞尔曲线，并且可以通过递归的方式来绘制。贝塞尔曲线是典型的递归法曲线。

一阶贝塞尔曲线就是对两个控制点进行线性插值，其参数方程为：
$$
B_1(t)=P_0+(P_1-P_0)t=(1-t)P_0+tP_1,t\in[0,1]
$$
二阶贝塞尔曲线由三个控制点组成，假设三个控制点依次为P~0~、P~1~、P~2~，先分别对P~0~P~1~和P~1~P~2~两段进行线性插值得到两个新控制点P~0~'、P~1~'，将二阶贝塞尔曲线降为一阶：
$$
P'_0=(1-t)P_0+tP_1\\
P'_1=(1-t)P_1+tP_2
$$
然后再对P~0~'P~1~'求线性插值：
$$
B_2(t)=(1-t)P'_0+tP'_1=(1-t)^2P_0+2t(1-t)P_1+t^2P_2
$$
三阶贝塞尔曲线由四个控制点组成，假设四个控制点依次为P~0~、P~1~、P~2~、P~3~，先分别对P~0~P~1~、P~1~P~2~和P~2~P~3~三段求线性插值得到三个新控制点P~0~'、P~1~'、P~2~'，将三阶贝塞尔曲线降为二阶：
$$
P'_0=(1-t)P_0+tP_1\\
P'_1=(1-t)P_1+tP_2\\
P'_2=(1-t)P_2+tP_3
$$
然后再利用新控制点求二阶贝塞尔曲线：
$$
P''_0=(1-t)P'_0+tP'_1\\
P''_1=(1-t)P'_1+tP'_2
$$
最后对P~0~''和P~1~''进行线性插值：
$$
B_3(t)=(1-t)P''_0+tP''_1=(1-t)^3P_0+3t(1-t)^2P_1+3t^2(1-t)P_2+t^3P_3
$$
不难发现，n阶贝塞尔曲线展开式系数是n阶二项式展开系数，可以得到n阶贝塞尔曲线的通项公式：
$$
B_n(t)=\sum_{i=0}^n\frac{n!}{i!(n-i)!}t^i(1-t)^{n-i}P_i,\ t\in[0,1]
$$
其中，n阶贝塞尔曲线的第i项系数B~i,n~(t)又被记作**Bernstein基函数**。
$$
B_{i,n}(t)=\frac{n!}{i!(n-i)!}t^i(1-t)^{n-1}\\
B_n(t)=\sum_{i=0}^nB_{i,n}(t)P_i,\ t\in[0,1]
$$
对n阶贝塞尔曲线进行求导可得：
$$
B'_n(t)=\sum_{i=0}^{n-1}B_{i,n-1}(t)[n(P_{i+1}-P_i)],\ t\in[0,1]
$$
经观察，n阶贝塞尔曲线的导数显然是n-1阶贝塞尔曲线，其控制点为原曲线控制点的组合。

贝塞尔曲线具有**端点性质**：第一个控制点和最后一个控制点恰好是曲线的起点和重点。

贝塞尔曲线具有**凸包性质**：贝塞尔曲线始终位于**包含了所有控制点的最小凸多边形**中。注意，不是按控制点顺序围成的凸多边形。

贝塞尔曲线具有**端点导数性质**：贝塞尔曲线在P~0~和P~n~处的斜率分别等于直线P~0~P~1~和直线P~n-1~P~n~的斜率，即贝塞尔曲线端点的一阶导数仅同其相邻的一条边向量有关，与更远的各边无关。推广的，贝塞尔曲线端点的r阶导数仅同其相邻的r条边向量有关，与更远的各边无关。

贝塞尔曲线最大的问题在于，高阶贝塞尔曲线的路径不够直观，很难让美术人员利用高阶贝塞尔曲线进行设计。但我们注意到，由于贝塞尔曲线具有端点性质和端点导数性质，二阶和三阶贝塞尔曲线的形状相对直观。所以在实际使用中我们会讨论到将多个贝塞尔曲线尤其是二、三阶贝塞尔曲线拼接成直观曲线的算法。

##### 如何拼接贝塞尔曲线？

我们先聊聊什么是**连续性**。

在讨论曲线连续性时，我们经常见到**C^n^连续(参数连续性)**和**G^n^连续(几何连续性)**的说法。如果不 同曲线都用参数方程来描述，那么在连接点处n阶导数连续，就被称为C^n^连续。在图形学中，一般最高只要求C^2^连续，因为更高阶的连续已经超出了人眼视觉能识别的层次。但C^n^连续有一个致命的缺陷，那就是连续性会随参数的选取而变化，这使得C^n^连续不具备普适性，不能用于广泛的学术交流。

为了避免这个缺陷，人们引入了几何连续性和自然参数方程，定义曲线的弧长s作为自然参数，这样任何参数方程都可以转换为自然参数方程，在连接点处自然参数方程的n阶导数连续，就称为G^n^连续。在图形学中，一般最高也只要求G^2^连续。

考察G^0^，G^1^，G^2^连续的几何意义。G^0^就是满足位置连续，即两曲线端点重合。G^1^满足两曲线在端点上具有相同的切线方向。G^2^满足曲线在端点上具有相同的自然参数二阶导矢，也就是对一般参数方程具有相同的曲率矢量。

曲率矢量的求法：
$$
k=\frac{r'(t)\times r''(t)}{|r'(t)|^3}
$$
设两条贝塞尔曲线P和Q的参数方程P(t)和Q(t)，它们的控制点序列分别是P~0~、P~1~……P~n~和Q~0~、Q~1~……Q~m~，则：

+ 满足C^0^和G^0^的连续条件是P(1)=Q(0)，即P~n~=Q~0~。

+ 满足G^1^的连续条件是P~n-1~、P~n~=Q~0~、Q~1~这**三点共线且顺序排列**。

+ 满足C^1^连续的条件是在满足G^1^连续的基础上，有**$|P_{n-1}P_n|=|Q_0Q_1|$**。

接下来进一步讨论满足C^2^和G^2^连续的条件。

根据端点导数性质，C^2^和G^2^的达成与P~n-2~和Q~2~有关。记a~i~=P~i~-P~i-1~，b~i~=Q~i~-Q~i-1~，则对曲线P和Q的端点求其曲率，也就是曲率矢量的模，得：
$$
K_P(1)=\frac{(n-1)|a_{n-1}\times a_n|}{n|a_n|^3}\\
K_Q(0)=\frac{(m-1)|b_1\times b_2|}{m|b_1|^3}
$$
将P~n-1~、P~n~=Q~0~、Q~1~所在的公切线记为直线l，a~n-1~与a~n~之间的夹角记为θ，b~1~与b~2~之间的夹角记为φ，P~n-2~到l的距离记为h~1~，Q~2~到l的距离记为h~2~。根据几何关系可知：
$$
|a_{n-1}\times a_n|= |a_{n-1}|\times|a_n|\times\sin\theta
=|a_{n-1}|\times|a_n|\times\frac{h_1}{|a_{n-1}|}=|a_n|h_1\\
|b_1\times b_2|= |b_1|\times|b_2|\times\sin\theta
=|b_1|\times|b_2|\times\frac{h_2}{|b_1|}=|b_1|h_2\\
$$
![贝塞尔曲线拼接](E:/Textures/图形学/贝塞尔曲线拼接.png)

过端点P~n~=Q~0~作公切线l的垂线v，过端点P~n-2~和Q~2~分别作l的平行线交v于点A、B。连接AP~n-1~、BQ~1~，过点P~n-1~作线端AP~n-1~的垂线CP~n-1~交v于C，过点Q~1~作线端BQ~1~的垂线DQ~1~交v于D。记C到l的距离为g~1~，D到l的距离为g~2~。

在直角三角形ACP~n-1~和直角三角形BDQ~1~中，有：
$$
a_n^2=h_1g_1\\
b_1^2=h_2g_2
$$
于是将曲率变形为：
$$
K_P(1)=\frac{n-1}{n}\frac{|a_n|h_1}{|a_n|^3}=\frac{n-1}{n}\frac1{g_1}\\
K_Q(0)=\frac{m-1}{m}\frac{|b_1|h_2}{|b_1|^3}=\frac{m-1}{m}\frac1{g_2}
$$
满足G^2^的条件是K~Q~(0)=βK~P~(1)，此时：
$$
\beta=\frac{n(m-1)}{m(n-1)}\frac{g_1}{g_2}
$$
满足C^2^的条件是β=1且两曲线具有公共的切平面，要满足β=1即：
$$
g_1=\frac{m(n-1)}{n(m-1)}g_2
$$
在特殊条件下，若m=n，则满足β=1的条件是g~1~=g~2~。

要满足两曲线具有公共切平面，则点P~n-2~、P~n-1~、P~n~=Q~0~、Q~1~、Q~2~五点必须共面，并且点P~n-2~和点Q~2~应位于另三个顶点所在公切线的同侧。

##### 如何获得匀速(自然参数)贝塞尔曲线？

我们已经得到了贝塞尔曲线的参数方程，但显然，如果将参数t作为时间变量，将物体沿曲线运动，我们很难得到一个匀速的运动效果。事实上，在靠近控制点处运动的较慢，而在曲线中段则运行的较快。同时，我们也很难在贝塞尔曲线的拼接处获得良好的平滑速度。

我们可以先求得B(t)相对于t的速度公式，我们先将t看作时间变量，在单个方向上对t求导就可以得到
$$
v(t)=\sqrt{B_x'(t)^2+B_y'(t)^2+B_z'(t)^2}
$$
现在我们以二阶贝塞尔曲线为例，记$B_x'(t)^2+B_y'(t)^2+B_z'(t)^2 = At^2+Bt+C$，将左式展开：
$$
\begin{equation}\begin{split} 
A&=\sum_m^{m\in \{x,y,z\}}4[m_0^2+4m_1^2+m_2^2-4m_0m_1+2m_0m_2-4m_1m_2]\\
B&=\sum_m^{m\in \{x,y,z\}}4[-2m_0^2-4m_1^2+6m_0m_1-2m_0m_2+2m_1m_2]\\
C&=\sum_m^{m\in \{x,y,z\}}4[m_0^2+m_1^2-2m_0m_1]
\end{split}\end{equation}
$$
注意到，A为完全平方式，我们设$\alpha = 2P_0-4P_1+2P_2$，则$A=\alpha_x^2+\alpha_y^2+\alpha_x^2$。同理我们记$\beta=2P_0-2P_1$，则有$C=\beta_x^2+\beta_y^2+\beta_z^2$，$B=-2(\alpha_x\beta_x+\alpha_z\beta_y+\alpha_z\beta_z)$。经整理得：
$$
\begin{equation}\begin{split} 
\alpha &= 2P_0-4P_1+2P_2\\
\beta&=2P_0-2P_1\\
A&=\alpha_x^2+\alpha_y^2+\alpha_x^2\\
B&=-2(\alpha_x\beta_x+\alpha_z\beta_y+\alpha_z\beta_z)\\
C&=\beta_x^2+\beta_y^2+\beta_z^2
\end{split}\end{equation}
$$
写成向量形式即：
$$
\begin{equation}\begin{split} 
\vec v_1&=2(\vec P_0-2\vec P_1+\vec P_2)\\
\vec v_2&=2(\vec P_0-\vec P_1)\\
A&=\vec v_1\cdot\vec v_2\\
B&=-2\vec v_1\cdot\vec v_2\\
C&=\vec v_2\cdot\vec v_2
\end{split}\end{equation}
$$
根据这个公式得出贝塞尔曲线的长度公式$L(t)$：
$$
\begin{equation}\begin{split} 
L(t)&=\int_0^tv(t)dt=\int_0^t\sqrt{At^2+Bt+C}dt\\
&=\frac1{8A^{\frac32}}(2\sqrt A(2Atv(t)+B(v(t)-\sqrt C))+(B^2-4AC)(\ln(B+2\sqrt{AC}-\ln(B+2AT+2\sqrt A v(t)))))
\end{split}\end{equation}
$$
显然，整条曲线的长度为L(1)，我们求变量t的函数$T(t)$，使得$L(T(t))=L(1)*t$，则$T(t)=L^{-1}[L(1)*t]$。由于对L(t)求逆函数几乎不可能，在这里我们利用L(t)的导数v(t)，使用牛顿切线法求出离散的近似解：
$$
T_{n+1}=T_n+\frac{\Delta L}{v(T_n)}
$$
其中$\Delta L$是每次前进的长度，如果按$t\in [0,1]$来计算前进的长度则：
$$
\Delta L = L(1)\times t -T_n
$$
有了上面的经验，我们再来求三阶贝塞尔曲线的$T_n$，记$B_x'(t)^2+B_y'(t)^2+B_z'(t)^2 = At^4+Bt^3+Ct^2+Dt+E$，将左式展开得：
$$
\begin{equation}\begin{split} 
A&=\sum_m^{m\in \{x,y,z\}}9[m_0^2+9m_1^2+9m_2^2+m_3^2-6m_0m_1+6m_0m_2-2m_0m_3-18m_1m_2+6m_1m_3-6m_2m_3]\\
B&=\sum_m^{m\in \{x,y,z\}}36[-m_0^2-6m_1^2-3m_2^2+5m_0m_1-4m_0m_2+m_0m_3+9m_1m_2-2m_1m_3+m_2m_3]\\
C&=\sum_m^{m\in \{x,y,z\}}18[3m_0^2+11m_1^2+2m_2^2-12m_0m_1+7m_0m_2-m_0m_3-11m_1m_2+m_1m_3]\\
D&=\sum_m^{m\in \{x,y,z\}}36[-m_0^2-2m_1^2+3m_0m_1-m_0m_2+m_1m_2]\\
E&=\sum_m^{m\in \{x,y,z\}}9[m_0^2+m_1^2-2m_0m_1]
\end{split}\end{equation}
$$
观察到，A和E为完全平方式，我们取$\alpha=3P_0-9P_1+9P_2-3P_3$，则$A=\alpha_x^2+\alpha_y^2+\alpha_z^2$。取$\beta=3P_0-3P_1$，则$E=\beta_x^2+\beta_y^2+\beta_z^2$。注意到，D是β的倍数，我们取$\gamma=3P_0-6P_1+3P_2$，有$D=-4(\beta_x\gamma_x+\beta_y\gamma_y+\beta_z\gamma_z)$。类似的，又得到$B=-4(\alpha_x\gamma_x+\alpha_y\gamma_y+\alpha_z\gamma_z)$。

C的推导过程比较繁琐，但核心思路如下：设$a=m_3-m_2,b=m_2-m_1,c=m_1-m_0$，将C配方得到与a、b、c、a+b、b+c、a+b+c有关得表达式，然后展开得到$C=18[2b^2+3c^2+ac-6bc]$，易得：
$$
C=\sum_m^{m\in \{x,y,z\}}4\gamma_m^2+2\alpha_m\beta_m
$$
最后整理整个公式：
$$
\begin{equation}\begin{split} 
\alpha &= 3P_0-9P_1+9P_2-3P_3\\
\gamma &= 3P_0-6P_1+3P_2\\
\beta &=3P_0-3P_1\\
A&=\alpha_x^2+\alpha_y^2+\alpha_x^2\\
B&=-4(\alpha_x\gamma_x+\alpha_y\gamma_y+\alpha_z\gamma_z)\\
C&=4(\gamma_x^2+\gamma_y^2+\gamma_y^2)+2(\alpha_x\beta_x+\alpha_y\beta_y+\alpha_z\beta_z)\\
D&=-4(\beta_x\gamma_x+\beta_y\gamma_y+\beta_z\gamma_z)\\
E&=\beta_x^2+\beta_y^2+\beta_z^2\\
\end{split}\end{equation}
$$
写成向量形式即：
$$
\begin{equation}\begin{split} 
\vec v_1&=3(\vec P_0-3\vec P_1+3\vec P_2-\vec P_3)\\
\vec v_2&=3(\vec P_0-2\vec P_1+\vec P_2)\\
\vec v_3&=3(\vec P_0-\vec P_1)\\
A&=\vec v_1\cdot\vec v_1\\
B&=-4\vec v_1\cdot\vec v_2\\
C&=4\vec v_2\cdot\vec v_2+2\vec v_1\cdot\vec v_3\\
D&=-4\vec v_2\cdot\vec v_3\\
E&=\vec v_3\cdot\vec v_3\\
\end{split}\end{equation}
$$

根据以上公式，我们可以通过一个数据结构Segment来保存一段3D三阶贝塞尔曲线：

~~~C#
public class Segment
{
    public float Length => GetLength();
    public Vector3[] controllers = new Vector3[4];

    public Vector3 this[uint i]
    {
        get
        {
            return controllers[i];
        }
        set
        {
            controllers[i] = value;
        }
    }

    public Vector3 Value(float t)
    {
        return (1 - t) * (1 - t) * (1 - t) * controllers[0] + 3 * t * (1 - t) * (1 - t) * controllers[1] + 3 * t * t * (1 - t) * controllers[2] + t * t * t * controllers[3];
    }

    public float Iterate(ref float t, float delta)
    {
        t = t + delta / Derive(t);
        if (t > 1) return t - 1;
        else return 0;
    }

    float GetLength()
    {
        float sum = 0;
        for(float f = 0.01f; f < 1; f++)
        {
            sum += 0.01f * Derive(f);
        }
        return sum;
    }

    float Derive(float t)
    {
        Vector3 v1 = 3 * (controllers[0] - 3 * controllers[1] + 3 * controllers[2] - controllers[3]);
        Vector3 v2 = 3 * (controllers[0] - 2 * controllers[1] + controllers[2]);
        Vector3 v3 = 3 * (controllers[0] - controllers[1]);
        float A = Vector3.Dot(v1, v1);
        float B = -4 * Vector3.Dot(v1, v2);
        float C = 4 * Vector3.Dot(v2, v2) + 2 * Vector3.Dot(v1, v3);
        float D = -4 * Vector3.Dot(v2, v3);
        float E = Vector3.Dot(v3, v3);
        return Mathf.Sqrt(A * t*t*t*t + B * t*t*t + C * t*t + D * t + E);
    }
}
~~~

通过对多个三阶贝塞尔曲线进行拼接，可以实现一个均匀三阶贝塞尔曲线：

~~~C#
public class Bezier 
{
    public float length;
    public List<Transform> points = new List<Transform>();
    public Segment[] segments;

    public Segment[] GetSegments()
    {
        if (points.Count <= 1) return null;
        Segment[] seg = new Segment[points.Count - 1];
        for(int i = 0; i < points.Count - 1; i++)
        {
            Segment s = new Segment();
            s.controllers = new Vector3[4];
            s[0] = points[i].position;
            s[1] = points[i].position + points[i].forward * points[i].lossyScale.z;
            s[2] = points[i + 1].position - points[i + 1].forward * points[i + 1].lossyScale.z;
            s[3] = points[i + 1].position;
            seg[i] = s;
        }
        segments = seg;
        return seg;
    }

    public Vector3[] GetLine(float freq)
    {
        List<Vector3> line = new List<Vector3>();
        int i = 0;
        float t = 0;
        for(; i < segments.Length; )
        {
            line.Add(segments[i].Value(t));
            Iterate(ref i, ref t, freq);
        }
        return line.ToArray();
    }

    public void Iterate(ref int i, ref float t, float delta)
    {
        for(; delta != 0; )
        {
            if (i >= segments.Length) return;
            delta = segments[i].Iterate(ref t, delta);
            if (delta == 0) break;
            i++;
            t = 0;
        }
    }

    public Vector3 Value(int i, float t)
    {
        return segments[i].Value(t);
    }

}
~~~

在此基础上增加Lengths数组还可以进一步增加通过长度查找顶点坐标的功能。

##### 什么是B样条曲线？

仅修改贝塞尔曲线的一个控制点，也会影响到整个曲线的形状。为了弥补这个缺陷，deBoor发明了B样条曲线，B样条曲线基于**deBoor-Cox递推公式**实现。

B样条曲线本质上是对多段短贝塞尔曲线的连续拼接。

n段p次B样条曲线由n+1个**控制点**P~0~、P~1~……P~n~练成的n段的控制折线，和一个m维**节点向量**U={u~0~,u~1~,……,u~m~}组成。

其中u~i~为**节点(pnots)**，m个节点将完整的B样条曲线划分成了m-1个曲线段，每个曲线段都定义在一个节点区间上。对p次的B样条曲线来说，这m-1个曲线段都是p次的贝塞尔曲线。

如果u~i~=u~i+1~=……=u~i+k-1~，那么u~i~节点被称为一个**重复度(multiplicity)**为k的**多重节点**，$1<k\le p$，节点的重复度也被称为度数。否则如果一个节点只出现一次，那么这就是一个**简单节点**。如果u~1~-u~0~=u~2~-u~1~=……=u~m~-u~m-1~，即节点等距，则称节点向量为均匀的，称B样条曲线为均匀B样条曲线，否则称节点向量为非均匀的，称B样条曲线为非均匀B样条曲线。

p次B样条曲线要求满足**m=n+p+1**，显然，不同于贝塞尔曲线，B样条曲线的次数与控制点的数量无关。贝塞尔曲线中的t取值范围是[0,1]，而B样条曲线中的u取值范围可能比[0,1]更小。

B样条曲线和贝塞尔曲线的核心差别是，B样条曲线用**B样条基函数**替代Bernstein基函数，B样条曲线的公式为：
$$
C(u)=\sum_{i=0}^nN_{i,p}(u)P_i
$$
其中N~i,p~(u)是**第i个p次B样条基函数**，也叫调和函数，或者p次规范B样条基函数。这个函数使用deBoor-Cox递推公式，其定义如下：
$$
N_{i,0}(u)=\left \{ \begin{array}{c}
1&u_i\le u<u_{i+1}\\0&otherwise
\end{array}\right.\\
N_{i,p}(u)\frac{u-u_i}{u_{i+p}-u_i}N_{i,p-1}(u)+\frac{u_{i+p+1}-u}{u_{i+p+1}-u_{i+1}}N_{i+1,p-1}(u)
$$

> 注：在该递推公式中认为0÷0=0。

可见，B样条基函数的形态主要受u~i~的取值影响。以U={0,0.25,0.5,0.75,1}的2段2次B样条曲线为例直观的感受B样条基函数的形象：

![B样条0次](Textures\B样条0次.png)

![B样条1次](Textures\B样条1次.png)

![B样条2次](Textures\B样条2次.png)

可以归纳得出，基函数N~i,p~(u)在区间[u~i~,u~i+p+1~)上非零。反过来，在区间[u~i~,u~i+1~)上，共有p+1个p次基函数非零，分别是：N~i-p,p~(u)，N~i-p+1,p~(u)，N~i-p+2,p~(N)……N~i-1,p~(u)和N~i,p~(u)。

当计算N~i,p~(u)时，它在区间[u~i~,u~i+p+1~)上非零。在deBoor-Cox递推公式中，N~i,p~(u)被拆解为一个N~i,p-1~(u)项和一个N~i+1,p-1~(u)项，前者在[u~i~.,u~i+p~)上非零，如果u∈[u~i~.,u~i+p~)，那么$\frac{u-u_i}{u_{i+p}-u_i}$恰好是u和这个区间左端的距离与这个区间长度的比；同理，后者在[u~i+1~,u~i+p+1~)上非零，如果u∈[u~i+1~,u~i+p+1~)，那么$\frac{u_{i+p+1}-u}{u_{i+p+1}-u_{i+1}}$恰好是u和这个区间右端的距离与这个区间长度的比。如图所示：

![B样条递推示意图](E:/Textures/图形学/B样条递推示意图.jpg)

B样条线的形状与节点的度数直接关联，两种具有特殊度数格式的B样条曲线如下：

+ **均匀B样条曲线**

  节点成等差数列均匀分布。

  ![均匀B样条](E:/Textures/图形学/均匀B样条.jpg)

+ **准均匀B样条曲线**

  在均匀B样条曲线的基础上，两端节点的度数为p+1，中间节点的度数为p。

  ![准均匀B样条](E:/Textures/图形学/准均匀B样条曲线.jpg)

一般的B样条曲线称为**开B样条曲线**，即曲线不会与控制折线的第一边和最后一边接触，对于开B样条曲线，其定义域为[u~p~,u~m-p~]。可见，均匀B样条曲线就是一种特殊的开B样条曲线。

强制第一个节点和最后一个节点的重复度为p+1，那么产生的曲线就会分别与第一边和最后一边相切，这样的曲线被称为**clamped-B样条曲线**。可见，准均匀B样条曲线就是一种特殊的clamped-B样条曲线。

通过重复节点和控制点，可以使产生的曲线形成闭环，称为**闭B样条曲线**。生成闭B样条曲线的方法有两种：

+ Wrapping控制点

  设计一个m+1个节点的节点序列U={$u_0=0,u_1=\frac1m,u_2=\frac2m,...u_{m-1}=\frac{m-1}m,u_m=1$}，对曲线的头p个和尾p个控制点，令$P_0=P_{n-p+1}$,$P_1=P{n-p+2}$,...,$P{p-2}=P_{n-1}$,$P_{p-1}=P_n$。曲线的定义域为[$u_p,u_{n-p}$]，且在连接点C($u_p$)=C($u_{n-p}$)上保证C^p-1^连续性。

+ Wrapping节点

  设计n+1个控制点，并使$P_n=P_0$。先找到具有n个节点的单调节点序列U，在U基础上增加p+1个节点，并使$u_{n+1}=u_0,u_{n+2}=u_1,...,u_{n+p}=u_{p-1},u_{n+p+1}=u_p$，这样得到一个定义域为[$u_0,u_{n+1}$]的闭曲线，在连接点C($u_0$)=C($u_{n+1}$)上保证C^p-1^连续性。

#### 参数曲面

##### 什么是贝塞尔曲面？

我们先复习贝塞尔曲线的定义：
$$
B_n(t)=\sum_{i=0}^nB_{i,n}(t)P_i,\ t\in[0,1]
$$
其中$B_{i,n}(t)$是Bernstein基函数：
$$
B_{i,n}(u)=\frac{n!}{i!(n-i)!}u^i(1-u)^{n-1}
$$

贝塞尔曲面本质上就是先对每一行做贝塞尔曲线，然后将这些贝塞尔曲线的结果作为列贝塞尔曲线的控制点。

我们用两个参数u、v代替t，分别在两个方向上描述贝塞尔曲面的形状，相对应的，我们需要n×m个控制点控制整个曲面，记控制点为$P_{ij},i\in n,j\in m$。

![贝塞尔曲面](E:/Textures/图形学/贝塞尔曲面.png)

在上图中，$q_i (u)$由每行的m个控制点控制，是一条m-1阶的贝塞尔曲线。即：
$$
q_i(u)=\sum_{j=0}^mB_{j,n}(u)P_{ij}
$$
紧接着，这n条贝塞尔曲线$q_i (u)$在$u=u_0$上取得了n个点，将这n个点作为v方向上的贝塞尔曲线的控制点得到整个贝塞尔曲面的方程：
$$
P(u,v)=\sum_{i=0}^nB_{i,n}(v)q_i(u)\\
P(u,v)=\sum_{i=0}^n\ [B_{i,n}(v)\sum_{j=0}^mB_{j,n}(u)P_{ij}]
$$
显然，贝塞尔曲面的定义域是参数范围[0,1]×[0,1]。

##### 什么是B样条曲面？

B样条曲面的思路和贝塞尔曲面一致，我们也用两个参数u、v代替t，使用n×m个控制点，则相对的，我们需要节点矢量U={u~0~,u~1~,...,u~n+p~}和V={v~0~,v~1~,...v~m+q~}，他们分别由n+p+1和m+q+1个节点组成，它们的要求和B样条曲线中一致。

只需将贝塞尔曲面中的Bernstein基函数改为B样条基函数就可以得到B样条曲面的定义，在此我们先复习一下B样条基函数的deBoor-Cox递推公式：
$$
N_{i,0}(u)=\left \{ \begin{array}{c}
1&u_i\le u<u_{i+1}\\0&otherwise
\end{array}\right.\\
N_{i,p}(u)\frac{u-u_i}{u_{i+p}-u_i}N_{i,p-1}(u)+\frac{u_{i+p+1}-u}{u_{i+p+1}-u_{i+1}}N_{i+1,p-1}(u)
$$
用B样条基函数替换Bernstein基函数：
$$
P(u,v)=P(u,v)=\sum_{i=0}^n\ [N_{i,n}(v)\sum_{j=0}^mN_{j,n}(u)P_{ij}]
$$
显然，B样条曲面的定义域是参数范围[u~p~,u~n+1~]×[v~q~,v~q+1~]。

##### 什么是Nurbs曲面？

**Nurbs**是**非均匀有理B样条**的缩写，B样条曲面是Nurbs曲面的一种特例。Nurbs建模(也被称为**曲面建模**)是工业3D建模的一个重要方式，另一种更常见的建模方式则是**多边形建模**。

Nurbs相比一般B样条曲面的改进在于，通过调整曲面中控制点的权，来产生更好的平滑性。我们记$W_{ij}$为控制点$P_{ij}$的权值，除此之外的定义都与B样条曲线相同。

下面直接给出Nurbs曲面的参数方程：
$$
S(u,v)=\frac
{\sum_{i=0}^n\sum_{j=0}^mN_{i,p}(u)N_{j,q}(v)W_{ij}P_{ij}	}
{\sum_{i=0}^n\sum_{j=0}^mN_{i,p}(u)N_{j,q}(v)W_{ij}}
$$
其中$N_{i,p}(u)$是B样条基函数。

显然，Nurbs曲面的定义域也是参数范围[u~p~,u~n+1~]×[v~q~,v~q+1~]。

##### 曲面如何进行渲染？

由于曲面是矢量图形，我们采用和曲线一样的渲染思路，在参数空间上以任意精细度对参数点进行采样，将采样结果组成四边形或三角形，然后就可以以多边形的方式对其进行渲染。

对于更高级的渲染方案，可以通过在距离摄像机近处密集采样，距离摄像机远除粗略采样，来提升渲染性能。

#### 曲面重建

##### 什么是逆向工程？





<div STYLE="page-break-after:always;"></div>

### 多边形建模

#### 多边形数据

##### 什么是多边形网格？

网格是由物体的点云连接形成的，点云中的顶点包括坐标、法线、切线等一系列信息。网格通常由三角形、四边形或者其它**凸多边形**组成，凹多边形会极大的影响渲染效率。一般的建模规范是，在CG领域尽量保持四边形建模，因为四边形网格能更方便的处理各类特效，同时对美术人员来说也易于修改；而在工业领域则应该导出三角形网格，因为三角形网格更加稳定，方便硬件理解。

工业领域的网格必须是封闭几何体，在3Dmax等模型软件中都有封闭性检查算法。而CG领域的网格则不必是封闭集合体，它可以是带有空洞的普通多边形或者面片，在CG领域使用面片和单面模型可以有效降低渲染消耗。

三维模型本身是不可见的，但可以根据线框在不同细节层次渲染。线框数据通过配置材质和纹理，然后经过着色器的渲染才能表现出完整的外观。

##### OBJ文件是如何储存多边形的?

OBJ是一种几何定义文件格式，Wavefront Technologies公司在可视化加强动画包中第一次使用了这个格式，文件格式是公开的，并具有及其优质的兼容性和跨平台、跨行业的通用性，在所有3D应用软件中被支持。

OBJ文件可以以ASCII编码也可以以二进制格式编码，以ASCII格式编码的后缀名为**.obj**，以二进制格式编码的后缀名为**.mod**。OBJ格式的三维网格模型储存了模型的顶点、面片、法向量纹理等几何信息。

OBJ文件使用标准得Polygon储存格式，直接储存顶点坐标和法线等数据，这导致OBJ文件无法导出骨骼动画，只能储存静态模型和材质信息。

下面的例子是一个标准立方体的**.obj**文件表示。

```OBJ
o cube
# List of geometric vertices, with (x,y,z[,w]) coordinates.
v -0.5 -0.5 0.5
v -0.5 -0.5 -0.5
v 0.5 -0.5 -0.5
v 0.5 -0.5 0.5
v -0.5 0.5 0.5
v 0.5 0.5 0.5
v 0.5 0.5 -0.5
v -0.5 0.5 -0.5

# List of texture coordinates, in (u, v [,w]) coordinates
vt 1 0
vt 1 1
vt 0 1
vt 0 0

# List of vertex normals in (x,y,z) form
vn -0.5774 -0.5774 0.5774
vn -0.5774 -0.5774 -0.5774
vn 0.5774 -0.5774 -0.5774
vn 0.5774 -0.5774 0.5774
vn -0.5774 0.5774 0.5774
vn 0.5774 0.5774 0.5774
vn 0.5774 0.5774 -0.5774
vn -0.5774 0.5774 -0.5774

# Polygonal face element
f 1/1/1 2/2/2 3/3/3 4/4/4
f 5/4/5 6/1/6 7/2/7 8/3/8
f 1/4/1 4/1/4 6/2/6 5/3/5
f 4/4/4 3/1/3 7/2/7 6/3/6
f 3/4/3 2/1/2 8/2/8 7/3/7
f 2/4/2 1/1/1 5/2/5 8/3/8
```

以"#"开头的行是obj格式中的备注。

obj格式中，使用不同的**行标志符**来开始一行，比如v/vt/vn/f，这些行标识符用来标注不同类型的数据，每行结尾不需要使用分号。

以o开头的行标注一个模型的开始，之后跟上模型的名称。

以g开头的行标注网格组的开始，之后跟上网格组的名称。一个模型中可以使用多个网格组，不同网格组可以分配不同的材质。如果不声明网格组，则整个模型文件使用同一个网格组。

以v开头的行表示顶点，之后跟上x y z [w]的值来储存顶点坐标。W是可选项，默认为1.0。一些应用支持顶点颜色，用x y z r g b [w]的格式来表示顶点，但这样的obj文件在不支持顶点颜色的应用中可能读取失败。

以vt开头的行表示纹理坐标，俗称uv坐标，之后跟上u v [w]的值来储存uv坐标，一般取值范围在0~1，w是可选项，默认值为0.0。

以vn开头的行表示法线方向，之后跟上x y z的值来储存顶点法线方向，该法线不一定是单位法线。

以f开头的行表示面片，之后跟上索引语句来将顶点分配给面片，索引语句一共有四种格式：

+ 顶点索引：以f v1 v2 v3 ...的格式分配的面片。v1、v2、v3等是顶点序号，以文件中第一个v标志行为1，逐个递增。一个面片至少分配3个顶点，但可以分配超过3个顶点，即obj格式不保证三角面。面中顶点的声明顺序一般按逆时针方向，即遵循右手螺旋定则。
+ 纹理坐标索引：以f v1/vt1 v2/vt2 v3/vt3 ...的格式分配的面片。v1、v2、v3等是顶点序号，vt1、vt2、vt3等是对应顶点的纹理坐标序号，序号分配方式和v类似。
+ 顶点法线索引：以f v1//vn1 v2//vn2 v3//vn3 ...的格式分配的面片。v1、v2、v3等是顶点序号，vn1、vn2、vn3等是对应顶点的法线方向序号，序号分配方式和v类似。
+ 顶点纹理法线索引：以f v1/vt1/vn1 v2/vt2/vn2 v3/vt3/vn3 ...的格式分配的面片。v1、v2、v3等是顶点序号，vt1、vt2、vt3等是对应顶点的纹理坐标序号，vn1、vn2、vn3等是对应顶点的法线方向序号。

在OBJ文件中如果希望一个顶点在不同面具有不同的纹理坐标或法线，最好的办法是构造多个重合的顶点。

##### STL文件是如何储存多边形的？

STL格式是美国3D SYSTEM公司提出的三位实体造型斯通的一个接口标准，采用三角形面片离散地近似表示三维模型，目前被工业界认为是3D打印和机械塑性领域地标准描述文件格式，在工程学、医学成像以及文物保护等方面广泛应用。

STL文件和OBJ文件能很好的互相转换。由于STL主要用在工业领域，并不关系材质和纹理，所以不保存纹理坐标，仅保存顶点坐标和面片法线方向。STL格式也有二进制和ASCII两种格式。

STL和OBJ格式的差别在于它不逐顶点的保存法向量而是逐面片的保存法向量。

二进制STL文件起始的80个字节是文件头，用于储藏零件名；紧接着用4个字节的整数描述模型的三角面片个数，后面逐个给出每个三角面片的几何信息。每个三角面片占用50个字节，依次是3个4字节浮点数作为面片法向量，3个4字节浮点数作为第1个顶点坐标，3个4字节浮点数作为第2个顶点坐标，3个4字节浮点数作为第3个顶点坐标，最后两个字节描述三角面片的属性信息。一个完整二进制文件的大小为三角面片数乘以50再加上84个字节的首部。

ASCII版本的STL文件逐行给出三角形面片几何信息，其格式如下：

```STL
solid filename cube
facet normal 0 1 0
outer loop
vertex -1 0 -1
vertex 1 0 -1
vertex -1 0 1
endloop
endfacet
facet normal 0 1 0
outer loop
vertex 1 0 -1
vertex -1 0 1
vertex 1 0 1
endloop
endfacet
endsolid filename cube
```

在facet-endfacet段中定义一个面片，在facet normal 后设置面片的法向量。之后用outer loop-endloop包围三个顶点的定义，在outer loop段中的三个顶点是围绕法向量逆时针定义的。

STL的ASCII码格式所占的空间一般是同一个模型二进制格式的五倍大小。

##### FBX文件是如何储存多边形的？

FBX属于AutoDesk设计的模型格式，因为Unity和UE引擎的使用而备受关注。FBX是封装好的二进制文件，没有真正的ASCII格式。虽然在3Dmax等模型软件中，存在将FBX文件导出成ASCII形式的接口，但导出的ASCII格式没有分析价值，在实际使用中，一般只能使用AutoDesk提供的c++ SDK对FBX进行读写，AutoDesk并没有提供FBX的二进制格式，这也是其饱受诟病的原因之一。

FBX以**scene graph**的结构来储存模型的信息的，从数据结构的角度讨论可以理解为一个多叉树，这种树形的储存结构和Unity、UE等引擎的场景结构十分匹配，并能很好的储存骨骼动画等信息，这也是FBX类型经常出现在游戏和电影等CG中的原因。

这个树从根节点Root开始，每个结点是一个KFbxNode类型的对象。每个KFbxNode维持一个双向的指针，也就是说子结点可以索引到父节点，父节点也可以索引到子结点。KFbxNode拥有一个字段NodeAttribute储存结点的类型，这些类型包括eMesh、eLight、eCamera、eSkeleton等，这个字段可以使用SDK轻松的访问到。

FBX中的Mesh可以使用多种不同得格式储存，包括Nurbes、Polygon、Triangle等。网格中的属性主要包括：顶点坐标、顶点颜色、顶点法向量、顶点切向量、UV坐标等。

在使用FBX SDK时，可以通过任意遍历树的算法来遍历KFbxNode，从中取出需要的数据。如遍历一个Mesh结点后，将它的所有顶点信息取出并整合，就可以把网格信息送往GPU进行渲染了。

##### Unity中是如何储存多边形的？

Unity引擎中用C#实现的Mesh类具有显著的面向对象特性。作为一个引擎，Unity的Mesh类负责将多种不同类型的模型文件封装成适合面向对象编程的Mesh对象。

Unity的Mesh类中最重要的成员包括顶点坐标数组vertices，uv坐标数组uv，三角形索引数组triangles，顶点法线数组normals，顶点切线数组tangents。除此之外，还有顶点颜色数组colors，骨骼数组bones等。

在Unity中，MeshFilter和MeshRenderer这两个脚本被用来实现Draw Call，其中MeshFilter用于装载顶点，而MeshRenderer用于设置渲染状态，Mesh数据必须与这两个脚本配合才能产生渲染效果。

##### GPU是如何储存多边形的？

目前的GPU可以通过被不同的API调用，处理不同结构的顶点数据。如在GLSL中，可以指定包括顶点数组、表面法线数组、颜色数组在内的大量数组作为缓冲区，然后通过不同的API调用实现对不同结构的处理。

最早期的GPU顶点缓冲只使用单个缓冲区(三角形列表)记录顶点信息，一个网格如果包含n个三角形，缓冲区中则有3n个顶点坐标，也就是9n个浮点数，如前文中的STL文件的储存方式，这种方式被称为非索引的三角形列表。在CPU向GPU传送数据时，将顶点坐标储存在连续的数组中，GPU会将每9个连续的浮点数作为一个三角形处理。在最新的DirectX和OpenGL库中依然保留了这样的顶点输入方式。

非索引的三角形列表是直观的，但顶点信息一般都存在冗余。如一个分段数为32的球体网格含有482个顶点和512个多边形，其中有64个三角面和448个四边形面，在传入GPU时这448个四边形面都需要转化为三角面，使用非索引的三角形列表时，GPU将接受总计64 + 448 \* 2 = 960个三角形的数组，也就是2880个顶点的数组。而这其中有两个顶点被32个面共用，有480个顶点被6个面共用，总共产生了2 \* 31 + 480 \* 5 = 2462条冗余顶点信息，达到了惊人的85.5%。

为了减少冗余，可以使用顶点在顶点列表中的索引来代替顶点构建三角形，这就形成了含索引的三角形列表。CPU将提供两份数组，第一份是所有顶点的坐标，第二份是由索引构成的三角形数组。在含有n个顶点、m个三角形的网格中，使用3n个浮点数记录顶点，3m个整数索引记录三角形。GPU会通过3个连续的索引获得3个顶点信息来获得一个待处理的三角形。这有效减少了CPU向GPU传输数据的时间，而间接内存访问带来的GPU的消耗上升对高并行高带宽的GPU来说不值一提。

在分段数为32的球体网格示例中，需要2880 * 3 * 4B = 33.75KB来实现非索引的三角形列表，而只需要482 * 3 * 4B + 512 * 3 * 4B = 16.90KB来实现含索引的三角形列表，空间节省了接近一半。在更复杂、顶点更密集的网格中，冗余数据的减少量会更加客观。

在实际应用中还需要向GPU传递顶点法线数组和纹理坐标数组，它们一般来说和顶点数组长度一致，GPU可以通过索引三角形列表中的同一个索引同时访问三个数组的对应信息。

##### 如何生成一个球体网格?

体积球和参数球的构建十分简单易懂，但网格球体的生成相比之下麻烦了不少。在此介绍4种常用的球体网格生成算法：

+ **UV-Sphere生成算法**

UV-Sphere是最常用的球体网格。在球坐标系中，将球体按照经度和纬度划分成若干个矩形，由此生成一个球：

~~~C#
void UVSphere(int numHorizontalSegments, int numVerticalSegments, out List<Vector3> vertices)
    {
        vertices = new List<Vector3>();
        vertices.Add(new Vector3(0, 1, 0));
        for (int h = 0; h < numHorizontalSegments; h++)
        {
            float angleh = (h + 1) * Mathf.PI / (numHorizontalSegments + 1);

            for (int v = 0; v <= numVerticalSegments; v++)
            {
                float anglev = 2 * Mathf.PI * v / numVerticalSegments;

                float x = Mathf.Sin(angleh) * Mathf.Cos(anglev);
                float y = Mathf.Cos(angleh);
                float z = Mathf.Sin(angleh) * Mathf.Sin(anglev);
                Vector3 point = new Vector3(x, y, z);
                vertices.Add(point);
            }
        }
        vertices.Add(new Vector3(0, -1, 0));
    }
~~~

如此形成的网格在两级处的布线十分密集，球体网格示意图如下：

<img src="Textures\UVSphere.png" alt="UVSphere" style="zoom:50%;" />

接下来我们要根据参数生成索引数组：

~~~C#
void UVSphere(int numHorizontalSegments, int numVerticalSegments, out List<int> indices)
    {
        indices = new List<int>();
        for (int i = 1; i <= numHorizontalSegments; i++)
        {
            indices.Add(0);
            indices.Add(i + 1);
            indices.Add(i);

            for (int j = 0; j < numVerticalSegments; j++)
            {
                int b = j * numHorizontalSegments + i;
                indices.Add(b);
                indices.Add(b + numHorizontalSegments);
                indices.Add(b - 1);

                indices.Add(b + numHorizontalSegments);
                indices.Add(b + numHorizontalSegments - 1);
                indices.Add(b - 1);
            }

            indices.Add(numHorizontalSegments * (numVerticalSegments + 1) + 1);
            indices.Add(numHorizontalSegments * numVerticalSegments + i);
            indices.Add(numHorizontalSegments * numVerticalSegments + i + 1);
        }

        indices.Add(numHorizontalSegments * (numVerticalSegments + 1) + 1);
        indices.Add(numHorizontalSegments * (numVerticalSegments + 1));
        indices.Add(numHorizontalSegments * numVerticalSegments + 1);
    }
~~~

+ **Cube-Sphere生成算法**

这个算法从一个立方体开始，通过在立方体的六个面上分布生成一个四叉树，我们可以将立方体进行细分。将细分后的点全部压缩到淡位球上，使网格接近球体的形态。如此生成的网格可以使用4-8网格进行LOD细分，但在八个顶点处的布线有明显的扭曲：

<img src="Textures\CubeSphere.jpg" alt="CubeSphere" style="zoom:50%;" />

~~~C#
void CubeSphere(int segments, out List<Vector3> vertices)
    {
        float half = segments * 0.5f;
        vertices = new List<Vector3>();

        //生成前面的中心
        for (int i = 1; i < segments; i++)
        {
            for(int j = 1; j < segments; j++)
            {
                vertices.Add(new Vector3(j - half, i - half, -half).normalized);
            }
        }
        //生成环
        for(int i = 0; i <= segments; i++)
        {
            for (int j = 0; j < segments; j++) vertices.Add(new Vector3(j - half, -half, i - half).normalized);
            for (int j = 0; j < segments; j++) vertices.Add(new Vector3(half, j - half, i - half).normalized);
            for (int j = 0; j < segments; j++) vertices.Add(new Vector3(half - j, half, i - half).normalized);
            for (int j = 0; j < segments; j++) vertices.Add(new Vector3(-half, half - j, i - half).normalized);
        }
        //生成后面的中心
        for (int i = 1; i < segments; i++)
        {
            for (int j = 1; j < segments; j++)
            {
                vertices.Add(new Vector3(j - half, i - half, half).normalized);
            }
        }
    }
~~~

~~~C#
void CubeSphere(int segments, out List<int> indices)
    {
        indices = new List<int>();
        int pows = (segments - 1) * (segments - 1);
        int biasc = 5 * segments * segments + 2 * segments + 1;
        int biasr = 4 * segments * segments;

        int[,] frontMap = new int[segments + 1, segments + 1];
        int[,] backMap = new int[segments + 1, segments + 1];
    
        for(int i = 0; i < segments-1; i++)
        {
            for(int j = 0; j < segments-1; j++)
            {
                frontMap[i + 1, j + 1] = j * (segments - 1) + i;
                backMap[i + 1, j + 1] = biasc + frontMap[i + 1, j + 1];
            }
        }
    
        for(int i = 0; i < segments; i++)
        {
            frontMap[i, 0] = i + pows;
            frontMap[segments, i] = i + pows + segments;
            frontMap[segments - i, segments] = i + pows + 2 * segments;
            frontMap[0, segments - i] = i + pows + 3 * segments;
            backMap[i, 0] = frontMap[i, 0] + biasr;
            backMap[segments, i] = frontMap[segments, i] + biasr;
            backMap[segments - i, segments] = frontMap[segments - i, segments] + biasr;
            backMap[0, segments - i] = frontMap[0, segments - i] + biasr;
        }

        for(int i = 0; i < segments; i++)
        {
            for(int j = 0; j < segments; j++)
            {
                indices.Add(frontMap[i, j]);
                indices.Add(frontMap[i, j + 1]);
                indices.Add(frontMap[i + 1, j]);

                indices.Add(frontMap[i, j + 1]);
                indices.Add(frontMap[i + 1, j + 1]);
                indices.Add(frontMap[i + 1, j]);

                indices.Add(backMap[i, j]);
                indices.Add(backMap[i + 1, j]);
                indices.Add(backMap[i, j + 1]);

                indices.Add(backMap[i, j + 1]);
                indices.Add(backMap[i + 1, j]);
                indices.Add(backMap[i + 1, j + 1]);
            }
        }
    
        for(int i = 0; i < segments; i++)
        {
            int b = pows + i * 4 * segments;
            for(int j = 0; j < 4 * segments - 1; j++)
            {
                indices.Add(b + j);
                indices.Add(b + j + 1);
                indices.Add(b + j + 4 * segments);

                indices.Add(b + j + 1);
                indices.Add(b + j + 4 * segments + 1);
                indices.Add(b + j + 4 * segments);
            }
            indices.Add(b);
            indices.Add(b + 4 * segments);
            indices.Add(b + 8 * segments - 1);

            indices.Add(b);
            indices.Add(b + 8 * segments - 1);
            indices.Add(b + 4 * segments - 1);   
        }
    }
~~~

+ **Ico-Sphere生成算法**

这个算法基于二十面体，对二十面体的每个三角形进行细分，然后将它们归一到单位球上。Icosphere生成的网格和Cubesphere有些类似，其布线相对来说更均匀一些，但缺少一个类似4-8网格的成熟算法来实现LOD：

> 有趣的是，对二十面体进行3段细分，得到的恰好是我们熟知的足球形网格体。

<img src="Textures\Icosphere.png" alt="Icosphere" style="zoom:50%;" />

在该算法中，我们先生成一个二十面体，然后将该二十面体的每个棱划分成$n$段，再在二十面体的每个面内部生成$\frac12(n-1)(n-2)$个新顶点，即对每个三角形总共生成$\frac12(n+1)(n+2)$个顶点，以此将每个三角形划分成$n^2$个子三角形，划分时生成的新点的坐标可以通过三角向量法求解。

~~~C#
void IcoSphere(int segments, out List<Vector3> vertices, out List<int> indices)
    {
        Vector3[] bases = {
        new Vector3(0.2764016f, 0.4472178f, 0),
        new Vector3(-0.2764016f, 0.4472178f, 0),
        new Vector3(-0.2764016f, -0.4472178f, 0),
        new Vector3(0.2764016f, -0.4472178f, 0),
        new Vector3(0.4472178f, 0, 0.2764016f),
        new Vector3(-0.4472178f, 0, 0.2764016f),
        new Vector3(-0.4472178f, 0, -0.2764016f),
        new Vector3(0.4472178f, 0, -0.2764016f),
        new Vector3(0, 0.2764016f, 0.4472178f),
        new Vector3(0, 0.2764016f, -0.4472178f),
        new Vector3(0, -0.2764016f, -0.4472178f),
        new Vector3(0, -0.2764016f, 0.4472178f)
    };

        int[] triangles = {
        0,1,8,1,0,9,0,8,4,8,1,5,
        9,0,7,1,9,6,0,4,7,1,6,5,
        6,9,10,7,10,9,4,8,11,5,11,8,
        2,6,10,3,10,7,3,4,11,2,11,5,
        2,5,6,3,7,4,2,10,3,2,3,11
    };

        vertices = new List<Vector3>();
        indices = new List<int>();
        for (int i = 0; i < 20; i++)
        {
            Vector3 p0 = bases[triangles[3 * i]];
            Vector3 p1 = bases[triangles[3 * i + 1]];
            Vector3 p2 = bases[triangles[3 * i + 2]];
            DivideTriangle(segments, p0, p1, p2, ref vertices, ref indices);
        }

    }

    void DivideTriangle(int segments, Vector3 p0, Vector3 p1, Vector3 p2, ref List<Vector3> vertices, ref List<int> indices)
    {
        int baseV = vertices.Count;    

        Vector3 v1 = p1 - p0;
        Vector3 v2 = p2 - p0;

        for (int i = 0; i <= segments; i++)
        {
            float u = (float)i / segments;
            for (int j = 0; j <= segments - i; j++)
            {
                float v = (float)j / segments;
                Vector3 point = p0 + u * v1 + v * v2;
                vertices.Add(point.normalized);
            }
        }

        int k = 0;
        for (int i = segments; i > 0; i--)
        {
            for (int j = 0; j < i; j++)
            {
                //向上的三角形
                indices.Add(baseV + k);
                indices.Add(baseV + k + i + 1);
                indices.Add(baseV + k + 1);

                //向下的三角形
                if (k - i - 1 >= 0)
                {
                    indices.Add(baseV + k);
                    indices.Add(baseV + k + 1);
                    indices.Add(baseV + k - i - 1);
                }

                k++;
            }
            k++;
        }
    }
~~~

+ **Fibonacci-Sphere生成算法**

我们能否找到一个算法，将球面均匀的划分为给定的任意多个顶点？首先我们对问题进行简化，先考虑在二维平面中实现将圆划分为任意多个顶点。

我们从极坐标系中开始，考虑极坐标方程$\theta=2\pi k\rho$，其中$k$是参数。我们将这个方程改为参数方程并使其离散化：
$$
\left\{\begin{array}{c}
\rho = i/n\\
\theta = 2\pi ki
\end{array}\right.
$$
$i\in \{0,1,2,...,n\}$，考虑在$k$的不同取值下，点阵图像的变化：

~~~C#
for(int i = 0; i < n; i++)
{
    float rho = i / (n-1);
    float theta = 2 * Mathf.PI * fraction * i;
    
    float x = rho * Mathf.Cos(theta);
    float y = rho * Mathf.Sin(theta);
    
    PlotPoint(x,y,color);
}
~~~

我们令fraction等于不同的参数，并做出了如下所示的图：

<img src="Textures\斐波那契图.png" alt="斐波那契图" style="zoom:80%;" />

注意到，当fraction恰好等于$\phi=\frac{\sqrt5-1}{2}=0.618$时，极坐标参数方程恰好能均匀的向所有角度发散。

这一特性也可以被利用在三维球体中，我们保留上面的角度函数，将它映射到一个球上——具体的做法是在球坐标系中保证另一个角度轴均匀的从球体的一级分布到另一级，如下图所示：

<img src="Textures\点阵球.png" alt="点阵球" style="zoom:50%;" />

显然，令fraction恰等于黄金分割比例$\phi$能使这个球体表面的顶点分布尽可能均匀，这时顶点的生成算法如下，这样生成的球面顶点分布被称为斐波那契球：

~~~C#
void FibonacciSphere(int numPoint, out List<Vector3> vertices)
{
	vertices = new List<Vector3>();
    float phi = (Mathf.Sqrt(5) + 1) / 2;
    
    for(int i = 0; i < n; i++)
	{  
    	float t = (float)i/n;
    	float theta = 2 * Mathf.PI * phi * i;
    	float tau = Mathf.Acos(1-2*t);
    
    	float x = Mathf.Sin(tau) * Mathf.Cos(theta);
    	float y = Mathf.Sin(tau) * Mathf.Sin(theta);
    	float z = Mathf.Cos(tau);
    
    	Vector3 point = new Vector3(x,y,z);
        vertices.Add(point);
	}
}
~~~

> 斐波那契球常被用于在三维空间中向空间锥体均匀的发射射线。

在获得了球面的均匀点阵后，我们就可以尝试通过生成三角形来形成球体网格了。这个算法有些复杂，我们会在【点云重建】中详谈。使用某种多边形重建算法后，斐波那契球就可以被映射成下面这样的球体：

<img src="Textures\fibonacci-sphere-delaunay.png" alt="fibonacci-sphere-delaunay" style="zoom: 33%;" />

映射方案不止一种，下面是另一种映射方式：

<img src="Textures\fibonacci-sphere-voronoi.png" alt="fibonacci-sphere-voronoi.png" style="zoom:33%;" />

#### 布尔运算

##### 如何将线段裁剪到立方体内？

布尔运算从交运算开始，是因为它与网格的空间裁剪逻辑一致。于是我们从二维线段裁剪算法开始介绍，随后扩展到三维线段裁剪和三维三角形裁剪。

对于线段和立方体的关系可以分为四种：

+ 两个顶点都位于立方体内；
+ 一个顶点位于立方体内，另一个顶点位于立方体外；
+ 两个顶点都位于立方体外，但与立方体相交； 
+ 两个顶点都位于立方体外，且与立方体不相交；

为了区分这四种情况，衍生出了**Cohen-Sutherland算法**和**Liang-Barsky算法**两种基本思路。

Cohen-Sutherland算法将空间按照相对于裁剪窗口左-中-右



#### 多边形变形

##### 如何构造几何体的凸包？



##### 什么是自由变形？



#### 点云重建

##### 如何重建三角形？



<div STYLE="page-break-after:always;"></div>

### 体积建模

#### 距离场渲染

##### 什么是像素着色器？

假设我们输入GPU的顶点信息是一个恰好覆盖整个屏幕的四边形，渲染这个四边形的片元着色器就被称为**像素着色器(Pixel Shader)**。这么称呼的原因是像素着色器的每个片元恰好覆盖屏幕中的一个像素。

对一个像素着色器来说，它的顶点输入如下(OBJ格式)：

```OBJ
o quad
# List of geometric vertices, with (x,y,z[,w]) coordinates.
v -1 -1 0
v -1 1 0
v 1 1 0
v 1 -1 0 

# List of texture coordinates, in (u, v [,w]) coordinates
vt 0 0
vt 0 1
vt 1 1
vt 1 0

# List of vertex normals in (x,y,z) form
vn 0 0 -1

# Polygonal face element
f 1/1/1 2/2/1 3/3/1 4/4/1
```

对像素着色器来说，顶点信息是无关紧要的。我们直接通过函数来表达几何体、光照和摄像机的渲染信息，而所有的几何体计算都是在片元着色器中实现的。在像素着色器中我们可以在不需要美术提供任何网格的前提下，只使用数学公式绘制完整的场景和动态效果。为了实现像素着色器，我们需要了解**距离场函数**这个数学工具，以及与它相关的**射线追踪函数**和**微分法线函数**。

像素着色器最大的特点是，可以渲染出场景中不存在的物体。

在像素着色器中，我们跳过了GPU渲染流水线中的很多空间变换相关的操作。所以我们需要用纯数学函数来表达一个几何体，以便获取渲染对象的属性，这个函数被称为**SDF(Signed Distance Functions，有向距离场函数)**。

SDF以一个空间中的顶点坐标P为输入，返回点P距离某几何体表面的距离D。D的符号表明了顶点与几何体的关系，当D小于0表示P点位于几何体内部，D大于0表示P位于几何体外部，D等于0表示P恰好位于几何体表面。

我们考虑一个最简单的例子，假设要渲染的几何体是一个球心位于原点，半径为1的球体。显然，我们的SDF函数的数学表达是这样的：
$$
f(x,y,z)=\sqrt{x^2+y^2+z^2}-1
$$
或者在GLSL中这样表达：

```glsl
float SDF_Sphere(vec3 p)
{
	return length(p) - 1.0;
}
```

2D像素着色器也需要使用SDF，如观察下面的这一GLSL函数，它表达了一个半径为1的圆：

```glsl
float SDF_Circle(vec2 p)
{
    return length(p) - 1.0;
}
```

在得到了想要的SDF后，我们可以使用**射线扫描步进算法(Raymarching Method)**将SDFs渲染出来。

与在光线追踪中类似，我们在视锥体的近裁剪平面虚拟出一个与屏幕的大小相同的网格，网格上的每一格与屏幕的一个像素，或者说生成的每一个片元对应。接下来，我们计算每一个片元的颜色，以此渲染出整个场景。而计算每个片元的方法，就是以摄像机为原点，往网格上每个格的方向发射射线，由射线来获取场景中距离摄像机最近的物体，然后根据物体的几何信息和光照信息来计算颜色。

<img src="Textures\光线追踪.png" alt="光线追踪" style="zoom: 50%;" />

在光线追踪算法中，场景是由标准的多边形组成的，所以要通过三角形碰撞算法计算射线与三角形的交点。而像素着色器的实现中，场景是由SDF函数组成的，所以我们需要将一个**采样点(Sample Point)**在一个射线上逐步的移动，以此获取场景信息。

一种朴素的算法是，以一个设定好的步长移动采样点，然后遍历场景中的SDF，以判断该点是否在某个SDF中。正如我们前面定义的，当SDF返回一个负数时，说明该点击中或击穿了某个几何体，那么我们就可以开始渲染这个像素了。如果没有，那我们继续移动采样点直到击中物体或超出我们设置的测试距离。 

一种更精确且更高效的方法被称为**球面追踪(Sphere Tracing)**——我们希望每次采样点移动的距离足够的远，以此减少计算量，但又不希望采样点前进的太远导致错过与任何一个几何体的相交。满足这个要求的采样点移动距离被称为**最远安全距离**，我们通过对一个采样点求出距离它最近的SDF的距离，换句话说，将点的坐标代入到所有SDF中得到的最小正数，作为下一次步进的最远安全距离，如图所示：

<img src="http://jamie-wong.com/images/16-07-11/spheretrace.jpg" alt="img" style="zoom:150%;" />

在上图中，从$p_0$开始发射射线，将点$p_0$代入所有SDF中，取返回值中的最小值为最远安全距离$d_0$，也就是到三角形障碍物的距离，将$p_0$沿射线方向移动$d_0$得到点$p_1$，这一步在几何学上相当于以$p_0$为球心做了一个与场景相切的球，然后将球面与射线相交的点作为下一次的采样点，这也是“球面追踪”这个名字的由来。以此类推得到点$p_1、p_2、p_3、p_4$，由于$p_4$距离一个表面的距离低于阈值，我们在$p_4$处停止射线追踪。

在GLSL中，射线扫描算法大概是这样的：

```glsl
float depth = start; //设置射线的初始值
for (int i = 0; i < MAX_MARCHING_STEPS; i++) {
    //eye为原点，向viewRayDirection方向发射射线，目前eye距离采样点的距离为depth
    float dist = SDF(eye + depth * viewRayDirection); 
    //到SDF的距离小于阈值，则认为射线触及了一个物体，返回这eye到物体的距离depth
    if (dist < EPSILON) return depth;
    //没有接触到物体，向前步进最远安全距离
    depth += dist;

    //超出射线的最远距离，返回end
    if (depth >= end) return end;
}
return end;
```

现在我们用射线扫描绘制了一个球体：

```glsl
const int MAX_MARCHING_STEPS = 255;
const float MIN_DIST = 0.0;
const float MAX_DIST = 100.0;
const float EPSILON = 0.0001;

//一个球体的SDF
float sphereSDF(vec3 samplePoint) {
    return length(samplePoint) - 1.0;
}

//整个场景的SDF，目前只有一颗球；如果场景中不止一颗球，则返回所有SDF的返回值的最小值
float sceneSDF(vec3 samplePoint) {
    /* 场景SDF的伪代码
	float minSDF = maxValue;
	foreach(SDF prim in AllSDFs)
	{
		if(minSDF > prim(samplePoint)) minSDF = prim(samplePoint);
	}
	return minSDF
	*/
    
    return sphereSDF(samplePoint);
}

//球面追踪算法，返回原点距离射线与场景交点的距离
float shortestDistanceToSurface(vec3 eye, vec3 marchingDirection, float start, float end) {
    float depth = start;
    for (int i = 0; i < MAX_MARCHING_STEPS; i++) {
        float dist = sceneSDF(eye + depth * marchingDirection);
        if (dist < EPSILON) {
			return depth;
        }
        depth += dist;
        if (depth >= end) {
            return end;
        }
    }
    return end;
}

//由片元的像素坐标生成一个射线
vec3 rayDirection(float fieldOfView, vec2 size, vec2 fragCoord) {
    vec2 xy = fragCoord - size / 2.0;
    float z = size.y / tan(radians(fieldOfView) / 2.0);
    return normalize(vec3(xy, -z));
}

//片元着色器入口，输入片元的像素坐标，输出片元的颜色
void mainImage( out vec4 fragColor, in vec2 fragCoord )
{
	vec3 dir = rayDirection(45.0, iResolution.xy, fragCoord);
    vec3 eye = vec3(0.0, 0.0, 5.0);
    float dist = shortestDistanceToSurface(eye, dir, MIN_DIST, MAX_DIST);
    
    if (dist > MAX_DIST - EPSILON) {
        // 没有射中物体，则返回背景色
        fragColor = vec4(0.0, 0.0, 0.0, 0.0);
		return;
    }
    
    fragColor = vec4(1.0, 0.0, 0.0, 1.0);
}
```

> 代码来自 https://www.shadertoy.com/view/llt3R4
>
> 你可以前往Shadertoy网站看到更多关于像素着色器的代码。https://www.shadertoy.com/

##### 像素着色器如何计算光照?

多边形光照一般使用顶点法线来计算，但在像素着色器中并没有保存任何顶点法线信息，我们该如何求得法线呢。

SDF函数在空间中形成了一个场，我们将SDF场在某点处的梯度作为该点的切线方向。

空间几何中梯度的求法：
$$
\Delta f = (\frac{\part f}{\part x},\frac{\part f}{\part y},\frac{\part f}{\part z})
$$
对计算机来说，我们不需要求函数的实导数(实际上也很难做到)，而只需要进行微分逼近：
$$
\vec n=\begin{bmatrix}f(x+\epsilon,y,z)-f(x-\epsilon,y,z)\\
f(x,y+\epsilon,z)-f(x,y-\epsilon,z)\\f(x,y,z+\epsilon)-f(x,y,z-\epsilon)\end{bmatrix}
$$

```glsl
vec3 estimateNormal(vec3 p) {
    return normalize(vec3(
        sceneSDF(vec3(p.x + EPSILON, p.y, p.z)) - sceneSDF(vec3(p.x - EPSILON, p.y, p.z)),
        sceneSDF(vec3(p.x, p.y + EPSILON, p.z)) - sceneSDF(vec3(p.x, p.y - EPSILON, p.z)),
        sceneSDF(vec3(p.x, p.y, p.z  + EPSILON)) - sceneSDF(vec3(p.x, p.y, p.z - EPSILON))
    ));
```

得到法线以后，使用任何一种光照模型即可。完整代码如下：

~~~GLSL
const int MAX_MARCHING_STEPS = 255;
const float MIN_DIST = 0.0;
const float MAX_DIST = 100.0;
const float EPSILON = 0.0001;

float sphereSDF(vec3 samplePoint) {
    return length(samplePoint) - 1.0;
}

float sceneSDF(vec3 samplePoint) {
    return sphereSDF(samplePoint);
}

float shortestDistanceToSurface(vec3 eye, vec3 marchingDirection, float start, float end) {
    float depth = start;
    for (int i = 0; i < MAX_MARCHING_STEPS; i++) {
        float dist = sceneSDF(eye + depth * marchingDirection);
        if (dist < EPSILON) {
			return depth;
        }
        depth += dist;
        if (depth >= end) {
            return end;
        }
    }
    return end;
}

vec3 rayDirection(float fieldOfView, vec2 size, vec2 fragCoord) {
    vec2 xy = fragCoord - size / 2.0;
    float z = size.y / tan(radians(fieldOfView) / 2.0);
    return normalize(vec3(xy, -z));
}

vec3 estimateNormal(vec3 p) {
    return normalize(vec3(
        sceneSDF(vec3(p.x + EPSILON, p.y, p.z)) - sceneSDF(vec3(p.x - EPSILON, p.y, p.z)),
        sceneSDF(vec3(p.x, p.y + EPSILON, p.z)) - sceneSDF(vec3(p.x, p.y - EPSILON, p.z)),
        sceneSDF(vec3(p.x, p.y, p.z  + EPSILON)) - sceneSDF(vec3(p.x, p.y, p.z - EPSILON))
    ));
}

//计算一个点光源对物体的光照贡献值,k_d是漫反射颜色,k_s是高光颜色,alpha是高光强度,p是被着色的点的坐标,eye是摄像机坐标,lightPos是点光源坐标,lightIntensity是点光源强度
vec3 phongContribForLight(vec3 k_d, vec3 k_s, float alpha, vec3 p, vec3 eye,
                          vec3 lightPos, vec3 lightIntensity) {
    vec3 N = estimateNormal(p);
    vec3 L = normalize(lightPos - p);
    vec3 V = normalize(eye - p);
    vec3 R = normalize(reflect(-L, N));
    
    float dotLN = dot(L, N);
    float dotRV = dot(R, V);
    
    if (dotLN < 0.0) {
        // Light not visible from this point on the surface
        return vec3(0.0, 0.0, 0.0);
    } 
    
    if (dotRV < 0.0) {
        // Light reflection in opposite direction as viewer, apply only diffuse
        // component
        return lightIntensity * (k_d * dotLN);
    }
    return lightIntensity * (k_d * dotLN + k_s * pow(dotRV, alpha));
}

//计算所有点光源对物体的光照,k_a是环境光颜色,k_d是漫反射颜色,k_s是高光颜色,alpha是高光强度,p是被照射点的坐标,eye是摄像机坐标
vec3 phongIllumination(vec3 k_a, vec3 k_d, vec3 k_s, float alpha, vec3 p, vec3 eye) {
    const vec3 ambientLight = 0.5 * vec3(1.0, 1.0, 1.0);
    vec3 color = ambientLight * k_a;
    
    vec3 light1Pos = vec3(4.0 * sin(iTime),
                          2.0,
                          4.0 * cos(iTime));
    vec3 light1Intensity = vec3(0.4, 0.4, 0.4);
    
    color += phongContribForLight(k_d, k_s, alpha, p, eye,
                                  light1Pos,
                                  light1Intensity);
    
    vec3 light2Pos = vec3(2.0 * sin(0.37 * iTime),
                          2.0 * cos(0.37 * iTime),
                          2.0);
    vec3 light2Intensity = vec3(0.4, 0.4, 0.4);
    
    color += phongContribForLight(k_d, k_s, alpha, p, eye,
                                  light2Pos,
                                  light2Intensity);    
    return color;
}

void mainImage( out vec4 fragColor, in vec2 fragCoord )
{
	vec3 dir = rayDirection(45.0, iResolution.xy, fragCoord);
    vec3 eye = vec3(0.0, 0.0, 5.0);
    float dist = shortestDistanceToSurface(eye, dir, MIN_DIST, MAX_DIST);
    
    if (dist > MAX_DIST - EPSILON) {
        // Didn't hit anything
        fragColor = vec4(0.0, 0.0, 0.0, 0.0);
		return;
    }
    
    // The closest point on the surface to the eyepoint along the view ray
    vec3 p = eye + dist * dir;
    
    vec3 K_a = vec3(0.2, 0.2, 0.2);
    vec3 K_d = vec3(0.7, 0.2, 0.2);
    vec3 K_s = vec3(1.0, 1.0, 1.0);
    float shininess = 10.0;
    
    vec3 color = phongIllumination(K_a, K_d, K_s, shininess, p, eye);
    
    fragColor = vec4(color, 1.0);
}
~~~

##### 有哪些常见的SDF？

**Exact SDF(精确距离函数)**和**Bound SDF(粗略距离函数)**是对SDF质量的分类。

精确距离函数精确的保留了一个几何体在欧几里得空间中的所有属性并精确的测量距离。而一个粗略距离函数并不返回一个精确的值，但在大部分情况下这仍然有效。

精度距离函数一般比粗略距离函数更受欢迎，因为它们生成的结果一般更好，并且在算法使用量大的情况下能一直保持高精度。但粗略距离函数在某些情况下是必须的，比如椭球体的SDF和简单平滑函数等，在这些情况下，由于数学算法过于复杂或没有对应的数学定义表现其精确距离，不得不使用粗略距离函数。

在任何情况下，尽量使用精确SDF，因为粗略SDF很可能导致整体质量的下降。

+ 球体

```glsl
float sdfSphere( vec3 p, float r )
{
    return length(p) - r;
}
```

+ 立方体

```glsl
float sdfBox( vec3 p, vec3 b )
{
    vec3 q = abs(p) - b;
    return length(max(q,0.0)) + min(max(q.x,max(q.y,q.z)),0.0);
    // 把q的分量分成4种情况讨论: 3-, 2-1+, 2+1-, 3+
    // q的分量为3-的情况,P在盒子内,取最大的一个分量(绝对值最小)返回,对应函数的后半段
    // q的分量不为3-的情况,P在盒子外,抛去负分量将剩余的分量取模返回,对应函数的前半段
}
```

+ 圆锥体

```glsl
float sdfCone( vec3 p, vec2 c )
{
    // c is the sin/cos of the angle
    vec2 q = vec2( length(p.xz), -p.y );
    float d = length(q-c*max(dot(q,c), 0.0));
    return d * ((q.x*c.y-q.y*c.x<0.0)?-1.0:1.0);
}
```

+ 胶囊体

```glsl
float sdfCapsule( vec3 p, vec3 a, vec3 b, float r )
{
  vec3 pa = p - a, ba = b - a;
  float h = clamp( dot(pa,ba)/dot(ba,ba), 0.0, 1.0 );
  return length( pa - ba*h ) - r;
}
```

+ 3D面片

```glsl
float udfQuad( vec3 p, vec3 a, vec3 b, vec3 c, vec3 d )
{
  vec3 ba = b - a; vec3 pa = p - a;
  vec3 cb = c - b; vec3 pb = p - b;
  vec3 dc = d - c; vec3 pc = p - c;
  vec3 ad = a - d; vec3 pd = p - d;
  vec3 nor = cross( ba, ad );

  return sqrt(
    (sign(dot(cross(ba,nor),pa)) +
     sign(dot(cross(cb,nor),pb)) +
     sign(dot(cross(dc,nor),pc)) +
     sign(dot(cross(ad,nor),pd))<3.0)
     ?
     min( min( min(
     dot2(ba*clamp(dot(ba,pa)/dot2(ba),0.0,1.0)-pa),
     dot2(cb*clamp(dot(cb,pb)/dot2(cb),0.0,1.0)-pb) ),
     dot2(dc*clamp(dot(dc,pc)/dot2(dc),0.0,1.0)-pc) ),
     dot2(ad*clamp(dot(ad,pd)/dot2(ad),0.0,1.0)-pd) )
     :
     dot(nor,pa)*dot(nor,pa)/dot2(nor) );
}
```

+ 圆

```glsl
float sdfCircle( vec2 p, float r )
{
  return length(p) - r;
}
```

+ 线段

```glsl
float sdfSegment( in vec2 p, in vec2 a, in vec2 b )
{
    vec2 pa = p-a, ba = b-a;
    float h = clamp( dot(pa,ba)/dot(ba,ba), 0.0, 1.0 );
    return length( pa - ba*h );
}
```

+ 矩形

```glsl
float sdfBox( in vec2 p, in vec2 b )
{
    vec2 d = abs(p)-b;
    return length(max(d,0.0)) + min(max(d.x,d.y),0.0);
}
```

+ 任意多边形

```glsl
float sdPolygon( in vec2[N] v, in vec2 p )
{
    float d = dot(p-v[0],p-v[0]);
    float s = 1.0;
    for( int i=0, j=N-1; i<N; j=i, i++ )
    {
        vec2 e = v[j] - v[i];
        vec2 w =    p - v[i];
        vec2 b = w - e*clamp( dot(w,e)/dot(e,e), 0.0, 1.0 );
        d = min( d, dot(b,b) );
        bvec3 c = bvec3(p.y>=v[i].y,p.y<v[j].y,e.x*w.y>e.y*w.x);
        if( all(c) || all(not(c)) ) s*=-1.0;  
    }
    return s*sqrt(d);
}
```

> 你可以在这里找到所有剩下的常用几何体的SDF：
>
> https://iquilezles.org/www/articles/distfunctions/distfunctions.htm

##### 有哪些常见的3D-SDF变形函数？

变形函数是对SDF进行变形的数学方法。

本质上，变形函数是对输入的顶点坐标进行了一次变换，将变换后的顶点坐标输入SDF。但这样的理解方式实在是太过抽象。在实际使用时，可以将变形函数理解成对模型的一个修饰。

下面的代码基本上是伪代码。

+ 平移和旋转，transform在这里指变换矩阵或四元数

```glsl
vec3 opTx( in vec3 p, in mat4 t, in sdf3d primitive )
{
    return primitive( inverse(t)*p );
}
```

+ 缩放

```glsl
float opScale( in vec3 p, in float s, in sdf3d primitive )
{
    return primitive(p/s)*s;
}
```

+ 镜像(以X轴镜像为例)

```glsl
float opSymX( in vec3 p, in sdf3d primitive )
{
    p.x = abs(p.x);
    return primitive(p);
}
```

+ 伸展：并不等同于缩放。如球体伸展后得到胶囊体，而球体缩放后得到椭球体。

```glsl
float opElongate( in sdf3d primitive, in vec3 p, in vec3 h )
{
    vec3 q = p - clamp( p, -h, h );
    return primitive( q );
}
```

+ 平滑

```glsl
float opRound( in sdf3d primitive, float rad )
{
    return primitive(p) - rad;
    //通过减一个标量,相当于对体积表面的每个点向外扩张了一个球体,球体的表面连接起来形成了一个平滑表面
}
```

+ 挤出：由2D SDF平移生成3D几何体

```glsl
float opExtrusion( in vec3 p, in sdf2d primitive, in float h )
{
    float d = primitive(p.xy)
    vec2 w = vec2( d, abs(p.z) - h );
    return min(max(w.x,w.y),0.0) + length(max(w,0.0));
}
```

+ 车削：由2D SDF旋转生成3D几何体

```glsl
float opRevolution( in vec3 p, in sdf2d primitive, float o )
{
    vec2 q = vec2( length(p.xz) - o, p.y );
    return primitive(q)
}
```

+ 布尔运算：SDF的并集、差集、交集

```glsl
float opUnion( float d1, float d2 ) {  min(d1,d2); }

float opSubtraction( float d1, float d2 ) { return max(-d1,d2); }

float opIntersection( float d1, float d2 ) { return max(d1,d2); }
```

+ 平滑布尔运算：SDF的平滑的并集、差集、交集

```glsl
float opSmoothUnion( float d1, float d2, float k ) {
    float h = clamp( 0.5 + 0.5*(d2-d1)/k, 0.0, 1.0 );
    return mix( d2, d1, h ) - k*h*(1.0-h); }

float opSmoothSubtraction( float d1, float d2, float k ) {
    float h = clamp( 0.5 - 0.5*(d2+d1)/k, 0.0, 1.0 );
    return mix( d2, -d1, h ) + k*h*(1.0-h); }

float opSmoothIntersection( float d1, float d2, float k ) {
    float h = clamp( 0.5 - 0.5*(d2-d1)/k, 0.0, 1.0 );
    return mix( d2, d1, h ) + k*h*(1.0-h); }
```

+ 无限阵列，参数c为阵列间隔

```glsl
float opRep( in vec3 p, in vec3 c, in sdf3d primitive )
{
    vec3 q = mod(p+0.5*c,c)-0.5*c;
    return primitive( q );
}
```

+ 有限阵列，参数l为阵列数

```glsl
vec3 opRepLim( in vec3 p, in float c, in vec3 l, in sdf3d primitive )
{
    vec3 q = p-c*clamp(round(p/c),-l,l);
    return primitive( q );
}
```

+ 溶解

```glsl
float opDisplace( in sdf3d primitive, in vec3 p )
{
    float d1 = primitive(p);
    float d2 = displacement(p);
    return d1+d2;
}
```

+ 扭曲

```glsl
float opTwist( in sdf3d primitive, in vec3 p )
{
    const float k = 10.0; // or some other amount
    float c = cos(k*p.y);
    float s = sin(k*p.y);
    mat2  m = mat2(c,-s,s,c);
    vec3  q = vec3(m*p.xz,p.y);
    return primitive(q);
}
```

+ 弯曲

```glsl
float opCheapBend( in sdf3d primitive, in vec3 p )
{
    const float k = 10.0; // or some other amount
    float c = cos(k*p.x);
    float s = sin(k*p.x);
    mat2  m = mat2(c,-s,s,c);
    vec3  q = vec3(m*p.xy,p.z);
    return primitive(q);
}
```

##### 有哪些常见的2D-SDF变形函数？

下面的代码基本上是伪代码。

+ 圆滑

```glsl
float opRound( in vec2 p, in float r )
{
  return sdShape(p) - r;
}
```

+ 线条加粗

```glsl
float opOnion( in vec2 p, in float r )
{
  return abs(sdShape(p)) - r;
}
```

很多变形函数和3D类似，不再重复。

#### 密度场渲染

##### 什么是密度场？

密度场就是通过函数、矩阵或3D纹理的方式，记录空间中每一点的物质属性，尤其是物质密度，并以此为基础进行渲染。

密度场摆脱了对多边形的依赖，在布尔运算等算法上有极高的效率，且可以用于描述云雾、次表面散射等用寻常模型难以描述的光学现象。

密度场一般来说有三种类型：

+ 第一种是逻辑密度场，即空间中每点的密度只有0和1两种情况，这种密度场可以用于区分实体和非实体，对不透明物体进行表示。显然，用这种方式来表示不透明物体，占用的空间比使用SDF、曲面或多边形多得多。这种表达方式最大的优势在于能更高效的对物体进行动态修改，如在流体模拟中使用逻辑密度场来表达流体。
+ 第二种是浮点密度场，即空间中每点的密度由0.0至1.0的浮点数表示。这种密度场可以用于表达具有体积的半透明物体，比如云、雾等体积雾效果，和火焰、丁达尔效应等体积光效果。在二维渲染中，浮点密度场也经常被用于表达流体和粘液等物体的形态。

+ 第三种是向量密度场，即空间中每点的属性由一个浮点向量来表达。使用向量允许开发者将颜色储存在密度场中，在之后的采样中实现颜色的叠加。实用的场合如模拟彩色烟雾等。

对密度场的渲染有**射线扫描法**和**步进立方体法**两种，这两种方法都涉及到对密度场的采样。

对函数式的密度场来说，我们只需要将采样点坐标输入函数，就可以返回指定点的密度值。一般来说，函数式密度场可以是噪声函数，有时也可以使用SDF作为密度场函数。而对矩阵式密度场进行采样时，我们需要考虑**采样插值**的问题。由于采样点一般都不能和矩阵上的点恰好对应，我们很可能需要对采样点周围的矩阵值进行插值。一般的插值方式是对周围八个矩阵点的值按照距离进行带权平均。

##### 什么是射线扫描算法？

对于逻辑密度场来说，其思想和在距离场渲染中的扫描步进类似，这里需要注意的是，我们不再能通过球面追踪来对这个算法进行优化了，所以我们必须注意步进步长的问题，因为这将直接影响到渲染的效率。对每条射线进行步进时，一旦迭代到实体就对其进行渲染。渲染实体需要得到与法线有关的信息，如果我们不是使用距离场来构造密度场，那么我们只能通过对实体表面周围点进行采样来估计法线的方向，对于这样的实体渲染我们已经在距离场渲染中谈到了类似的算法。

接下来我们着重讨论在浮点密度场中，使用射线扫描算法实现体积云的方法。

我们从摄像机沿视角方向发射射线，在射线上对密度场进行一系列采样，将每点采样的结果理解为该点云雾颗粒的密度，然后根据这些结果来计算此处透光的多少，并得到这条射线方向上进入摄像机的光的亮度。

显然，采样值、也就是微粒浓度越高的方向上，摄像机能接收到的光照就越少。我们可以使用$f(x)=e^{-D(x)}$来实现对采样值的归一化，计算出在某浓度下光线的投射率，浓度越高，透射率越低。其中$D(x)$为在射线上，距离为x处的采样值密度。
$$
I_v=e^{-\int_{0}^{l}D(x)dx}\approx e^{-\sum_{i=0}^n(x_i-x_{i-1})D(x_i)}
$$
我们可以对这条射线上的采样值进行求和，然后将采样和进行归一化，根据归一得到的透射率与缓冲区中的色彩进行混合，从而得到一个简单的体积雾效果。

如果我们希望实现更真实的云效果，我们必须对现实中云的成像原理有足够的了解。

云由小水珠组成，当光线射入水珠时，会发生折射和反射，这导致部分光线不会进入人眼，从而计算出了云的亮度。这个过程是很难计算的，其中一种方案是使用光线追踪【见全局光照】，即每次迭代都递归的计算反射方向和折射方向的光辐照度。显然，使用光追渲染体积云可以产生最真实的效果，但大多数情况下我们并不需要这么真实的效果，或者我们承担不起指数级的复杂度。

在不使用光线追踪的情况下，我们对光照模型进行简化。即我们假设光线在体积中会反/折射且仅会反/折射一次，并且假设光线会在进入眼睛的过程中衰减，且衰减的量与光在云中的光程以及光路上水滴粒子的密度有关。

<img src="Textures\射线扫描体积云.png" alt="射线扫描体积云.png" style="zoom: 33%;" />

在这样的模型中，我们通过步进模拟出了如上图所示的光路，假设人眼所看见的光线是这些从不同角度入射，但最终反射或折射到视线方向上的光的总和。在一束光以这样的一次折射进入人视野的过程中，总共经历了三次衰减：第一次是光从光源到折射点(图中红点)的过程中的衰减，第二次是光的方向折射或反射到视角方向上时发生的衰减，第三次是光从折射点到人眼的过程中的衰减。

我们来看看光线穿过水滴微粒时的行为：由于云中水滴微粒的直径和可见光的波长差不多，光在穿过水滴微粒时发生的散射效应遵循**米氏散射(Mie Scattering)**的原理(米氏散射与瑞利散射相对，后者用于描述微粒直径远小于可见光波长时的散射情况，可以用来模拟大气层中光的折射，并以此渲染基于真实物理的霞光)。米氏散射表达了在微粒直径与光波长差距不大的情况下，光的出射辐照度的分布情况与出射角度的关系，可见在这种情况下光更多的透过微粒而更少的被折射到其它方向。

<img src="Textures\光线散射.png" alt="光线散射.png" style="zoom:50%;" />

我们依然假设水滴浓度越高，光线的衰减就越明显，所以我们仍使用$f(x)=e^{-D(x)}$来计算在直线方向上的透射度，也就是第一次和第三次衰减中的透射程度。而对于第二次衰减，显然我们可以用光线方向和视角方向的夹角的余弦值对这一步产生的衰减进行计算。

~~~hlsl
fixed scattering = (-dot(normalize(lightDir), normalize(viewDir)) * 0.5 + 0.5) * _ScatteringValue + _ScatteringBase;
~~~

最终整个采样的过程如下：

~~~glsl
const float stepSize = 11;


float transmittance = 1;
vec3 lightEnergy = 0;

while(dstTravelled < dstLimit)
{
    rayPos = entryPoint + rayDir * dstTravelled;
    float density = sampleDensity(rayPos);
    if (density > 0) {
        vec3 dirToLight = _WorldSpaceLightPos0.xyz;
        float dstInsideBox = rayBoxDst(boundsMin, boundsMax, rayPos, 1/dirToLight).y;
                
        float stepSize = dstInsideBox/numStepsLight;
        float totalDensity = 0;

        for (int step = 0; step < numStepsLight; step ++) {
            rayPos += dirToLight * stepSize;
            totalDensity += max(0, sampleDensity(rayPos) * stepSize);
        }

        float transmittance = exp(-totalDensity * lightAbsorptionTowardSun);
     	float lightTransmittance = darknessThreshold + transmittance * (1-darknessThreshold);
        
        float scattering = (dot(normalize(dirToLight), normalize(rayDir)) * 0.5 + 0.5) * _ScatteringValue + _ScatteringBase;
        
        lightEnergy += density * stepSize * transmittance * lightTransmittance * scattering * phaseVal;
        transmittance *= exp(-density * stepSize * lightAbsorptionThroughCloud);
        if (transmittance < 0.01) { break; }      
    }
    dstTravelled += stepSize;
}
~~~

##### 什么是步进立方体算法？

步进立方体算法是一种在密度场中采样来生成多边形的算法，在生成多边形之后再依照多边形渲染的过程进行正常的渲染。 相比射线扫描算法，步进立方体算法可以生成并保存多边形，所以在密度场的修改频率不高的情况下，使用步进立方体算法可以节省很多不必要的性能消耗。

我们首先考虑逻辑密度场的情况，首先我们将问题局限在一个小立方体之中，立方体一共有八个顶点，这八个顶点位于逻辑密度场当中，所以在对这八个顶点进行采样时，每个顶点可以得到一个1或0的采样值，其中1代表采样点位于网格内部，0代表采样点位于网格外部。根据顶点是否位于网格内部，我们可以生成不同的三角形。

由于立方体有8个顶点，也就是说，存在256种不同的小立方体配置。我们将这256种小立方体归纳为15类，其它情况都是这15种情况的旋转、镜像或反相：

![Marching Cubes.png](Textures\Marching Cubes.png)

接下来我们需要用一种数据结构来记录在这256种立方体中的三角形序列，显然我们可以使用一个长度为256的指针数组来实现这个映射。接下来我们将八个顶点依次编号为0……7，顶点的编号作为位数，构成一个8位的整数，正好作为数组的索引，而数组的内容则是构成三角形的边序列，这里我们提供一个已经归纳好的步进立方体索引数组，然后再讲解这个数组的使用方法。

~~~C#
		public static int[,] triangulation = {
            {-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//0
            { 0, 8, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//1
            { 0, 1, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//2
            { 1, 8, 3, 9, 8, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//3
            { 1, 2, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//4
            { 0, 8, 3, 1, 2, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//5
            { 9, 2, 10, 0, 2, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//6
            { 2, 8, 3, 2, 10, 8, 10, 9, 8, -1, -1, -1, -1, -1, -1, -1 },			//7
            { 3, 11, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//8
            { 0, 11, 2, 8, 11, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//9
            { 1, 9, 0, 2, 3, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//10
            { 1, 11, 2, 1, 9, 11, 9, 8, 11, -1, -1, -1, -1, -1, -1, -1 },			//11
            { 3, 10, 1, 11, 10, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//12
            { 0, 10, 1, 0, 8, 10, 8, 11, 10, -1, -1, -1, -1, -1, -1, -1 },			//13
            { 3, 9, 0, 3, 11, 9, 11, 10, 9, -1, -1, -1, -1, -1, -1, -1 },			//14
            { 9, 8, 10, 10, 8, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//15
            { 4, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//16
            { 4, 3, 0, 7, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//17
            { 0, 1, 9, 8, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//18
            { 4, 1, 9, 4, 7, 1, 7, 3, 1, -1, -1, -1, -1, -1, -1, -1 },				//19
            { 1, 2, 10, 8, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//20
            { 3, 4, 7, 3, 0, 4, 1, 2, 10, -1, -1, -1, -1, -1, -1, -1 },				//21
            { 9, 2, 10, 9, 0, 2, 8, 4, 7, -1, -1, -1, -1, -1, -1, -1 },				//22
            { 2, 10, 9, 2, 9, 7, 2, 7, 3, 7, 9, 4, -1, -1, -1, -1 },				//23
            { 8, 4, 7, 3, 11, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//24
            { 11, 4, 7, 11, 2, 4, 2, 0, 4, -1, -1, -1, -1, -1, -1, -1 },			//25
            { 9, 0, 1, 8, 4, 7, 2, 3, 11, -1, -1, -1, -1, -1, -1, -1 },				//26
            { 4, 7, 11, 9, 4, 11, 9, 11, 2, 9, 2, 1, -1, -1, -1, -1 },				//27
            { 3, 10, 1, 3, 11, 10, 7, 8, 4, -1, -1, -1, -1, -1, -1, -1 },			//28
            { 1, 11, 10, 1, 4, 11, 1, 0, 4, 7, 11, 4, -1, -1, -1, -1 },				//29
            { 4, 7, 8, 9, 0, 11, 9, 11, 10, 11, 0, 3, -1, -1, -1, -1 },				//30
            { 4, 7, 11, 4, 11, 9, 9, 11, 10, -1, -1, -1, -1, -1, -1, -1 },			//31
            { 9, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//32
            { 9, 5, 4, 0, 8, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//33
            { 0, 5, 4, 1, 5, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//34
            { 8, 5, 4, 8, 3, 5, 3, 1, 5, -1, -1, -1, -1, -1, -1, -1 },				//35
            { 1, 2, 10, 9, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//36
            { 3, 0, 8, 1, 2, 10, 4, 9, 5, -1, -1, -1, -1, -1, -1, -1 },				//37
            { 5, 2, 10, 5, 4, 2, 4, 0, 2, -1, -1, -1, -1, -1, -1, -1 },				//38
            { 2, 10, 5, 3, 2, 5, 3, 5, 4, 3, 4, 8, -1, -1, -1, -1 },				//39
            { 9, 5, 4, 2, 3, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//40
            { 0, 11, 2, 0, 8, 11, 4, 9, 5, -1, -1, -1, -1, -1, -1, -1 },			//41
            { 0, 5, 4, 0, 1, 5, 2, 3, 11, -1, -1, -1, -1, -1, -1, -1 },				//42
            { 2, 1, 5, 2, 5, 8, 2, 8, 11, 4, 8, 5, -1, -1, -1, -1 },				//43
            { 10, 3, 11, 10, 1, 3, 9, 5, 4, -1, -1, -1, -1, -1, -1, -1 },			//44
            { 4, 9, 5, 0, 8, 1, 8, 10, 1, 8, 11, 10, -1, -1, -1, -1 },				//45
            { 5, 4, 0, 5, 0, 11, 5, 11, 10, 11, 0, 3, -1, -1, -1, -1 },				//46
            { 5, 4, 8, 5, 8, 10, 10, 8, 11, -1, -1, -1, -1, -1, -1, -1 },			//47
            { 9, 7, 8, 5, 7, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//48
            { 9, 3, 0, 9, 5, 3, 5, 7, 3, -1, -1, -1, -1, -1, -1, -1 },				//49
            { 0, 7, 8, 0, 1, 7, 1, 5, 7, -1, -1, -1, -1, -1, -1, -1 },				//50
            { 1, 5, 3, 3, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//51
            { 9, 7, 8, 9, 5, 7, 10, 1, 2, -1, -1, -1, -1, -1, -1, -1 },				//52
            { 10, 1, 2, 9, 5, 0, 5, 3, 0, 5, 7, 3, -1, -1, -1, -1 },				//53
            { 8, 0, 2, 8, 2, 5, 8, 5, 7, 10, 5, 2, -1, -1, -1, -1 },				//54
            { 2, 10, 5, 2, 5, 3, 3, 5, 7, -1, -1, -1, -1, -1, -1, -1 },				//55
            { 7, 9, 5, 7, 8, 9, 3, 11, 2, -1, -1, -1, -1, -1, -1, -1 },				//56
            { 9, 5, 7, 9, 7, 2, 9, 2, 0, 2, 7, 11, -1, -1, -1, -1 },				//57
            { 2, 3, 11, 0, 1, 8, 1, 7, 8, 1, 5, 7, -1, -1, -1, -1 },				//58
            { 11, 2, 1, 11, 1, 7, 7, 1, 5, -1, -1, -1, -1, -1, -1, -1 },			//59
            { 9, 5, 8, 8, 5, 7, 10, 1, 3, 10, 3, 11, -1, -1, -1, -1 },				//60
            { 5, 7, 0, 5, 0, 9, 7, 11, 0, 1, 0, 10, 11, 10, 0, -1 },				//61
            { 11, 10, 0, 11, 0, 3, 10, 5, 0, 8, 0, 7, 5, 7, 0, -1 },				//62
            { 11, 10, 5, 7, 11, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//63
            { 10, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//64
            { 0, 8, 3, 5, 10, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//65
            { 9, 0, 1, 5, 10, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//66
            { 1, 8, 3, 1, 9, 8, 5, 10, 6, -1, -1, -1, -1, -1, -1, -1 },				//67
            { 1, 6, 5, 2, 6, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//68
            { 1, 6, 5, 1, 2, 6, 3, 0, 8, -1, -1, -1, -1, -1, -1, -1 },				//69
            { 9, 6, 5, 9, 0, 6, 0, 2, 6, -1, -1, -1, -1, -1, -1, -1 },				//70
            { 5, 9, 8, 5, 8, 2, 5, 2, 6, 3, 2, 8, -1, -1, -1, -1 },					//71
            { 2, 3, 11, 10, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//72
            { 11, 0, 8, 11, 2, 0, 10, 6, 5, -1, -1, -1, -1, -1, -1, -1 },			//73
            { 0, 1, 9, 2, 3, 11, 5, 10, 6, -1, -1, -1, -1, -1, -1, -1 },			//74
            { 5, 10, 6, 1, 9, 2, 9, 11, 2, 9, 8, 11, -1, -1, -1, -1 },				//75
            { 6, 3, 11, 6, 5, 3, 5, 1, 3, -1, -1, -1, -1, -1, -1, -1 },				//76
            { 0, 8, 11, 0, 11, 5, 0, 5, 1, 5, 11, 6, -1, -1, -1, -1 },				//77
            { 3, 11, 6, 0, 3, 6, 0, 6, 5, 0, 5, 9, -1, -1, -1, -1 },				//78
            { 6, 5, 9, 6, 9, 11, 11, 9, 8, -1, -1, -1, -1, -1, -1, -1 },			//79
            { 5, 10, 6, 4, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//80
            { 4, 3, 0, 4, 7, 3, 6, 5, 10, -1, -1, -1, -1, -1, -1, -1 },				//81
            { 1, 9, 0, 5, 10, 6, 8, 4, 7, -1, -1, -1, -1, -1, -1, -1 },				//82
            { 10, 6, 5, 1, 9, 7, 1, 7, 3, 7, 9, 4, -1, -1, -1, -1 },				//83
            { 6, 1, 2, 6, 5, 1, 4, 7, 8, -1, -1, -1, -1, -1, -1, -1 },				//84
            { 1, 2, 5, 5, 2, 6, 3, 0, 4, 3, 4, 7, -1, -1, -1, -1 },					//85
            { 8, 4, 7, 9, 0, 5, 0, 6, 5, 0, 2, 6, -1, -1, -1, -1 },					//86
            { 7, 3, 9, 7, 9, 4, 3, 2, 9, 5, 9, 6, 2, 6, 9, -1 },					//87
            { 3, 11, 2, 7, 8, 4, 10, 6, 5, -1, -1, -1, -1, -1, -1, -1 },			//88
            { 5, 10, 6, 4, 7, 2, 4, 2, 0, 2, 7, 11, -1, -1, -1, -1 },				//89
            { 0, 1, 9, 4, 7, 8, 2, 3, 11, 5, 10, 6, -1, -1, -1, -1 },				//90
            { 9, 2, 1, 9, 11, 2, 9, 4, 11, 7, 11, 4, 5, 10, 6, -1 },				//91
            { 8, 4, 7, 3, 11, 5, 3, 5, 1, 5, 11, 6, -1, -1, -1, -1 },				//92
            { 5, 1, 11, 5, 11, 6, 1, 0, 11, 7, 11, 4, 0, 4, 11, -1 },				//93
            { 0, 5, 9, 0, 6, 5, 0, 3, 6, 11, 6, 3, 8, 4, 7, -1 },					//94
            { 6, 5, 9, 6, 9, 11, 4, 7, 9, 7, 11, 9, -1, -1, -1, -1 },				//95
            { 10, 4, 9, 6, 4, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//96
            { 4, 10, 6, 4, 9, 10, 0, 8, 3, -1, -1, -1, -1, -1, -1, -1 },			//97
            { 10, 0, 1, 10, 6, 0, 6, 4, 0, -1, -1, -1, -1, -1, -1, -1 },			//98
            { 8, 3, 1, 8, 1, 6, 8, 6, 4, 6, 1, 10, -1, -1, -1, -1 },				//99
            { 1, 4, 9, 1, 2, 4, 2, 6, 4, -1, -1, -1, -1, -1, -1, -1 },				//100
            { 3, 0, 8, 1, 2, 9, 2, 4, 9, 2, 6, 4, -1, -1, -1, -1 },					//101
            { 0, 2, 4, 4, 2, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//102
            { 8, 3, 2, 8, 2, 4, 4, 2, 6, -1, -1, -1, -1, -1, -1, -1 },				//103
            { 10, 4, 9, 10, 6, 4, 11, 2, 3, -1, -1, -1, -1, -1, -1, -1 },			//104
            { 0, 8, 2, 2, 8, 11, 4, 9, 10, 4, 10, 6, -1, -1, -1, -1 },				//105
            { 3, 11, 2, 0, 1, 6, 0, 6, 4, 6, 1, 10, -1, -1, -1, -1 },				//106
            { 6, 4, 1, 6, 1, 10, 4, 8, 1, 2, 1, 11, 8, 11, 1, -1 },					//107
            { 9, 6, 4, 9, 3, 6, 9, 1, 3, 11, 6, 3, -1, -1, -1, -1 },				//108
            { 8, 11, 1, 8, 1, 0, 11, 6, 1, 9, 1, 4, 6, 4, 1, -1 },					//109
            { 3, 11, 6, 3, 6, 0, 0, 6, 4, -1, -1, -1, -1, -1, -1, -1 },				//110
            { 6, 4, 8, 11, 6, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//111
            { 7, 10, 6, 7, 8, 10, 8, 9, 10, -1, -1, -1, -1, -1, -1, -1 },			//112
            { 0, 7, 3, 0, 10, 7, 0, 9, 10, 6, 7, 10, -1, -1, -1, -1 },				//113
            { 10, 6, 7, 1, 10, 7, 1, 7, 8, 1, 8, 0, -1, -1, -1, -1 },				//114
            { 10, 6, 7, 10, 7, 1, 1, 7, 3, -1, -1, -1, -1, -1, -1, -1 },			//115
            { 1, 2, 6, 1, 6, 8, 1, 8, 9, 8, 6, 7, -1, -1, -1, -1 },					//116
            { 2, 6, 9, 2, 9, 1, 6, 7, 9, 0, 9, 3, 7, 3, 9, -1 },					//117
            { 7, 8, 0, 7, 0, 6, 6, 0, 2, -1, -1, -1, -1, -1, -1, -1 },				//118
            { 7, 3, 2, 6, 7, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//119
            { 2, 3, 11, 10, 6, 8, 10, 8, 9, 8, 6, 7, -1, -1, -1, -1 },				//120
            { 2, 0, 7, 2, 7, 11, 0, 9, 7, 6, 7, 10, 9, 10, 7, -1 },					//121
            { 1, 8, 0, 1, 7, 8, 1, 10, 7, 6, 7, 10, 2, 3, 11, -1 },					//122
            { 11, 2, 1, 11, 1, 7, 10, 6, 1, 6, 7, 1, -1, -1, -1, -1 },				//123
            { 8, 9, 6, 8, 6, 7, 9, 1, 6, 11, 6, 3, 1, 3, 6, -1 },					//124
            { 0, 9, 1, 11, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//125
            { 7, 8, 0, 7, 0, 6, 3, 11, 0, 11, 6, 0, -1, -1, -1, -1 },				//126
            { 7, 11, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//127
            { 7, 6, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//128
            { 3, 0, 8, 11, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//129
            { 0, 1, 9, 11, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//130
            { 8, 1, 9, 8, 3, 1, 11, 7, 6, -1, -1, -1, -1, -1, -1, -1 },				//131
            { 10, 1, 2, 6, 11, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//132
            { 1, 2, 10, 3, 0, 8, 6, 11, 7, -1, -1, -1, -1, -1, -1, -1 },			//133
            { 2, 9, 0, 2, 10, 9, 6, 11, 7, -1, -1, -1, -1, -1, -1, -1 },			//134
            { 6, 11, 7, 2, 10, 3, 10, 8, 3, 10, 9, 8, -1, -1, -1, -1 },				//135
            { 7, 2, 3, 6, 2, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//136
            { 7, 0, 8, 7, 6, 0, 6, 2, 0, -1, -1, -1, -1, -1, -1, -1 },				//137
            { 2, 7, 6, 2, 3, 7, 0, 1, 9, -1, -1, -1, -1, -1, -1, -1 },				//138
            { 1, 6, 2, 1, 8, 6, 1, 9, 8, 8, 7, 6, -1, -1, -1, -1 },					//139
            { 10, 7, 6, 10, 1, 7, 1, 3, 7, -1, -1, -1, -1, -1, -1, -1 },			//140
            { 10, 7, 6, 1, 7, 10, 1, 8, 7, 1, 0, 8, -1, -1, -1, -1 },				//141
            { 0, 3, 7, 0, 7, 10, 0, 10, 9, 6, 10, 7, -1, -1, -1, -1 },				//142
            { 7, 6, 10, 7, 10, 8, 8, 10, 9, -1, -1, -1, -1, -1, -1, -1 },			//143
            { 6, 8, 4, 11, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//144
            { 3, 6, 11, 3, 0, 6, 0, 4, 6, -1, -1, -1, -1, -1, -1, -1 },				//145
            { 8, 6, 11, 8, 4, 6, 9, 0, 1, -1, -1, -1, -1, -1, -1, -1 },				//146
            { 9, 4, 6, 9, 6, 3, 9, 3, 1, 11, 3, 6, -1, -1, -1, -1 },				//147
            { 6, 8, 4, 6, 11, 8, 2, 10, 1, -1, -1, -1, -1, -1, -1, -1 },			//148
            { 1, 2, 10, 3, 0, 11, 0, 6, 11, 0, 4, 6, -1, -1, -1, -1 },				//149
            { 4, 11, 8, 4, 6, 11, 0, 2, 9, 2, 10, 9, -1, -1, -1, -1 },				//150
            { 10, 9, 3, 10, 3, 2, 9, 4, 3, 11, 3, 6, 4, 6, 3, -1 },					//151
            { 8, 2, 3, 8, 4, 2, 4, 6, 2, -1, -1, -1, -1, -1, -1, -1 },				//152
            { 0, 4, 2, 4, 6, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//153
            { 1, 9, 0, 2, 3, 4, 2, 4, 6, 4, 3, 8, -1, -1, -1, -1 },					//154
            { 1, 9, 4, 1, 4, 2, 2, 4, 6, -1, -1, -1, -1, -1, -1, -1 },				//155
            { 8, 1, 3, 8, 6, 1, 8, 4, 6, 6, 10, 1, -1, -1, -1, -1 },				//156
            { 10, 1, 0, 10, 0, 6, 6, 0, 4, -1, -1, -1, -1, -1, -1, -1 },			//157
            { 4, 6, 3, 4, 3, 8, 6, 10, 3, 0, 3, 9, 10, 9, 3, -1 },					//158
            { 10, 9, 4, 6, 10, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//159
            { 4, 9, 5, 7, 6, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//160
            { 0, 8, 3, 4, 9, 5, 11, 7, 6, -1, -1, -1, -1, -1, -1, -1 },				//161
            { 5, 0, 1, 5, 4, 0, 7, 6, 11, -1, -1, -1, -1, -1, -1, -1 },				//162
            { 11, 7, 6, 8, 3, 4, 3, 5, 4, 3, 1, 5, -1, -1, -1, -1 },				//163
            { 9, 5, 4, 10, 1, 2, 7, 6, 11, -1, -1, -1, -1, -1, -1, -1 },			//164
            { 6, 11, 7, 1, 2, 10, 0, 8, 3, 4, 9, 5, -1, -1, -1, -1 },				//165
            { 7, 6, 11, 5, 4, 10, 4, 2, 10, 4, 0, 2, -1, -1, -1, -1 },				//166
            { 3, 4, 8, 3, 5, 4, 3, 2, 5, 10, 5, 2, 11, 7, 6, -1 },					//167
            { 7, 2, 3, 7, 6, 2, 5, 4, 9, -1, -1, -1, -1, -1, -1, -1 },				//168
            { 9, 5, 4, 0, 8, 6, 0, 6, 2, 6, 8, 7, -1, -1, -1, -1 },					//169
            { 3, 6, 2, 3, 7, 6, 1, 5, 0, 5, 4, 0, -1, -1, -1, -1 },					//170
            { 6, 2, 8, 6, 8, 7, 2, 1, 8, 4, 8, 5, 1, 5, 8, -1 },					//171
            { 9, 5, 4, 10, 1, 6, 1, 7, 6, 1, 3, 7, -1, -1, -1, -1 },				//172
            { 1, 6, 10, 1, 7, 6, 1, 0, 7, 8, 7, 0, 9, 5, 4, -1 },					//173
            { 4, 0, 10, 4, 10, 5, 0, 3, 10, 6, 10, 7, 3, 7, 10, -1 },				//174
            { 7, 6, 10, 7, 10, 8, 5, 4, 10, 4, 8, 10, -1, -1, -1, -1 },				//175
            { 6, 9, 5, 6, 11, 9, 11, 8, 9, -1, -1, -1, -1, -1, -1, -1 },			//176
            { 3, 6, 11, 0, 6, 3, 0, 5, 6, 0, 9, 5, -1, -1, -1, -1 },				//177
            { 0, 11, 8, 0, 5, 11, 0, 1, 5, 5, 6, 11, -1, -1, -1, -1 },				//178
            { 6, 11, 3, 6, 3, 5, 5, 3, 1, -1, -1, -1, -1, -1, -1, -1 },				//179
            { 1, 2, 10, 9, 5, 11, 9, 11, 8, 11, 5, 6, -1, -1, -1, -1 },				//180
            { 0, 11, 3, 0, 6, 11, 0, 9, 6, 5, 6, 9, 1, 2, 10, -1 },					//181
            { 11, 8, 5, 11, 5, 6, 8, 0, 5, 10, 5, 2, 0, 2, 5, -1 },					//182
            { 6, 11, 3, 6, 3, 5, 2, 10, 3, 10, 5, 3, -1, -1, -1, -1 },				//183
            { 5, 8, 9, 5, 2, 8, 5, 6, 2, 3, 8, 2, -1, -1, -1, -1 },					//184
            { 9, 5, 6, 9, 6, 0, 0, 6, 2, -1, -1, -1, -1, -1, -1, -1 },				//185
            { 1, 5, 8, 1, 8, 0, 5, 6, 8, 3, 8, 2, 6, 2, 8, -1 },					//186
            { 1, 5, 6, 2, 1, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//187
            { 1, 3, 6, 1, 6, 10, 3, 8, 6, 5, 6, 9, 8, 9, 6, -1 },					//188
            { 10, 1, 0, 10, 0, 6, 9, 5, 0, 5, 6, 0, -1, -1, -1, -1 },				//189
            { 0, 3, 8, 5, 6, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//190
            { 10, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//191
            { 11, 5, 10, 7, 5, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//192
            { 11, 5, 10, 11, 7, 5, 8, 3, 0, -1, -1, -1, -1, -1, -1, -1 },			//193
            { 5, 11, 7, 5, 10, 11, 1, 9, 0, -1, -1, -1, -1, -1, -1, -1 },			//194
            { 10, 7, 5, 10, 11, 7, 9, 8, 1, 8, 3, 1, -1, -1, -1, -1 },				//195
            { 11, 1, 2, 11, 7, 1, 7, 5, 1, -1, -1, -1, -1, -1, -1, -1 },			//196
            { 0, 8, 3, 1, 2, 7, 1, 7, 5, 7, 2, 11, -1, -1, -1, -1 },				//197
            { 9, 7, 5, 9, 2, 7, 9, 0, 2, 2, 11, 7, -1, -1, -1, -1 },				//198
            { 7, 5, 2, 7, 2, 11, 5, 9, 2, 3, 2, 8, 9, 8, 2, -1 },					//199
            { 2, 5, 10, 2, 3, 5, 3, 7, 5, -1, -1, -1, -1, -1, -1, -1 },				//200
            { 8, 2, 0, 8, 5, 2, 8, 7, 5, 10, 2, 5, -1, -1, -1, -1 },				//201
            { 9, 0, 1, 5, 10, 3, 5, 3, 7, 3, 10, 2, -1, -1, -1, -1 },				//202
            { 9, 8, 2, 9, 2, 1, 8, 7, 2, 10, 2, 5, 7, 5, 2, -1 },					//203
            { 1, 3, 5, 3, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//204
            { 0, 8, 7, 0, 7, 1, 1, 7, 5, -1, -1, -1, -1, -1, -1, -1 },				//205
            { 9, 0, 3, 9, 3, 5, 5, 3, 7, -1, -1, -1, -1, -1, -1, -1 },				//206
            { 9, 8, 7, 5, 9, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//207
            { 5, 8, 4, 5, 10, 8, 10, 11, 8, -1, -1, -1, -1, -1, -1, -1 },			//208
            { 5, 0, 4, 5, 11, 0, 5, 10, 11, 11, 3, 0, -1, -1, -1, -1 },				//209
            { 0, 1, 9, 8, 4, 10, 8, 10, 11, 10, 4, 5, -1, -1, -1, -1 },				//210
            { 10, 11, 4, 10, 4, 5, 11, 3, 4, 9, 4, 1, 3, 1, 4, -1 },				//211
            { 2, 5, 1, 2, 8, 5, 2, 11, 8, 4, 5, 8, -1, -1, -1, -1 },				//212
            { 0, 4, 11, 0, 11, 3, 4, 5, 11, 2, 11, 1, 5, 1, 11, -1 },				//213
            { 0, 2, 5, 0, 5, 9, 2, 11, 5, 4, 5, 8, 11, 8, 5, -1 },					//214
            { 9, 4, 5, 2, 11, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//215
            { 2, 5, 10, 3, 5, 2, 3, 4, 5, 3, 8, 4, -1, -1, -1, -1 },				//216
            { 5, 10, 2, 5, 2, 4, 4, 2, 0, -1, -1, -1, -1, -1, -1, -1 },				//217
            { 3, 10, 2, 3, 5, 10, 3, 8, 5, 4, 5, 8, 0, 1, 9, -1 },					//218
            { 5, 10, 2, 5, 2, 4, 1, 9, 2, 9, 4, 2, -1, -1, -1, -1 },				//219
            { 8, 4, 5, 8, 5, 3, 3, 5, 1, -1, -1, -1, -1, -1, -1, -1 },				//220
            { 0, 4, 5, 1, 0, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//221
            { 8, 4, 5, 8, 5, 3, 9, 0, 5, 0, 3, 5, -1, -1, -1, -1 },					//222
            { 9, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//223
            { 4, 11, 7, 4, 9, 11, 9, 10, 11, -1, -1, -1, -1, -1, -1, -1 },			//224
            { 0, 8, 3, 4, 9, 7, 9, 11, 7, 9, 10, 11, -1, -1, -1, -1 },				//225
            { 1, 10, 11, 1, 11, 4, 1, 4, 0, 7, 4, 11, -1, -1, -1, -1 },				//226
            { 3, 1, 4, 3, 4, 8, 1, 10, 4, 7, 4, 11, 10, 11, 4, -1 },				//227
            { 4, 11, 7, 9, 11, 4, 9, 2, 11, 9, 1, 2, -1, -1, -1, -1 },				//228
            { 9, 7, 4, 9, 11, 7, 9, 1, 11, 2, 11, 1, 0, 8, 3, -1 },					//229
            { 11, 7, 4, 11, 4, 2, 2, 4, 0, -1, -1, -1, -1, -1, -1, -1 },			//230
            { 11, 7, 4, 11, 4, 2, 8, 3, 4, 3, 2, 4, -1, -1, -1, -1 },				//231
            { 2, 9, 10, 2, 7, 9, 2, 3, 7, 7, 4, 9, -1, -1, -1, -1 },				//232
            { 9, 10, 7, 9, 7, 4, 10, 2, 7, 8, 7, 0, 2, 0, 7, -1 },					//233
            { 3, 7, 10, 3, 10, 2, 7, 4, 10, 1, 10, 0, 4, 0, 10, -1 },				//234
            { 1, 10, 2, 8, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//235
            { 4, 9, 1, 4, 1, 7, 7, 1, 3, -1, -1, -1, -1, -1, -1, -1 },				//236
            { 4, 9, 1, 4, 1, 7, 0, 8, 1, 8, 7, 1, -1, -1, -1, -1 },					//237
            { 4, 0, 3, 7, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//238
            { 4, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//239
            { 9, 10, 8, 10, 11, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//240
            { 3, 0, 9, 3, 9, 11, 11, 9, 10, -1, -1, -1, -1, -1, -1, -1 },			//241
            { 0, 1, 10, 0, 10, 8, 8, 10, 11, -1, -1, -1, -1, -1, -1, -1 },			//242
            { 3, 1, 10, 11, 3, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//243
            { 1, 2, 11, 1, 11, 9, 9, 11, 8, -1, -1, -1, -1, -1, -1, -1 },			//244
            { 3, 0, 9, 3, 9, 11, 1, 2, 9, 2, 11, 9, -1, -1, -1, -1 },				//245
            { 0, 2, 11, 8, 0, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//246
            { 3, 2, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//247
            { 2, 3, 8, 2, 8, 10, 10, 8, 9, -1, -1, -1, -1, -1, -1, -1 },			//248
            { 9, 10, 2, 0, 9, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//249
            { 2, 3, 8, 2, 8, 10, 0, 1, 8, 1, 10, 8, -1, -1, -1, -1 },				//250
            { 1, 10, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//251
            { 1, 3, 8, 9, 1, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },			//252
            { 0, 9, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//253
            { 0, 3, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 },		//254
            {-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 }		//255
        };                                                                                                   
~~~

> 来源自http://paulbourke.net/geometry/polygonise/

要使用这个数组我们需要先统一立方体顶点和边上的序号：

<img src="Textures\Marching Cubes Vert.png" alt="Marching Cubes Vert.png" style="zoom:50%;" />

在上图中，我们通过密度场采样得知编号为1、5、7的顶点位于体积内，我们将1、5、7转换为二进制的10100010，即十进制的162，通过查上面的表得 { 5, 0, 1, 5, 4, 0, 7, 6, 11}，这串数字有9个有效值，它的含义是创建3个三角形，这3个三角形的顶点分别位于边序号5、0、1，边序号5、4、0和边序号7、6、11上。

通过顶点编号生成Triangulation索引得代码如下：

~~~C#
int cubeIndex = 0;
for(int i = 0; i < 8; i++)
{
    if(cube.values[i] == 1)
    {
        cube |=  1 << i;
    }
}
~~~

得到了边所在得序号后，我们要想办法获得生成三角形所需得结点，这就要使用另外得一个表来获得边对应的两个端点的序号：

~~~C#
        public static int[,] indexE2V = {
            {0,1},{1,2},{2,3},{3,0},
            {4,5},{5,6},{6,7},{7,4},
            {0,4},{1,5},{2,6},{3,7}
        };
~~~

在逻辑密度场中，只需要先对一个小立方体上的八个顶点采样并构造Triangulation索引，然后查索引表得到边对应的三角形序列，然后根据查E2V表获得三角形顶点所在边的两个立方体顶点坐标，然后取两个顶点的中点作为三角形顶点即可。为了生成整个密度场的网格，我们需要将整个密度场分割成足够多的小立方体，对每个小立方体进行这个操作，从而将整个密度场分割成一定数量的三角形。遍历小立方体的这个过程就被称为**步进(marching)**。

如果我们升级到浮点密度场，我们会发现，立方体两个顶点的中点并不一定是三角形顶点准确的位置，为此我们需要在步进时进行插值。假设我们将浮点值0.0设为表面的密度值，用负数表达处于物体内部，用正数表达处于物体外部，那么我们可以根据这两个采样值的绝对值来表达表面位于两个点之间的位置。例如两个顶点的采样值分别为3和-2，那么我们可以知道三角形顶点应该在更靠近-2的位置。假设插值过程是线性的，我们可以得到这样的插值公式：
$$
P_t = \frac{|W_A|}{|W_A|+|W_B|}P_B
$$

##### 如何并行实现步进立方体算法? 

我们以浮点型密度场为例，展示在Unity中，配合Compute Shader对步进立方体算法的实现：

+ 密度数据结构

我们需要一个数据结构来在CPU中储存密度信息，这些数据可能来自外界输入，也可能是通过算法生成(比如3D噪声图)。

~~~C#
[System.Serializable]
public class Volume
{
    #region 可修改的参数和数据
    /// <summary>
    /// 储存体积
    /// </summary>
    public float[] data;

    /// <summary>
    /// 展示箱的大小
    /// </summary>
    public Vector3 Size;
    /// <summary>
    /// 每个方向上点的个数
    /// </summary>
    public Vector3Int SamplesDensity
    {
        get { return samplesDensity; }
        set
        {
            samplesDensity = value;
        }
    }
    public Volume(Vector3 size, Vector3Int density)
    {
        Size = size;
        SamplesDensity = density;
        data = new float[SamplesCount];
    }

     #region 属性
    /// <summary>
    /// 每个方向上Cube的个数
    /// </summary>
    public Vector3Int VoxelsDensity => SamplesDensity - Vector3Int.one;
    /// <summary>
    /// 单个Cube的大小
    /// </summary>
    public Vector3 VoxelSize => new Vector3(Size.x / (float)VoxelsDensity.x, Size.y / (float)VoxelsDensity.y, Size.z / (float)VoxelsDensity.z);
    /// <summary>
    /// 点的总个数
    /// </summary>
    public int SamplesCount => SamplesDensity.x * SamplesDensity.y * SamplesDensity.z;
    /// <summary>
    /// Cube的总个数
    /// </summary>
    public int VoxelCount => VoxelsDensity.x * VoxelsDensity.y * VoxelsDensity.z;
    /// <summary>
    /// 每个方向上线程的个数
    /// </summary>
    public Vector3Int VoxelThreadCount => new Vector3Int(Mathf.CeilToInt((float)VoxelsDensity.x / 8f), Mathf.CeilToInt((float)VoxelsDensity.y / 8f), Mathf.CeilToInt((float)VoxelsDensity.z / 8f));
    /// <summary>
    /// 每个方向上线程的个数
    /// </summary>
    public Vector3Int SamplesThreadCount => new Vector3Int(Mathf.CeilToInt((float)SamplesDensity.x / 8f), Mathf.CeilToInt((float)SamplesDensity.y / 8f), Mathf.CeilToInt((float)SamplesDensity.z / 8f));
    #endregion

    private int GetIndex(Vector3Int v)
    {
        return (v.y * SamplesDensity.x + v.x) * SamplesDensity.z + v.z;
    }

    /// <summary>
    /// 获得某点的密度
    /// </summary>
    /// <param name="v"></param>
    /// <returns></returns>
    public float this[Vector3Int v]
    {
    get
        {
            if (v.x >= SamplesDensity.x || v.y >= SamplesDensity.y || v.z >= SamplesDensity.z) throw new System.IndexOutOfRangeException();
            int index = GetIndex(v);
            return data[index];
        }
        set
        {
            if (v.x >= SamplesDensity.x || v.y >= SamplesDensity.y || v.z >= SamplesDensity.z) throw new System.IndexOutOfRangeException();
            int index = GetIndex(v);
            data[index] = value;
        }
    }

    public static void Serialize(Volume v, string path)     {
        using (FileStream fs = new FileStream(path, FileMode.OpenOrCreate))
        {
            BinaryFormatter bf = new BinaryFormatter();
            bf.Serialize(fs, v);
            fs.Close();
            fs.Dispose();
        }
    }

    public static Volume Deserialize(string path)
    {
        Volume v;
        using (FileStream fs = new FileStream(path, FileMode.Open))
        {
            BinaryFormatter bf = new BinaryFormatter();
            v = (Volume)bf.Deserialize(fs);
            fs.Close();
            fs.Dispose();
        }
        return v;
    }
}
~~~

+ 读取密度数据

这里我们将密度数据转换成带坐标数据和密度数据的点，储存在ComputeBuffer中。
由于ComputeBuffer位于GPU中，而GPU读取显存比读取CPU缓存快得多，所以在之后的运算中，我们会将数据一直暂存在GPU中。

  ~~~C#
	public void InitBuffers(Volume volume)
    {
        this.volume = volume;

        pointsBuffer = new ComputeBuffer(volume.SamplesCount, sizeof(float) * 4);
        triangleBufferA = new ComputeBuffer(volume.VoxelCount * 5, sizeof(float) * 3 * 3, ComputeBufferType.Append);
        triangleBufferB = new ComputeBuffer(volume.VoxelCount * 5, sizeof(float) * 3 * 3, ComputeBufferType.Append);

        frontBuffer = triangleBufferA;
        backBuffer = triangleBufferB;

        frontBuffer.SetCounterValue(0);
        backBuffer.SetCounterValue(0);
    }
  ~~~

为了方便Marching Cube的实现，我们将顶点坐标也储存在GPU缓冲区里，这是典型的用空间换时间的做法，这样在密度场被修改时，系统不需要重复计算顶点的坐标，以此提升实时渲染的效率。

~~~hlsl
// Each #kernel tells which function to compile; you can have many kernels
#pragma kernel ReadData
#pragma kernel WriteData

static const int numThreads = 8;

int numPointsX;
int numPointsY;
int numPointsZ;
float3 cellSize;

RWStructuredBuffer<float> datas;
RWStructuredBuffer<float4> points;

int indexFromCoord(int x, int y, int z) {
    return y * numPointsX * numPointsZ + x * numPointsZ + z;
}  

[numthreads(numThreads,numThreads,numThreads)]
void ReadData (int3 id : SV_DispatchThreadID)
{
    if (id.x >= numPointsX || id.y >= numPointsY || id.z >= numPointsZ) {
        return;
    }

    float3 pos = id * cellSize - float3(numPointsX-1, numPointsY-1, numPointsZ-1) * cellSize * 0.5f;

    int index = indexFromCoord(id.x, id.y, id.z);

    float value = datas[index];

    points[index] = float4(pos, value);
}


[numthreads(numThreads,numThreads,numThreads)]
void WriteData (int3 id : SV_DispatchThreadID)
{
    float3 pos = id * cellSize;

    int index = indexFromCoord(id.x, id.y, id.z);

    float value = points[index].w;

    datas[index] = value;
}
~~~

在CPU里调用这个Compute Shader：

~~~C#
    public void ReadData(Volume volume)
    {
        this.volume = volume;

        Vector3Int threadCount = volume.SamplesThreadCount;

        ComputeBuffer dataBuffer = new ComputeBuffer(volume.SamplesCount, sizeof(float));
        transfer.SetInt("numPointsX", volume.SamplesDensity.x);
        transfer.SetInt("numPointsY", volume.SamplesDensity.y);
        transfer.SetInt("numPointsZ", volume.SamplesDensity.z);
        transfer.SetVector("cellSize", volume.VoxelSize);
        dataBuffer.SetData(volume.data);
        transfer.SetBuffer(0, "datas", dataBuffer);
        transfer.SetBuffer(0, "points", pointsBuffer);
        transfer.Dispatch(0, threadCount.x, threadCount.y, threadCount.z);
        //理论上,pointsBuffer已经设置完毕

        dataBuffer.Dispose();
    }
~~~

+ 实现Marching Cube算法

~~~hlsl
#pragma kernel March
#include "/Includes/MarchTables.compute"

static const int numThreads = 8;

struct Triangle {
    float3 vertexC;
    float3 vertexB;
    float3 vertexA;
    uint3 cubeID;
};

AppendStructuredBuffer<Triangle> triangles;
RWStructuredBuffer<float4> points;

float3 base;

int numPointsX;
int numPointsY;
int numPointsZ;

float isoLevel;

float3 interpolateVerts(float4 v1, float4 v2) {
    float t = (isoLevel - v1.w) / (v2.w - v1.w);
    return v1.xyz + t * (v2.xyz-v1.xyz);
}

int indexFromCoord(int x, int y, int z) {
    return y * numPointsX * numPointsZ + x * numPointsZ + z;
}

[numthreads(numThreads,numThreads,numThreads)]
void March (int3 id : SV_DispatchThreadID)
{   
    int3 bas = int3((int)base.x, (int)base.y, (int)base.z);

    // Stop one point before the end because voxel includes neighbouring points
    if (id.x >= numPointsX-1 || id.y >= numPointsY-1 || id.z >= numPointsZ-1) {
        return;
    }

    id += bas;

    // 8 corners of the current cube
    float4 cubeCorners[8] = {
        points[indexFromCoord(id.x, id.y, id.z)],
        points[indexFromCoord(id.x + 1, id.y, id.z)],
        points[indexFromCoord(id.x + 1, id.y, id.z + 1)],
        points[indexFromCoord(id.x, id.y, id.z + 1)],
        points[indexFromCoord(id.x, id.y + 1, id.z)],
        points[indexFromCoord(id.x + 1, id.y + 1, id.z)],
        points[indexFromCoord(id.x + 1, id.y + 1, id.z + 1)],
        points[indexFromCoord(id.x, id.y + 1, id.z + 1)]
    };

    // Calculate unique index for each cube configuration.
    // There are 256 possible values
    // A value of 0 means cube is entirely inside surface; 255 entirely outside.
    // The value is used to look up the edge table, which indicates which edges of the cube are cut by the isosurface.
    int cubeIndex = 0;
    if (cubeCorners[0].w < isoLevel) cubeIndex |= 1;
    if (cubeCorners[1].w < isoLevel) cubeIndex |= 2;
    if (cubeCorners[2].w < isoLevel) cubeIndex |= 4;
    if (cubeCorners[3].w < isoLevel) cubeIndex |= 8;
    if (cubeCorners[4].w < isoLevel) cubeIndex |= 16;
    if (cubeCorners[5].w < isoLevel) cubeIndex |= 32;
    if (cubeCorners[6].w < isoLevel) cubeIndex |= 64;
    if (cubeCorners[7].w < isoLevel) cubeIndex |= 128;

    // Create triangles for current cube configuration
    for (int i = 0; triangulation[cubeIndex][i] != -1; i +=3) {
        // Get indices of corner points A and B for each of the three edges
        // of the cube that need to be joined to form the triangle.
        int a0 = cornerIndexAFromEdge[triangulation[cubeIndex][i]];
        int b0 = cornerIndexBFromEdge[triangulation[cubeIndex][i]];

        int a1 = cornerIndexAFromEdge[triangulation[cubeIndex][i+1]];
        int b1 = cornerIndexBFromEdge[triangulation[cubeIndex][i+1]];

        int a2 = cornerIndexAFromEdge[triangulation[cubeIndex][i+2]];
        int b2 = cornerIndexBFromEdge[triangulation[cubeIndex][i+2]];

        Triangle tri;
        tri.vertexA = interpolateVerts(cubeCorners[a0], cubeCorners[b0]);
        tri.vertexB = interpolateVerts(cubeCorners[a1], cubeCorners[b1]);
        tri.vertexC = interpolateVerts(cubeCorners[a2], cubeCorners[b2]);
        tri.cubeID = id;
        triangles.Append(tri);
    }
}
~~~

注意，在这里使用了结构体Triangle，是因为由于GPU的并行运算特性，假如我们直接将顶点输出到顶点数组，我们就不能准确的控制接连产生的三个顶点是否恰好位于同一个三角形中。

在CPU端调用这个compute shader：

~~~C#
    public void MarchAll()
    {
        Vector3Int threadCount = volume.VoxelThreadCount;
        frontBuffer.SetCounterValue(0);
        march.SetBuffer(0, "points", pointsBuffer);
        march.SetBuffer(0, "triangles", frontBuffer);
        march.SetVector("base", Vector4.zero);
        march.SetInt("numPointsX", volume.SamplesDensity.x);
        march.SetInt("numPointsY", volume.SamplesDensity.y);
        march.SetInt("numPointsZ", volume.SamplesDensity.z);
        march.SetFloat("isoLevel", isoLevel);
        march.Dispatch(0, threadCount.x, threadCount.y, threadCount.z);

        numTris = GetNumTris(frontBuffer);
    }

	int GetNumTris(ComputeBuffer triangleBuffer)
    {
        int[] triCountArray = { 0 };
        ComputeBuffer triCountBuffer = new ComputeBuffer(1, sizeof(int), ComputeBufferType.Raw);
        ComputeBuffer.CopyCount(triangleBuffer, triCountBuffer, 0);
        triCountBuffer.GetData(triCountArray);
        int numTris = triCountArray[0];
        triCountBuffer.Dispose();
        return numTris;
    }
~~~

+ 三角形的解包

前面我们为了防止并行操作产生的异常，将顶点以Triangle结构体的格式进行了输出，现在我们需要将它们解包成顶点数组：

~~~hlsl
// Each #kernel tells which function to compile; you can have many kernels
#pragma kernel UnpackTriangles

struct Triangle {
    float3 vertexC;
    float3 vertexB;
    float3 vertexA;
    uint3 cubeID;
};

int triangleCount;

RWStructuredBuffer<float3> points;

RWStructuredBuffer<Triangle> triangles;


[numthreads(256,1,1)]
void UnpackTriangles (int3 id : SV_DispatchThreadID)
{
    if(id.x >= triangleCount) return;

    Triangle tri = triangles[id.x];
    
    points[id.x * 3] = tri.vertexC;
    points[id.x * 3 + 1] = tri.vertexB;
    points[id.x * 3 + 2] = tri.vertexA;
}
~~~

这段代码用于调用这个Compute Shader：

~~~C#
Vector3[] UnpackTriangles(ComputeBuffer triangleBuffer, int numTris)
    {
        Vector3[] vertices = new Vector3[numTris * 3];
        if (numTris == 0) throw new System.Exception("The numTris input is 0.");
        ComputeBuffer verticeBuffer = new ComputeBuffer(numTris * 3, sizeof(float) * 3);
        unpacker.SetInt("triangleCount", numTris);
        unpacker.SetBuffer(0, "triangles", triangleBuffer);
        unpacker.SetBuffer(0, "points", verticeBuffer);
        unpacker.Dispatch(0, Mathf.CeilToInt(numTris / 64f), 1, 1);
        verticeBuffer.GetData(vertices);
        verticeBuffer.Dispose();
        return vertices;
    }
~~~

+ 顶点数组压缩

显然，我们输出的顶点数组具有大量的重复顶点。通过一个压缩算法为它生成索引数组，能有效的对网格的大小进行压缩：

~~~C#
Vector3[] UnpackTriangles(ComputeBuffer triangleBuffer, int numTris)
    {
        Vector3[] vertices = new Vector3[numTris * 3];
        if (numTris == 0) throw new System.Exception("The numTris input is 0.");
        ComputeBuffer verticeBuffer = new ComputeBuffer(numTris * 3, sizeof(float) * 3);
        unpacker.SetInt("triangleCount", numTris);
        unpacker.SetBuffer(0, "triangles", triangleBuffer);
        unpacker.SetBuffer(0, "points", verticeBuffer);
        unpacker.Dispatch(0, Mathf.CeilToInt(numTris / 64f), 1, 1);
        verticeBuffer.GetData(vertices);
        verticeBuffer.Dispose();
        return vertices;
    }
~~~

+ 生成网格


~~~C#
public Mesh GenerateMesh()
    {
        Mesh mesh = new Mesh();

        Vector3[] vertices = UnpackTriangles(frontBuffer, numTris);
        int vertexCount = CompressVertices(ref vertices, out int[] triangles);

        if(vertexCount >= 65536) mesh.indexFormat = UnityEngine.Rendering.IndexFormat.UInt32;     
        else mesh.indexFormat = UnityEngine.Rendering.IndexFormat.UInt16;

        mesh.vertices = vertices;
        mesh.triangles = triangles;

        mesh.RecalculateNormals();

        mesh.bounds = new Bounds(Vector3.zero, volume.Size);

        return mesh;
    }
~~~

我们也可以尝试在纯CPU端实现步进立方体算法，但需要一些针对性的优化。

结合八叉树对步进立方体算法进行优化，为了避免穷举，我们可以尽可能抛弃所有保证不会生成三角形的体积很大的部分。

首先，我们要构造这样的一棵八叉树：每一个叶子结点指向一个Cube，同时储存Cube结点的最小值和最大值；每个内部结点指向它的八个子结点，并储存这八个子结点的最小值和最大值。这样，对每一个细分层次的八叉树，我们都可以快速知道一整个区域的Cube中采样点的密度最大值和最小值。显然，假如生成面的阈值是t，当且仅当$v_{min}\le t\le v_{max}$时，这个结点代表的区域需要进行进一步的细分，否则可以将整个区域抛弃。

在进行Marching Cube算法时，我们从根节点开始深度遍历八叉树，抛弃所有$v_{min}\gt t$或$v_{max}<t$的结点，然后对所有没有被抛弃的叶子结点进行查表并生成三角形。

显然，为了构建这棵八叉树，我们仍然需要对整个密度场进行一次遍历。但在对密度场产生修改后，重新调整八叉树的消耗就低得多。当一个密度点被修改时，我们可以自底向上的调整整个八叉树储存的$v_{min}$和$v_{max}$s。

进一步优化这颗八叉树，我们可以用一个线性数组代替指针来储存这颗八叉树。同时，还可以通过让体积数据的布局更适应八叉树的遍历顺序代替矩阵格式储存，提升内存的相应效率。

我们在生成顶点时，根据线段两端的密度值计算了点在线段上的位置。但对每个这样的线段来说，总是存在4个相邻的Cube，也就是说这个线段总是会被插值4次。所以，当第一次访问一个边缘时，可以计算该边缘上等值面的顶点，然后将边缘和顶点一起储存在一个哈希表中。这样，每当需要边上的顶点时，可以先尝试在哈希表中查找该边缘。而当第4次访问边缘时，即可知道它再也不可能被访问，所以就可以将其从哈希表中移除，以此保证哈希表的大小维持在一个相对低的状态。由于这个优化方法需要用到一个全局哈希表，所以并不适合直接用于GPU加速的Marching Cube，但在纯CPU运算中非常实用。

##### 什么是滚雪球算法？



##### 什么是错切-变形算法？



#### 体积生成

##### 三角网格如何生成距离场函数?

三角网格的SDF生成，有两大类方法。一个是基于波动方程，一个是距离变换法。

接下来我们尝试使用波动方程法解网格体距离场问题。

> 原论文：https://www.pnas.org/content/pnas/93/4/1591.full.pdf

##### 三角网格如何生成密度场?



<div STYLE="page-break-after:always;"></div>

## 场景渲染

### 纹理和采样

#### 图像储存与压缩

##### 可以怎样储存图像?

显然，图像由像素组成。在计算机中，几乎所有的像素都是由RGB和RGBA组成的。RGB每个像素是24位，RGBA每个像素是32位。虽然像素的组成没有变化，但不同格式的图片储存像素的方式是不同的。

图像是二维数组，而内存是一维的，不同的图像格式会将像素按不同的顺序排列。主流的排列顺序有两种，第一种是DirectX和OpenGL、OpenCV等都承认的行优先从上往下排列，即左上角的像素作为数组的第一个元素，右下角的像素作为数组的最后一个元素；第二种是bmp和windows下的GDI、GDI+格式定义的行优先从下往上排列，即左下角的像素作为数组的第一个元素，右上角的像素作为数组的最后一个元素。这两者互相转换的公式为：$y_2=height-1-y_1$。

每个像素中RGBA四个成分在硬盘、内存中的排列顺序不同。而最常见的排列方法只有RGB、BGR、RGBA、BGRA这几种。绝大多数图形库或环境是BGR和BGRA排列。

在实时渲染系统中，图像的宽和高会有特别的要求。

**由于图像通常使用BC系列压缩法的缘故，图像的像素宽度和像素高度必须为4的倍数**。如果其宽度和高度不是4的倍数，就必须添加额外的冗余像素来将其补充到4的倍数，这无疑浪费了空间，而这些空间本可以用来增强细节。**如果该图像需要计算Mipmap，则更应该保证其长和宽都为2的整数次幂**，因为如果不这么做，就难以保证每层Mipmap的长宽为4的倍数，会导致在Mipmap的压缩过程中浪费了比平常还要多的空间。

图像在储存时通常都需要进行压缩，图像的压缩方式一般可以用以下三种方法进行分类。

+ **有损的和无损的**

  有损压缩表示在压缩图像大小的过程中损失了图片的信息，也降低了图片的质量。且这种损失是不可逆的。常见的有损压缩手段是按照一定的算法将相邻的像素点合并。

  无损压缩指像素的质量没有损耗，我们任何时候都可以完整的还原无损压缩后的信息。

+ **使用索引色的和使用直接色的**

  索引色指用一个数字来表示一种颜色，在储存图像时每个像素用一个索引而非RGBA四位来记录颜色。这种方法在图像上颜色数量较少时压缩效果明显，但只能储存有限颜色，通常是256种。最主流的压缩算法一般是基于索引色的有损压缩算法，将不同颜色的像素近似为相同颜色并进行索引色压缩。

  直接色指每位像素都用完整的RGBA来记录颜色。当然，在损失可以接受的条件下，RGBA并不一定是用8位储存的。

+ **点阵图和矢量图**

  点阵图也就是位图，是最常用的图像储存方式。位图缩放会失真，因为位图是由像素点阵组成的，在渲染时储存的像素会直接被显示在显示器的指定位置。

  矢量图并不记录像素的信息，而是记录了对图像中形状的描述。在打开矢量图时，需要先经过渲染，将这些描述转换为像素信息，才能被显示出来。图像信息越规则、简洁，矢量图的储存体积就越小；图像信息越无序、复杂，矢量图的储存体积就越大，甚至可能超过点阵图。

##### 有哪些图像压缩格式？

+ ##### *.BMP

  BMP是BitMap的缩写，是无损的，既支持索引色也支持直接色的点阵图。

  BMP由文件图头、结构头和像素列表组成。BMP格式是几乎不压缩的，BMP格式下可以使用索引来对颜色压缩，但必须保证整幅图的颜色严格小于$2^{2^n}$，如一幅图上所有像素的取值不超过256种，则可以将整幅图以索引格式压缩为平均每个像素1字节左右(因为索引表也需要空间)。假如说一幅图上所有像素的取值不超过$2^{16}$种，也可以以索引格式将图像压缩为平均每个像素2字节左右。

  BitMap是常见位图格式中占用空间最多的，现在除了在Windows操作系统中还比较常见之外，几乎没有再使用BMP的环境。

+ ***.JPEG**

  JPEG是Joint Photographic Experts Group的缩写，是有损的、采用直接色的、点阵图。JPEG和JPG是完全相同的格式，后者是前者在DOS时代8.3文件命名规则中简化后缀名规则时的产物。

  JPEG并不支持RGB模式，而是支持YUV模式，前面我们提到过，YUV颜色空间更匹配人眼视觉，这使得压缩后的色彩损失难以被人眼注意到。在压缩前会将RGB颜色转换到YUV空间，然后对图片进行降采样，一般采样比为2:1:1或4:2:2，经过降采样后JPEG的精度就损失了一半但空间也缩减了一半。

  之后图片会被划分为多个8*8的矩阵，对每一个矩阵进行DCT变换(离散余弦变换，傅里叶变换的变体，由于图形学实际上不涉及JPEG的加解码，不在此深究)，得到一个频率系数矩阵。

  将浮点数类型的频率系数矩阵量化为整数矩阵，在这一步图片会与原图片产生一定差异，也就是失真。

  最后将压缩后的数据按某种编码方案排列储存，JPEG标准允许任意选择编码方案，常用的是哈夫曼编码，这一步实际上并不影响压缩体积。

  JPEG是一种灵活的图片压缩方式，用户可以自由选择压缩比，压缩比越大，品质就越差，用户可以根据实际需要进行权衡。

+ ***.GIF**

  GIF全称Graphics Interchange Format，是无损的、采用索引色的、点阵图。

  GIF采用**LZW(Lempel-ZivWalch)压缩算法**对图像进行压缩，提供了JPEG没有的透明度通道，并创造性的允许在一个文件中存放多幅彩色图片，这就是GIF以动图闻名的原因。

  由于GIF格式仅支持8bit的索引色，即在整个图片中，只能存在256种不同的颜色。这使得GIF不适用于储存色彩丰富的照片等信息，更适合存放logo和线框图等。

+ ***.PNG**

  PNG全称Portable Network Graphics，PNG-8是PNG的索引色版本。PNG-8是无损的、使用索引色的、点阵图。PNG也存在直接色版本PNG-24，他是无损的、使用直接色的、点阵图。
  
  PNG-8是非常好的GIF格式替代者，除非需要动画的支持，否则我们没有理由使用GIF而不是PNG-8，因为在相同的图片效果下，PNG-8具有更小的文件体积。除此之外，PNG-8还支持透明度的调节，而GIF并不支持。
  
  从显示效果上来看，PNG-24跟BMP没有不同。PNG-24的优点在于，它压缩了图片的数据，使得同样效果的图片，PNG-24格式的文件大小要比BMP小得多。当然，一般而言，PNG-24的文件大小是JPEG的五倍之多，不过相比JPEG的优势在于它支持透明通道。

+ ***.SVG**

  SVG全称Scalable Vector Graphics，是使用 XML 来描述二维图形和绘图程序的语言，这也代表了SVG 图像可通过文本编辑器来创建和修改。SVG代码可以在浏览器直接打开，可以直接嵌入HTML代码。

  开发者可以通过使用SVG预定义的图形元素来绘制图形，包括矩形、圆形、椭圆、线端、折线、多边形、路径等。每种图形都提供了很多参数，如矩形提供了圆角半径和填充色等参数。而路径(Path)则提供了最多样化的程序化图形生成方案。因为SVG不是图形学考虑的重点在此不赘述。

+ ***.DXF**

  DXF是CAD设计的一种ASCII编码格式。
  
  Dxf文件的最小单元是码-值对，码和值各占一行。码的定义和SVG中的图形类似，DXF中保存的信息包括了直点、线、圆、圆弧、折线、多边形、曲线等。

##### 有哪些纹理压缩格式？

尽管像jpg、png的压缩率很高，但并不适合纹理，主要问题是不支持像素的随机访问，而前面提到的很多图像压缩格式都必须在访问像素时对整幅图像进行解码。

目前**DirectX平台下的纹理压缩通常使用DXT或者BC系列标准**，它们都是基于GPU着色架构的支持随机访问的压缩算法。其中，由于BC1实际上等同于DXT1、BC2实际上等同于DXT3、BC3实际上等同于DXT5，鉴于此，我们仅介绍BC系列的算法。除了BC系列和DXT系列外，还有**OpenGL平台上的ETC**、**IOS平台上的PVRTC**和**跨平台的开源标准ASTC**等纹理压缩格式。

BC的全称是**Block Compression(块压缩)**，是一种有损压缩算法。其中的Block指的是4x4个像素组成的矩形，在压缩时BC系列会以Block为单位进行逐个压缩，换句话说，在解压时也可以以Block为单位逐个解压，因此可以支持随机访问。压缩时，纹理会被切成独立的4x4小块，这些数据被压缩后会被储存在连续的一个内存块中。这种标准的块布局是为了让GPU能高效的使用这种格式渲染，由于GPU会顺序的读取数据，所以流压缩算法和压缩率可变算法不适用于这种应用场景。这种布局的另一种好处是，由于在采样时通常具有连续性，即采集某个纹素后很可能会采集与它相邻的纹素，将它们一起压缩可以方便GPU对L1缓存和L2缓存进行管理。

BC系列格式基于这样的一个假设：认为在一个块(4x4像素)中，颜色基本不变。所以在块压缩算法对每个块构建了一个**调色板(Palette)**，调色板包含了有限的几种颜色，而块中的每个像素只使用一个索引来获取调色板上的某种颜色。一般来说，调色板是在RGB线性空间中的一条线段，将线段等距离细分为若干段，则像素保存的索引就可以表达在线段上不同位置的颜色。综上所述，**BC系列压缩后的每个Block包括两种主要数据：调色板线段两段的色彩，以及每个像素在调色板上采样使用的索引**。

BC算法一共有7个标准，分别记位BC1、BC2、BC3、BC4、BC5、BC6、BC7，**这7个标准相互独立，适用于不同格式的纹理，彼此之间没有谁更先进之分**。

BC1标准储存RGB数据，并支持1bit的Alpha通道(就是只支持完全透明或完全不透明)，它使用8字节64个比特来储存每个Block的16个像素，使其平均压缩效率为每像素4比特。调色板线段的两个端点以RGB5:6:5的格式储存在总共4个字节中，如果使用Alpha通道，则改为RGBA5:5:5:1的格式仍然储存在4个字节中，而每个像素的索引占2位，可以表示在调色板线段上的四种色彩。对绝大部分标准彩色图，也就是漫反射图来说，BC1是压缩率最高的压缩格式。

BC2储存RGBA数据，每个块使用16个字节。其前8个字节使用BC1标准储存RGB部分，然后将每个像素使用4位未压缩的Alpha通道储存在后8个字节中。

BC3标准储存RGBA数据，是BC1和BC4的混合。它使用16个字节来储存每个Block，其中前8个字节使用BC1标准储存RGB部分，后8个字节使用BC4储存Alpha部分。这个标准常用在需要完整Alpha通道的半透明漫反射贴图中，或者用在需要将颜色纹理和灰度纹理打包在一起的纹理如视差纹理中。

BC4标准储存灰度图像，也就是只保存一个色彩通道，每个块使用8个字节。调色板线段的两个端点分别使用一个字节，并且每个像素的索引占3位，可以表示在调色板线段上的8种亮度。这使得在保存灰度图时BC4的效果比其它标准好很多，常用在深度纹理、金属度纹理等灰度纹理的压缩中。

BC5是一种双通道模式，只储存两个色彩通道，每个块使用16个字节，其中前8个字节和后8个字节分别使用BC4格式储存两个通道。由于法线纹理可以只储存两个方向，然后在运行中重构第三个方向，所以BC5常用在法线纹理的压缩中。

BC6是BC6H的简写，它是一种基于HDR的RGB压缩格式，每个块使用16个字节。其中调色板线段的两个端点以RGB16:16:16的浮点格式储存在总共12个字节中，其中每个通道的16位由1位符号位+5位指数位+10位尾数位组成。每个像素的索引占2位，可以表示在调色板线段上的四种色彩。

BC7是一种灵活的格式，号称纹理智能压缩算法，它将每个块压缩到16个字节，并根据每个块的情况选择压缩的方案。BC7在压缩时，每个Block都需要分定一个Mode，BC7一共有8种Mode，每种Mode有完全不同的编码方式，精确定义每一个bit的编码含义，有的Mode支持Alpha通道，有的不支持，有的支持HDR，有的不支持。而且，颜色排列顺序不一定是RGBA，如果某个通道需要的精度比Alpha更高，就会将Alpha通道和该通道互换，每个块使用2个bit储存Rotation信息，这里的Rotaion表示哪个通道和Alpha进行了互换。然后，按照Mode将Block细分为最多3个**子集(Subset)**，每个子集使用类似BC1的方式进行压缩。按照排列组合来说，一共有$3^{16}$种划分子集的方法，BC7在设计时选择了64种划分方案，称为Partition，每个Block都必须分配一个Partition。将这些情况考虑在一起，BC7的压缩速率远低于前六种，但解压速度并没有落后很多，所以大多数纹理都可以使用BC7自动压缩。

| 格式 | 块压缩大小 | 压缩率  | 质量 | 描述                                         | 例子             |
| ---- | ---------- | ------- | ---- | -------------------------------------------- | ---------------- |
| BC1  | 64 bit     | 4bpp    | 中等 | 调色板线段端点RGB5:6:5或RGBA5:5:5:1，2位索引 | 漫反射纹理       |
| BC2  | 128 bit    | 8bpp    | 中等 | 64 bit BC1 RGB + 每像素4位未压缩Alpha        | 已过时           |
| BC3  | 128 bit    | 8bpp    | 中等 | 64 bit BC1 RGB + 64 bit BC4 A                | 视差纹理         |
| BC4  | 64 bit     | 4bpp    | 良好 | 调色板线段端点R8，3位索引                    | 灰度图如深度图   |
| BC5  | 128 bit    | 8bp良好 | 良好 | 64 bit BC4 R + 64 bit BC4 G                  | 法线纹理         |
| BC6H | 128 bit    | 8bpp    | 优秀 | 调色板线段端点RGB16:16:16(1:5:10)，2位索引   | HDR              |
| BC7  | 128 bit    | 8bpp    | 优秀 | 根据Mode使用BC1-BC6的混合编码                | 几乎适用任何纹理 |

**ETC也是一种块压缩算法，将4x4的块压缩为8个字节**。它不是使用调色板线段来压缩块中的颜色，而是使用基准色+偏移色的方法。ETC将4x4的块分成两个2x4或两个4x2的子块(由一个flip位控制水平或垂直划分)，使用2xRGB4:4:4或者RGB3:3:3+RGB5:5:5来记录子块的基准色，基准色的大小由子块内像素的平均值决定。然后每个子块定义了3位修饰位，修饰位的大小由子块内像素的方差决定，每个像素使用2位索引位来选取一个偏移值：

| 修饰位 | 索引位00 | 索引位01 | 索引位10 | 索引位11 |
| ------ | -------- | -------- | -------- | -------- |
| 000    | -8       | -2       | 2        | 8        |
| 001    | -17      | -5       | 5        | 17       |
| 010    | -29      | -9       | 9        | 29       |
| 011    | -42      | -13      | 13       | 42       |
| 100    | -60      | -18      | 18       | 60       |
| 101    | -80      | -24      | 24       | 80       |
| 110    | -106     | -33      | 33       | 106      |
| 111    | -183     | -47      | 47       | 183      |

像素在解码时，根据自己的索引位和所属子块的修饰位取得自己的偏移色Off，然后在基准色上叠加偏移色还原原图颜色:

 ~~~hlsl
Color = BaseColor + float3(Off, Off, Off);
 ~~~

**ASTC是一个与BC7类似的智能块压缩算法**，ASTC将每块固定压缩为128位，但不同的是ASTC块的大小可变，可选ASTC4x4到ASTC12x12不同压缩级别的方案。同时ASTC还支持不同分类的贴图、支持HDR和LDR，支持跨平台、甚至支持3D纹理压缩。ASTC的压缩非常复杂，分了很多可变的配置数据，在此不深入探究。

**PVRTC不是块压缩算法**，它将整幅纹理分为了高频和低频信号。其中，低频信号(就是非细节部分)由两张16倍降采样(在两个维度上各缩小四倍)的图像A和B表示，而高频信号由全分辨率且每像素2仅位的M图记录。在解码时，A和B图像通过双线性插值【见滤波模式】放大到原大小，然后根据M上的权值进行混合。PVRTC将图像分成若干个4x4的块，每个块使用64位进行压缩，前16位储存图像A，然后15位储存图像B，第31位为Mode位，最后32位储存M图。其中，图像A使用RGB5:5:5或ARGB3:4:4:4编码，图像B使用RGB5:5:4或ARGB3:4:4:3编码，使用RGB还是ARGB模式由每部分的第一位决定。虽然PVRTC将纹理分成了若干块，但它仍不是块压缩算法，因为在采样时，必须读取相邻的4个PVRTC块，才能根据它们的A值和B值通过双线性插值解码它们之间的5x5块。换句话说，**PVRTC比块压缩算法消耗4倍的GPU带宽**。

![PVRTC](Textures\PVRTC.jpg)

#### 纹理坐标和寻址

##### 什么是纹素？

我们经常听到纹理这个词。显然，纹理一般是一副图，并且是一副位图，但它的本质并不是一副位图那么简单。

我们都知道，位图由一系列像素组成，而像素由三个或四个通道组成。任何一个具有图形显示功能的软件，都会将像素中的通道分别按照RGBA处理，然后传给显示器，并由显示器在对应的位置上渲染出颜色。所以，一般来说我们都会认为像素是用来保存颜色的，而位图是用来保存颜色的组合的。

但我们必须抛开平常看待位图的方式去看待纹理————虽然纹理中每个点储存的仍然是RGBA这四个通道，但我们更愿意将它们理解成4(3)个8bit的通道，而不仅仅是一个32(24)位的颜色信息。或者说，我们将一副图看作一个二维数组，这个数组中的每一项由32(24)位的结构组成，而这个结构具体用于储存颜色或是什么别的信息，则由开发者自己决定。纹理中的每个像素块和位图中的每个像素块虽然结构一致，但功能不同，于是，我们将前者称为**纹素(texel)**以与**像素(pixel)**进行区分。

##### 什么是纹理坐标？

纹理坐标就是给纹理上的每一个纹素一个坐标。

显然，纹理坐标应当与纹理的大小无关，任意分辨率的图像都应当能合理的在纹理坐标系统中工作。所以一般来说纹理坐标使用归一化的值，即[0,1]×[0,1]，并且横坐标用u表示，纵坐标用v表示，所以纹理坐标也被称为UV坐标。举例说明，如果有一副512×1024的图像，当它作为纹理时，像素坐标的(256,512)就会被映射为纹理坐标的(0.5,0.5)。

一般来说，美术进行建模时，会进行名为UV裁剪，俗名展UV的操作。这个操作的意义就是通过3D软件，用形象化的方式设置模型上每个顶点的UV坐标。

![展UV](E:/Textures/图形学/展UV.png)

当顶点UV设定好后，我们就可以通过GPU的插值功能轻松的得到片元的UV坐标。在着色器层面，我们可以通过片元的UV坐标获得纹理中某点的数据，这就是纹理寻址。如图所示：

![UV映射](E:/Textures/图形学/UV映射.gif)

一般来说，所有顶点的UV坐标最终都会落于上面所说的[0,1]×[0,1]的范围内。但是也有例外，技术人员必须确保超出范围的UV值也能与某个纹素相对应，这就引入了**重复模式(Wrapping mode)**的概念。

##### 如何生成纹理坐标？

提到纹理坐标，常会提到一个名为**三平面投影(Tri-planar)**的算法。但这个算法事实上并不是一个可以生成顶点纹理坐标的算法，**三平面投影算法仅是一个渲染算法**，尽管在进行地形编辑时这个算法依然有用。**三平面投影算法旨在防止纹理出现因UV拉扯导致的扭曲**，**它使用片元世界坐标xy,xz,yz对漫反射贴图进行了3次采样，将它们线性混合得到一个基本可信的新漫反射颜色**。当然，解决类似的问题也可以使用FBM调制，这更能避免大规模重复的贴图引起的视觉不适。

相比三平面投影算法，柱坐标投影和球坐标投影是一类好理解的生成顶点纹理坐标的算法，在不考虑顶点和顶点的关系的情况下，可以自动根据顶点的XYZ坐标获得一组UV：

+ 柱坐标投影：将模型空间坐标转换到柱坐标$(\rho,\phi, z)$，然后将UV坐标烘焙为$u=\phi，v=z$
+ 球坐标投影：将模型空间坐标转换到球坐标$(r,\theta,\phi)$，然后将UV坐标烘焙为$u=\theta, v=\phi$

当然，在投影时可以对网格预先进行平移、旋转、缩放来尽可能地生成均等的展开图。

另一种基于立方体栈开图的UV展开算法是在六个方向上对模型做投影，然后根据物体表面模型空间法线的情况，选择六边形上的一个面，将该点投影到这个面上。选择模型空间法线的三个分量中绝对值最大的那一个，然后根据其符号决定投影到哪个面，以法线$n=(0.8,0.5,0.2)$为例，由于x分量绝对值最大且为正数，所以投影到立方体标注为+X的平面上:

![Cubic Mapping](Textures\Cubic Mapping.png)

接下来将投影得到的坐标做归一化，分析一个面上顶点的位置，取得其坐标的最大和最小值，就自然可以将每个面上的UV都归一到[0,1]之间。最后，按照所需的细节程度缩放投影得到的六个矩形，将它们排列在[0,1]的UV空间中。



##### 什么是重复模式？

重复模式用来定义当UV坐标超出[0,1]×[0,1]时，对纹素进行寻址的方式。

+ 缺省重复模式：忽略整数，重复纹理。
+ 重复镜像模式：当纹理坐标的整数部分是奇数时使用镜像重复。
+ 边缘截断模式：当纹理坐标值大于边缘时，截取到边缘处的纹素，形成一个被拉伸的边缘。
+ 边界截断模式：范围外的纹理坐标会用指定值(一般是全0，也就是黑色)填充。

如图所示：

![重复模式](Textures\重复模式.jpg)

##### 什么是滤波模式？

纹素坐标是整数值，但UV坐标并不是，所以我们必须有一个算法来决定如何GPU如何根据顶点或片元的纹理坐标，找到对应的纹素值。

+ 最近相邻滤波(Nearest neighbor filtering，或者Point sampling)：UV坐标乘以纹理大小，然后四舍五入取最近的纹素。最近邻方法获取的纹素看起来有明显的像素块，一般只用于像素风格的图像。在使用Mipmap时，一般会选择最接近的Mipmap层次再进行采样，这可以有效解决闪烁和锯齿。
+ 双线性滤波(Bilinear filtering，或者liner filtering)：根据一个坐标到其附近最近的四个像素点的距离进行加权平均，得到一个混合的颜色，如下图所示：

![线性滤波](E:/Textures/图形学/线性滤波.jpg)

+ 三线性滤波(Trilinear filtering)：专门被用于解决远距离采样时Mipmap层次突变的问题。思路是在相邻的两个Mipmap层次上分别进行双线性滤波，再对得到的两个值进行线性插值。这个算法比双线性滤波效果更好但消耗翻倍。注意，当距离足够小和足够大，超过了Mipmap最大和最小的层次时，三线性滤波会退化为双线性滤波。
+ 各项异性滤波(Anisotropic filtering)：当一个目标表面和透视摄像机之间的角度较大时，纹理的填充面积实际上并不是正方形，这引入了模糊和闪烁等瑕疵。于是，各项异性滤波需要考虑在一个非正方体区域内进行采样和混合，在一些简单的实现中，显卡使用长方形的纹理代替正方形，达到了较好的近似效果。但是这种方法在处理边界时仍不理想，原因是在倾斜的表面上，近端边界比远端边界拥有更多的像素。于是一些高端显卡使用了梯形纹理，当然这也要求更大的运算量。

#### 纹理应用

##### 有哪些常用纹理类型？

从根本上来说，在着色器层次，任何一种各项异性特性的实现，都可以依靠一个纹理实现。一般来说，我们也把这些纹理称为**贴图**。

+ **漫反射纹理**：储存漫反射信息的纹理，使用最正宗的RGB颜色信息，是最五彩斑斓的纹理类型，用于表达物体表面本身的颜色。
+ **高度纹理**：使用一副灰度图储存每个点表面和原始表面的高度差。但是实际上，在实时渲染中，我们并不会真的通过改变物体表面的顶点坐标来实现凹凸效果，因为顶点坐标的改变必然是在顶点着色器中实现的，而片元着色器计算光照时的高度是根据线性插值而不是采样得来的，这使得高度纹理的细节丢失很明显。最简单的处理方法是直接把高度纹理叠加在渲染好的表面上，造成亮度上的扰动，从而让人以为是凹凸的。
+ **位移纹理**：这个贴图是针对曲面细分着色器实现的。曲面细分流水线可以在顶点离摄像机足够近且面向摄像机时，计算出额外的顶点。这些顶点除了能增强模型的光滑等级外，也可以实现对模型表面细节的实际修改。在高度纹理中我们提到，顶点坐标的改变是在顶点着色器中实现的，而片元着色器的高度则是由顶点插值而来的，在具有曲面细分功能的GPU中，这个问题不再是问题，因为只要顶点足够密集，模型表面的细节就可以被更精密的采样到。在ZBrush中曲面细分的这个特点就被表现得淋漓尽致。
+ **法线纹理**：法线纹理用RGB三个通道分别储存法线向量在XYZ轴上的分量大小，从而保存了仅通过线性插值可能会丢失掉的细节信息，在顶点数有限的条件下实现更真实的光照。我们提到的法线纹理一般都指切线空间下的法线纹理，RGB通道分别对应切线空间下的XYZ轴分量，也就是切线方向、法线方向和反切线方向。由于法线贴图中绝大多数纹素的方向都是朝向法线方向，或与法线方向接近的，所以法线贴图多半都是蓝蓝的。由于法线方向是归一化的，我们可以仅储存两个方向的值再推导得出第三个方向，这使得法线贴图具有特殊的压缩方式。当然，使用梯度算法，我们也可以快速的根据高度纹理推算出法线纹理。
+ **映射纹理**：映射纹理是一个用于非真实化渲染的经典案例。映射纹理也可以被称为渐变纹理，它将整个UV空间视作一个函数空间，将纹素的值视作函数$f(u,v)$的因变量，得到了$(u,v)\rightarrow c$的一个映射，其中c可以被解释为RGBA的色彩值也可以被解释成任意32(24)位的信息。渐变纹理的应用场景举例：1、将漫反射强度映射为突变的色调，实现卡通风格渲染；2、将亮度映射为新的亮度曲线，实现对比度增强；3、在不同亮度上渲染不同的纹理，实现各项异性渲染或素描风格渲染等。映射纹理中最常用的的一个是**RampMap**，即坡道贴图，被用于进行个性化的漫反射渲染(见【BRDF】)。
+ **遮罩纹理**：用于配合其它纹理或特性，可以将它理解为实现特殊功能的映射纹理。比如说，金属度遮罩纹理使用一副灰度图储存物体的金属性，对这个贴图采样可以快速的模拟出具有各项异性的金属材质，但是相比使用光照模型计算各项异性显然占用更大的储存空间，并且也有增加Draw Call的危险。也有很多情况下将高度纹理作为高光遮罩，控制高光强度，实现模拟表面凹凸的效果。除此之外，遮罩纹理还可以用于混合多张图片，例如在地形材质上可能就需要对多种地形贴图进行混合。

##### 如何使用纹理储存浮点数据?

根据所需精度的不同，我们可以选择储存四通道的小数或储存双通道的小数。

为了方便着色器的运算，由于在矢量运算架构中四个通道一般被视为四个8位(或更多位)的分量而不是一个32位的整体，我们不会选择使用IEEE 754标准来编码浮点数，而是使用一种适合矢量运算架构的定点数转换标准来对数值进行加码和解码。

简单描述这个转换标准：将四维向量的每个分量，看作在255进制中的四个位，则RGBA四个分量分别储存$255^{-1},255^{-2},255^{-3},255^{-4}$四个阶的值。

我们以四通道的编码和解码为例，考虑编码浮点数$V=0.428 892 506 335 670 286 168 815 553 376 87$，第一步我们需要把这个浮点数$V$乘以255来得到映射值$M^R=109.367 5891155959229730479661111$，然后我们将$M^R$的整数部分$M^R_I=109$送入$R$通道。第二步，取$M^R$小数部分$M^R_F=0.367 5891155959229730479661111$，得$M^G=M^R_F\times 255=93.7352244769603581272313583305$，$M^G_I=93$。以此类推，得到$M^B_I=187$和$M^A_I=122$，最终得到RGBA四维向量$(109,93,187,122)$，化为浮点数即$(0.427,0.365,0.733,0.478)$。

将数字解码为浮点数，则执行上述操作的逆操作，得：
$$
V_{\mathrm{decode}}=\frac{M^R_I}{255}+\frac{M^G_I}{255^2}+\frac{M^B_I}{255^3}+\frac{M^A_I}{255^4}
$$
将向量$(109,93,187,122)$代入后，得到$V_{\mathrm{decode}}=0.42889250610587918969443775580356$，最终误差小于$2.30\times 10^{-10}$。

利用了矢量运算架构的并行性，我们在进一步简化了这个算法，使其不必进行四次乘法和取整：

~~~HLSL
inline float4 EncodeFloatRGBA(float v)
{
	float4 kEncodeMul = float4(1.0, 255.0, 65025.0, 16581375.0);
	float kEncodeBit = 1.0 / 255.0;
	float4 enc = kEncodeMul * v; 
	enc = frac(enc);
	enc -= enc.yzww * kEncodeBit;
	return enc;
}
~~~

着重讲解encode的计算逻辑：对所需结果的R分量来说，它标准值等于$\lfloor v * 255\rfloor/255$，由于取的是浮点形式，所以结果也可以理解为是在$v$的基础上减去了一部分$d$，在这个算法中，我们用enc.y * kEncodeBit近似了$d$。之所以说是近似，是因为实际的$d$应该包括在取整时被截掉的整个浮点部分，而enc.y * kEncodeBit只统计了精度为1/255的部分。类似的，讨论G分量，它的标准值应为$\lfloor v*65025\rfloor /255$，在frac那一步时，整数部分是被统计在R分量里的低精度部分，通过frac函数被截去，再减去被统计在B分量里的高精度部分，以此类推。在计算A分量时，用w自己近似估计了更高精度的分量。

解码方法比较好理解：

~~~HLSL
inline float DecodeFloatRGBA(float4 enc)
{
	float4 kDecodeDot = float4(1.0, 1/255.0, 1/65025.0, 1/16581375.0);
	return dot(enc, kDecodeDot);
}
~~~

使用简化算法的误差理论上更大，但用同样的数据测试，加码再解码后的误差约为$1.13\times 10^-{10}$，比原算法还精确，可见对于一般用途来说，使用新算法产生的精度误差在可接受范围内。

而双通道加解码则是四通道的简化版：

~~~HLSL
inline float2 EncodeFloatRG(float v)
{
	float2 kEncodeMul = float2(1.0, 255.0);
	float kEncodeBit = 1.0 / 255.0;
	float2 enc = kEncodeMul * v; 
	enc = frac(enc);
	enc.x -= enc.y * kEncodeBit;
	return enc;
}

inline float DecodeFloatRG(float2 enc)
{
	float2 kDecodeDot = float2(1.0, 1/255.0);
	return dot(enc, kDecodeDot);
}
~~~

##### 如何使用纹理储存法线数据？

法线纹理和高度纹理都是储存物体表面凹凸属性的纹理，它们最直接的作用是在次世代游戏中，通过法线或高度纹理保存细节计算光照，以此避免使用过多的顶点导致性能下降。

高度纹理在生效时，一般会直接转化为法线纹理使用。给定一张高度图H，对于图上的点H(i,j)，我们可以知道其沿UV两个方向的向量是：
$$
U(i,j)=(1,0,aH(i+1,j)-aH(i-1,j))\\
V(i,j)=(0,1,aH(i,j+1)-aH(i,j-1))
$$
其中a为控制参数。我们可以叉乘得到其法向量：

$$
N(i,j)=\frac{U(i,j)\times V(i,j)}{||U(i,j)\times V(i,j)||}=\frac{(-U_z,-V_z,1)}
{\sqrt{U_z^2+V_z^2+1}}
$$
这样的到的法向量会比较尖锐，我们可以用**Sobel滤波**得出更好的法线纹理效果，即对于高度图的每个像素，考虑它与水平方向和竖直方向上的像素差，将差值当作该点对应的法线在x和y方向上的位移，归一后将其储存到法线纹理的r和g分量即可。
$$
U(i,j)=(H(i+1,j)-H(i-1,j),0,0)\\
V(i,j)=(0,0,H(i,j+1)-H(i,j-1))\\
N(i,j)=Normalize(U(i,j)+V(i,j)+(0,1,0))
$$

高度纹理的主要优势在于一些与高度有关的特效或逻辑运算中，比如储存地形高度一般使用高度纹理而不是法线纹理。注意，单独使用高度纹理时不能直接根据高度纹理的数据在顶点着色器中修改顶点坐标，因为在顶点着色器中修改了顶点坐标后，再在片元着色器中使用转换后的法线贴图进行光照计算会产生异常。

法线纹理储存切线空间下的法线方向(**切线的方向等于uv坐标下u的方向，对应切线空间的x轴；副法线方向等于uv坐标下v的方向，对应切线空间的y；法线则对应切线空间的z**)，它的主流的压缩方式是BC5格式和DXT5(BC3)格式。DXT5的储存方式是在贴图的RGBA纹素的各个通道分别储存**(1,y,1,x)**，而BC5格式则是**(x,y,0,1)**​。这两种格式都基于**块状压缩(Block compression)**思想，所以都要求贴图大小是$4\times4$大小的整数倍。

在Unity中可以用同一个函数解压这两种格式：

~~~HLSL
fixed3 UnpackNormalmapRGforAG(fixed4 packednormal)
{
	packednormal.x *= packednormal.w;
	
	fixed3 normal;
	normal.xy = packednormal.xy * 2 - 1;
	normal.z = sqrt(1 - saturate(dot(normal.xy, normal.xy)));
	return normal;
}
~~~

法线贴图的生成被称为**烘焙法线贴图**。一般来说，美术开发人员会先制作一个将用于实时渲染的底面数的模型，称为“低模”，然后在低模的基础上再制作一个不会用在实时渲染中的高面数的“高模”，然后将高模的细节以法线贴图的形式储存下来，在运行时使用低模进行效率更高的渲染，但通过法线贴图来实现类似高模的视觉效果。

**烘焙法线贴图，需要事先分配好低模的UV坐标**，而高模则不需要分配UV坐标，并保证高模基本叠加在低模上。在烘焙时，烘焙程序会沿低模表面法线发射射线并与高模的内表面相交，这条射线的距离通常是有限的，这是为了避免侦测到远处的无关三角形，这个距离通常被称为**烘焙距离**或**笼距离**。在检测到相交后，在高模的三角形上通过插值获得对应点的法线，将这个高模法线记录在基于低模UV的一副纹理上，这就是法线纹理的生成：

<img src="Textures\烘焙法线纹理.jpg" alt="烘焙法线纹理.jpg" style="zoom:67%;" />

由于在法线烘焙过程中需要进行大量相交计算，自然的我们会想到利用光追加速硬件来加速这一过程。GPU加速法线烘焙中使用了基于体素的GPU加速相交测试方法。鉴于此，整个GPU填充格式包括一个以3D纹理格式填充的高模三角形索引索引(指向该体素对应的三角形的序列的首地址)，高模三角形索引数组(三角形序列，每两个Cell之间用-1隔开，指向顶点数组和法线数组的地址)，一个高模的顶点坐标数组和高模的顶点法线数组：

![GPU加速法线烘焙填充](Textures\GPU加速法线烘焙填充.png)

填充完毕后，接下来的烘焙在多次迭代(多Pass)中完成：第一个Pass根据法线纹理的分辨率遍历所有像素点，通过像素点对应的UV坐标对应的顶点坐标和法线坐标，生成若干光线射线的表达式，并获得光线对应体素的初始位置和迭代方向、迭代步长。第二个以及接下来的若干个Pass，继续遍历所有UV点，在一张步进贴图上更新步进距离和光线坐标，在对应Cell中进行相交测试，如果相交则将世界空间法线转换到切线空间渲染到法线纹理中。

##### 如何使用法线纹理计算光照？

法线纹理中储存的是切线空间中的法线方向，所以在计算光照时我们需要对法线所在的坐标空间进行变换，使它与其它光照要素处于同一坐标空间下。

按照数学变换的思路，这个操作自然是交给矩阵完成，那么我们该如何构造矩阵呢？在**坐标空间变换**一节中我们已经推出了公式：
$$
M_{c-p}=\begin{bmatrix}\mid&\mid&\mid&\mid\\
\vec{V_{px}}&\vec{V_{py}}&\vec{V_{pz}}&\vec{P_c}\\
\mid&\mid&\mid&\mid\\0&0&0&1
\end{bmatrix}
$$
复习公式中符号的含义，V~px~、V~py~、V~pz~分别代表子坐标系的三个基向量在父坐标系中的表示，P~c~代表子坐标系的原点在父坐标系中的坐标，得到从子坐标系向父坐标系转换的坐标变换矩阵M~c-p~。

这里将切线空间作为模型坐标空间的子坐标系，显然，V~px~、V~py~、V~pz~分别对应了在模型坐标下的切线、副法线、法线向量，P~c~则为模型坐标下的顶点坐标。我们尝试在Unity Shader中实现切线空间下的光照计算(光照公式详见【真实感渲染-BRDF】)：

~~~ShaderLab
Shader "Custom/TangentLighting"

Properties
{
	_Color ("Color", Color) = (1,1,1,1)
	_MainTex ("Main Tex" 2D) = "white" {}
	_NormalMap ("Normal Map" 2D) = "bump" {}
	_Specular ("Specular", Color) = (1,1,1,1)
	_Gloss ("Gloss", Range(8.0, 256)) = 20
}
SubShader
{
	Pass
	{
		Tags{"LightMode"="ForwardBase"}
		CGPROGRAM
			#pragma vertex vert
			#pragma fragment frag
			
			#include "UnityCG.cginc"
			#include "Lighting.cginc"
			
			uniform fixed4 _Color;
			uniform sampler2D _MainTex;
			uniform fixed4 _MainTex_ST;
			uniform sampler2D _NormalMap;
			uniform fixed4 _NormalMap_ST;
			uniform fixed4 _Specular;
			float _Gloss;
			
			struct a2v
			{
				float4 vertex : POSITION;
				float4 normal : NORMAL;
				float4 tangent : TANGENT;
				float4 texcoord : TEXCOORD0;
			}
			
			struct v2f
			{
				float4 pos : SV_POSITION;
				float4 uv : TEXCOORD0;
				float4 lightDir : TEXCOORD1;
				float4 viewDir : TEXCOORD2;
			}
			
			v2f vert(a2v v)
			{
				v2f o;
				o.pos = mul(UNITY_MATRIX_MVP, v.vertex);
				
				o.uv.xy = TRANSORM_TEX(v.texcoord, _MainTex);
				o.uv.zw = TRANSORM_TEX(v.texcoord, _NormalMap);
				
				//这里计算了模型坐标下的副法线，并计算了从模型空间转换到切线空间的变换矩阵rotation
				float3 binormal = cross(normalize(v.mormal), normalize(v.tangent.xyz)) * v.tangent.w;
				float3x3 rotation = float3x3(v.tangent.xyz, binormal.xyz, v.normal);
				
				o.lightDir = mul(rotaion, ObjSpaceLightDir(v.vertex)).xyz;
				o.viewDir = mul(rotaion, ObjSpaceViewDir(v.vertex)).xyz;
				
				return o;
			}
			
			fixed4 frag(v2f i) : SV_TARGET
			{
				fixed3 tangentLightDir = normalize(i.lightDir);
				fixed3 tangentViewDir = normalize(i.viewDir);
				
				fixed4 packedNormal = tex2D(_NormalMap, i.uv.zw);
				fixed3 tangentNormal = UnpackNormal(packedNormal);
				
				fixed3 albedo = tex2D(_MainTex, i.uv).tgb * _Color.rgb;
				fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo;
				
				fixed3 diffuse = _LightColor0.rgb * albedo * max(0, dot(tangentLightDir, tangentViewDir));
				
				fixed3 halfDir = normalize(tangentLightDir + tangentViewDir);
				fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0, dot(tangentNormal, halfDir)), _Gloss);
				
				return fixed4(ambient + diffuse + specular, 1.0);
			}
		
		ENDCG
	}
}
~~~

前面我们在顶点着色器中将所有光照转换到了切线空间中处理，但如果我们因为某种需要而必须使用世界坐标下的光照，我们就需要获得一个新的法线变换矩阵来将贴图中的法线变换到世界空间坐标中。注意，我们即将变换的是法线，所以我们需要复习一下法线变换的知识，**法线变换所用的矩阵是普通空间变换所用矩阵的逆转置矩阵**，现在我们用$M_{W\rightarrow T}$表示从世界空间向切线空间转换的变换矩阵，比照上面的例子，我们很容易得到：
$$
M_{W\rightarrow T}=\begin{bmatrix}\mid&\mid&\mid&\mid\\
\vec{V_{px}}&\vec{V_{py}}&\vec{V_{pz}}&\vec{P_c}\\
\mid&\mid&\mid&\mid\\0&0&0&1
\end{bmatrix}
$$
其中V~px~、V~py~、V~pz~分别对应了在世界坐标下的切线、副法线、法线向量，P~c~则为世界坐标下的顶点坐标。

接下来计算从切线空间到世界空间使用的法线变换矩阵$G_{T\rightarrow W}$：
$$
G_{T\rightarrow W}=(M_{T\rightarrow W}^{-1})^T=((M_{W\rightarrow T}^{-1})^{-1})^T=M_{W\rightarrow T}^T
$$
也就是说，我们只需要使用$M_{W\rightarrow T}$的转置矩阵，就可以将法线从切线空间转换到世界坐标空间了。由于对法线的采样应该在片元中进行，我们将$M_{W\rightarrow T}$矩阵的转置作为参数从顶点传递到片元。

我们提到过法线纹理在储存时可以进行压缩，在Unity Shader中我们使用UnpackNormal函数来将其解压，完整的代码如下：

~~~ShaderLab
Shader "Custom/WorldLighting"

Properties
{
	_Color ("Color", Color) = (1,1,1,1)
	_MainTex ("Main Tex" 2D) = "white" {}
	_NormalMap ("Normal Map" 2D) = "bump" {}
	_Specular ("Specular", Color) = (1,1,1,1)
	_Gloss ("Gloss", Range(8.0, 256)) = 20
}
SubShader
{
	Pass
	{
		Tags{"LightMode"="ForwardBase"}
		CGPROGRAM
			#pragma vertex vert
			#pragma fragment frag
			
			#include "UnityCG.cginc"
			#include "Lighting.cginc"
			
			uniform fixed4 _Color;
			uniform sampler2D _MainTex;
			uniform fixed4 _MainTex_ST;
			uniform sampler2D _NormalMap;
			uniform fixed4 _NormalMap_ST;
			uniform fixed4 _Specular;
			float _Gloss;
			
			struct a2v
			{
				float4 vertex : POSITION;
				float4 normal : NORMAL;
				float4 tangent : TANGENT;
				float4 texcoord : TEXCOORD0;
			}
			
			struct v2f
			{
				float4 pos : SV_POSITION;
				float4 uv : TEXCOORD0;
				float4 TtoW0 : TEXCOORD1;
				float4 TtoW1 : TEXCOORD2;
				float4 TtoW2 : TEXCOORD3;
			}
			
			v2f vert(a2v v)
			{
				v2f o;
				o.pos = mul(UNITY_MATRIX_MVP, v.vertex);
				
				o.uv.xy = TRANSORM_TEX(v.texcoord, _MainTex);
				o.uv.zw = TRANSORM_TEX(v.texcoord, _NormalMap);
				
				//计算世界坐标系下的法线、切线、副法线
				float3 worldPos = mul(_Object2World, v.vertex).xyz;
				float3 worldNoraml = UnityObjectToWorldNormal(v.nonrmal);
				float3 worldTangnet = UnityObjectToWorldDir(v.tangent.xyz);
				float3 worldBinormal = cross(worldNormal, worldTangent) * v.tangent.w;
				
				//TtoW0、TtoW1、TtoW2可以直接组装成Mw-t的转置矩阵
				o.TtoW0 = float4(worldTangnet.x, worldBinormal.x, worldNormal.x, worldPos.x);
				o.TtoW1 = float4(worldTangnet.y, worldBinormal.y, worldNormal.y, worldPos.y);
				o.TtoW2 = float4(worldTangnet.z, worldBinormal.z, worldNormal.z, worldPos.z);
				
				return o;
			}
			
			fixed4 frag(v2f i) : SV_TARGET
			{
			
				float3 worldPos = float3(i.TtoW0.w, i.TtoW1.w, i.TtoW2.w);
				
				fixed3 lightDir = normalize(UnityWorldSpaceLightDir(worldPos));
				fixed3 viewDir = normalize(UnityWorldSpaceViewDir(worldPos));
				
				fixed3 normal = UnpackNormal(tex2D(_NormalMap, i.uv.zw));
				fixed3x3 TtoW = fixed3x3(i.TtoW0.xyz, i.TtoW1.xyz, i.TtoW2.xyz);
				normal = normalize(mul(TtoW, normal));
				
				fixed3 albedo = tex2D(_MainTex, i.uv).rgb * _Color.rgb;
				fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo;
				
				fixed3 diffuse = _LightColor0.rgb * albedo * max(0, dot(lightDir, viewDir));
				
				fixed3 halfDir = normalize(lightDir + viewDir);
				fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0, dot(normal, halfDir)), _Gloss);
				
				return fixed4(ambient + diffuse + specular, 1.0);
			}
		
		ENDCG
	}
}
~~~

##### 什么是视差纹理和视差映射？

**视差纹理是一种增强的法线纹理，它是在法线纹理的基础上，在Alpha通道中储存了高度/深度信息**。旨在通过在渲染某片元时偏移uv坐标，采样该点周围的纹理，来实现视觉上的几何体高低视差，这一步被称为**视差映射**。其存在的意义是解决法线贴图在侧对物体表面凸起部分时产生的穿帮问题：

<img src="Textures\法线纹理穿帮问题.png" alt="法线纹理穿帮问题" style="zoom:50%;" />

简单来说，视察映射就是要根据表面法线贴图和凹凸纹理的情况，**在遮挡发生时修正采样所用的uv**，避免渲染出本应被遮挡的表面，如图中A点是物体表面的一点，我们希望通过偏移实习让片元A采样到B点对应的纹理，以此渲染出A点被B点遮挡的效果：

<img src="Textures\视差映射.png" alt="视差映射" style="zoom:50%;" />

视差映射算法关心如何从点A获得点B的纹理坐标，也就是要获得图中的Offset值。由于我们要做的偏移Offset是应用在uv空间中的，所以接下来我们的讨论全部基于切线空间，因为在切线空间中计算可以方便的分离在u分量和v分量的偏移。在此，我们关注从A点出发指向B点上方的向量$\vec v$，由于这个向量必定指向摄像机，所以我们已知该向量的方向，只要再求出其长度，自然就可以得到在U方向和V方向上需要的偏移量Offset。可惜的是，$\vec v$的长度没有一个精确的解法，只能进行估算。

**一种朴素的视差映射算法使用H(A)，也就是A点对应的高度值来估算向量$\vec v$的长度**：

~~~ShaderLab
inline float2 ParallaxUvDelta(v2f i)
{
	// 视差图中的alpha描述高度数据，_ParallaxScale是高度纹理的倍率
	half h = tex2D(_ParallaxMap, i.uv).a * _ParallaxScale;
	// 切线空间中的视线方向
	float3 viewDir = normalize(i.viewDir);
	// 将三维的视线向量投影到二维的 uv 平面，乘以高度数据
	float2 delta = viewDir.xy / viewDir.z * h;
	return delta;
}

......
//在片元着色器中的调用
float2 uvDelta = ParallaxUvDelta(i);
i.uv += uvDelta;
//获得新的uv后，用新的uv对法线、漫反射等贴图采样
~~~

当然，在上图中可以发现，粗暴的使用H(A)估算$\vec v$的长度起到的作用及其有限，这个估算是很不准确的。

**一种进阶的视差映射被称为视差遮蔽映射，这种算法使用了高度步进的思想，每次步进固定的高度来判断是否将接触到表面：**

<img src="Textures\视差遮蔽映射.png" alt="视差遮蔽映射" style="zoom:50%;" />

为了取到距离摄像机最近的目标表面，我们需要**从一个接近摄像机的采样点开始步进**，每次步进一定的高度，直到采样点的高度低于目标表面的高度，即采样点本身的高度小于采样点对应的在高度纹理中的高度，此时采样点已经进入目标表面内部，也就是说明点B对应的UV就在最近的两次采样点对应的UV坐标之间，我们可以对最后两次采样的UV根据其对应的高度纹理值插值，得到更精确估计的B点UV：

~~~ShaderLab
inline float2 ParallaxUv(v2f i)
{
	// 切线空间中的视线方向
	float3 viewDir = normalize(i.viewDir);
	// _ParallaxScale调节高度变化的倍率，_Step调节步进的最多次数，每次步进高度为1/_Step，_Step > 1
	float stepHeight = _ParallaxScale / _Step;
	// 得到在步进深度为_stepHeight的情况下，UV的步进量
	float2 deltaUV = viewDir.xy * stepHeight / viewDir.z;
	
	// 初始点位于靠近摄像机的位置
	float2 currentUV = i.uv + deltaUV * _Step;
	float currentH = _ParallaxScale;
	float currentMap = tex2D(_ParallaxMap, i.uv).a;
	
	//步进最多_Step次
	for(int k = 0; k < _Step; k++)
	{
		currentUV -= deltaUV;
		currentH -= stepHeight;
		currentMap = tex2D(_ParallaxMap, currentUV).a;
		//如果当前采样点位于目标表面下方则停止步进
		if(currentH <= currentMap)
		{
			//得到上一次步进的位置
			float2 prevUV = currentUV + deltaUV;
			float prevH = currentH + stepHeight;
			float prevMap = tex2D(_ParallaxMap, prevUV).a;
	
			float afterD = currentMap - currentH;
			float beforeD = prevH - prevMap;
			float w = afterD / (afterD + beforeD);
			float2 finalUV = lerp(currentUV, prevUV, w);
	
			return finalUV;
		}
	}
	
	//如果步进结束，直接输出i.uv
	return i.uv;
}

......
//在片元着色器中的调用
i.uv = ParallaxUV(i);
//获得新的uv后，用新的uv对法线、漫反射等贴图采样
~~~

视差遮蔽映射的准确度远高于原版视差映射，但由于每个片元都需要多次采样，性能消耗远高于原版视差映射。

##### 什么是基于高度的纹理混合？

这个技术是一个在地貌笔刷效果中常用的思路。

在使用地貌笔刷修改地貌时，时常需要使用按权混合来对若干个纹理进行混合。其核心代码通常如下：

~~~ShaderLab
fixed3 WeightedTex2D()
{
	float4 weights = tex2D(_Control, uv).rgba;
	fixed3 layer0 = tex2D(_Splat0, uv).rgb;
	fixed3 layer1 = tex2D(_Splat1, uv).rgb;
	fixed3 layer2 = tex2D(_Splat2, uv).rgb;
	fixed3 layer3 = tex2D(_Splat3, uv).rgb;
	
	return fixed3(layer0 * weights.r + layer1 * weights.g + layer2 * weights.b + layer3 * weights.a);
}
~~~

使用这种基于权值的混合方案，可以实现很平滑的过渡，但难以实现自然的过渡。举例来讲，在实现沙地纹理和石砖纹理的混合时，使用基于权值的混合看起来就像沙子污染了石砖。而在现实生活中，沙子不会黏在石砖上，而是会落到石砖的间隙里，让较高的石砖保持干净。

通过获得贴图的高度(或深度)信息，我们可以在混合时逐片元的判断高度对混合权值的影响。在实践中，我们可以把贴图的高度信息储存在专门的高度贴图、或漫反射贴图/法线贴图的Alpha通道中，在此的示例中我们将高度信息储存在了漫反射贴图的Alpha通道里：

~~~ShaderLab
float3 HeightBlendedTex2D(float2 uv)
{
	float4 weights = tex2D(_Control, uv).rgba;
	//这里的alpha通道储存了对应纹理的高度信息
	fixed4 layer0 = tex2D(_Splat0, uv).rgba;
	fixed4 layer1 = tex2D(_Splat1, uv).rgba;
	fixed4 layer2 = tex2D(_Splat2, uv).rgba;
	fixed4 layer3 = tex2D(_Splat3, uv).rgba;
	
    float4 heights = float4(layer0.a, layer1.a, layer2.a, layer3.a);
    float4 HxW = heights * weights;
    
    //保存了混合高度的权重中最大的一个通道
    float maxW = max(max(max(HxW.r, HxW.g), HxW.b), HxW.a);
    float4 blend = step(maxW, HxW);
    
    
	return blend.r * layer0 + blend.b * layer1 + blend.b * layer2 + blend.a * layer3;
}
~~~

在这个版本的混合中，只选择了高度权值最高的一个纹理，而抛弃了其它的所有通道。通过这样的混合，已经可以让沙子嵌入石砖的缝里，但仍存在比较明显的边缘感，接下来我们引入高度阈值来增强整体的渐变感：

~~~ShaderLab
float3 HeightBlendedTex2D(float2 uv)
{
	float4 weights = tex2D(_Control, uv).rgba;
	//这里的alpha通道储存了对应纹理的高度信息
	fixed4 layer0 = tex2D(_Splat0, uv).rgba;
	fixed4 layer1 = tex2D(_Splat1, uv).rgba;
	fixed4 layer2 = tex2D(_Splat2, uv).rgba;
	fixed4 layer3 = tex2D(_Splat3, uv).rgba;
	
	float4 heights = float4(layer0.a, layer1.a, layer2.a, layer3.a);
    float4 HxW = heights * weights;
    
    float maxW = max(max(max(HxW.r, HxW.g), HxW.b), HxW.a);
    
    //对权重进行偏移，使其整体为负数，然后使低于阈值的部分截断到0
    float4 blend = max(HxW - maxW + _Threshold, 0) * weights;
    //对该向量标准化，使r+g+b+a=1
    blend = blend / (blend.r + blend.g + blend.b + blend.a);
	
	return blend.r * layer0 + blend.g * layer1 + blend.b * layer2 + blend.a * layer3;
}
~~~

在这个版本中，对比了具有最大的与高度有关的权重的纹理，将权重相差过大的部分截断到了0，也就是舍弃了高度差距过大的纹理在该片元处的权重。相比上一个版本，在砖缝处沙子的扩散感更明显，能产生更平滑的视觉效果。

##### 什么是Cubemap？

在图形学中，**立方体纹理(Cubemap)**是**环境映射(Environment Mapping)**的一种实现方法。

立方体纹理是由六个独立的正方形纹理组成的集合，这六个2D纹理是同一个立方体的六个面，所以Cubemap可以被理解为一个有贴图的立方体。我们用+X,-X,+Y,-Y,+Z,-Z来代表立方体六个面上的六个纹理：

![Cubemap](Textures\Cubemap.png)

在Unity中选择Project-Create-Legacy-CubeMap即可通过六张2D纹理创建一个Cubemap，或者在导入设置中将一个Cubemap的展开图设置为Cubemap，程序会自动将其裁剪为合适的天空盒。除此之外，可以调用Camera.RenderToCubemap来将摄像机观察到的场景储存到六张纹理中组成Cubemap。

和2D纹理或3D纹理的采样使用2D或3D坐标不同，对立方体纹理的采样使用的是一个单位化的方向向量。根据这个向量构建一个以立方体的中心为起点的射线，当它向外延伸时就会和六个纹理中的某一个相交，相交点的像素值就是采样得到的结果。然后可以通过简单的几何关系来获得采样的具体位置：

~~~HLSL
float4 texCUBE(sampler2D posX, sampler2D negX, sampler2D posY, sampler2D negY, sampler2D posZ, sampler2D negZ, float3 dir)
{
	
	float mag = max(max(abs(dir.x), abs(dir.y)), abs(dir.z));
	if(mag == abs(dir.x))
	{
		if(dir.x > 0)
			return tex2D(posX, float2(1-(dir.z+1)/2, (dir.y+1)/2));
		else
			return tex2D(negX, float2((dir.z+1)/2, (dir.y+1)/2));
	}
	else if(mag == abs(dir.y))
	{
		if(dir.y > 0)
			return tex2D(posY, float2((dir.x+1)/2, 1-(dir.z+1)/2));
		else
			return tex2D(negY, float2((dir.x+1)/2, (dir.z+1)/2));
	}
	else if(mag == abs(dir.z))
	{
		if(dir.z > 0)
			return tex2D(posZ, float2((dir.x+1)/2, (dir.y+1)/2));
		else
			return tex2D(negZ, float2(1-(dir.x+1)/2, (dir.y+1)/2));
	}
}
~~~

在HLSL中封装了这个语句来对立方体纹理进行采样：

~~~HLSL
texCUBE(samplerCUBE _CubeMap, float3 dir);
~~~

我们可以使用立方体纹理来实现物体的反射和折射效果。以物体为中心渲染一个Cubemap，根据反射定率和折射定率获得反射方向或折射方向，然后在这个方向上对Cubemap进行采样，得到对应方向上的物体颜色，然后再将其与物体表面的其它光照信息混合即可。

将反射与折射通过菲涅尔公式混合，就可以得到菲涅尔反射。复习菲涅尔公式的Schlick近似等式：
$$
L_2\approx [F_0+(1-F_0)(1-cos\theta)^5]L_1
$$

~~~HLSL
fixed fresnel = _FresnelScale + (1-FresnelScale) * pow(1-dot(worldViewDir,worldNormal), 5);
fixed3 reflection = texCUBE(_Cubemap, i.worldRefl).rgb;
fixed3 refrection = texCUBE(_Cubemap, i.worldRefr).rgb;
fixed3 fresnelMix = lerp(refrection, reflection, saturate(fresnel));

fixed3 diffuse = _LightColor0.rgb * _Color.rgb * max(0, dot(worldNormal, worldLightDir));

fixed3 col = ambient + lerp(diffuse, fresnelMix, _Gauss);
~~~

在使用Cubemap进行反射或折射计算时，我们可能需要注意在采样时进行向量校正。换句话说，通常来说，直接使用反射方向对Cubemap进行采样通常会采样到错误的方向。在很多复杂的或模糊的反射效果中，这种偏差是可以接受的，但在大面积的反射中——尤其是在大型水体的反射效果中——这种偏差就不可接受了。下图解释了这种偏差发生的原因：

<img src="Textures\Cubemap采样校正.png" alt="Cubemap采样校正" style="zoom:33%;" />

在上图中，ABCD为Cubemap的边界，O是Cubemap中心。PQ是产生反射的表面，M是当前产生反射的片元所在点，$\vec {MN}$是法线方向，V是摄像机坐标，$\vec {VM}$是视线方向，则$\vec {MR}$是反射光的方向。但是，如果我们直接用$\vec {MR}$进行采样，我们实际采得的是$\vec {OS}$方向上点S的纹素值。所以我们有必要通过几何计算将采样反向由$\vec {OS}$校正到$\vec {OR}$。

观察这幅图片中的几何关系，通过CPU传入的常量，我们可以在着色器中得到点O的坐标和点V的坐标，在着色器中我们可以得知点M的坐标，根据$\vec {VM}$我们可以计算得到$\vec {MR}$的方向。由因为我们已知$\vec {OM}$的大小和方向，所以只要我们估算出$\vec MR$的大小，就可以通过向量加法得到$\vec {OR}$的值。

一种常见的估算方案是，假设点R位于Cubemap的边界上，则我们可以通过射线$\vec {MR}$与立方体的求交来获得点R的位置，进而得到$\vec {OR}$的值。这里我们称“假设”，是因为点R的具体位置与其在Cubemap上的具体深度有关。

考虑深度信息，通常使用在类似Raymarch的方法来进行计算，其具体方法为：

+ 在Cubemap上记录场景的光照信息的同时，额外记录场景的深度信息
+ 设定一个步进步长$\lambda$
+ 从反射点M开始，沿着方向$\vec MR$进行步进，每次步进距离$\lambda$，获得采样点的世界坐标T
+ 沿着$\vec {OT}$对Cubemap采样，获得该方向上的深度值，与$|\vec {OT}|$比较，判断采样点T是否接触到了Cubemap中的一个实体：如没有，继续步进；如果接触到了，停止步进
+ 令R=T，对$\vec {OR}$采样，获得校准的反射方向与反射像素值

鉴于Cubemap向量校正的复杂特性，只推荐使用Cubemap实现表面法线复杂或表面模糊的折射和反射效果，在大面积清晰的反射和折射效果中推荐使用屏幕空间算法。

##### 什么是Matcap纹理?

**Matcap纹理**是一张中间为球面的纹理，被用于实现**材质捕获(Matcap)**效果。Matcap的原理并不复杂，Matcap纹理上不同位置的纹素作为不同法线方向的光照颜色，然后根据观察空间的法线在Matcap纹理上采样，将采样结果直接渲染出来。效果如图所示：

![Matcap](Textures\Matcap.png)

这个做法的优点在于，避免了大量的光照运算，却能在很多情况下实现PBR的光照效果。

接下来我们以Unity Shader为例实现Matcap：

实现Matcap的核心算法就是根据观察空间下的法线从材质球贴图上采样。所以第一步，我们需要将模型空间的法线转换到观察空间下：

~~~ShaderLab
o.NtoV.x = mul(UNITY_MATRIX_IT_MV[0], v.normal);
o.NtoV.y = mul(UNITY_MATRIX_IT_MV[1], v.normal);
~~~

如果我们需要使用法线贴图，我们改为将从法线贴图上采样的切线空间下的法线转换到观察空间：

```shaderlab
TANGENT_SPACE_ROTATION;
o.TtoV0 = normalize(mul(rotation, UNITY_MATRIX_IT_MV[0].XYZ));
o.TtoV1 = normalize(mul(rotation, UNITY_MATRIX_IT_MV[1].XYZ));
```

~~~shaderlab
float3 normal = normalize(UnpackNormal(tex2D(_BumpMap, i.uv.zw)));
half2 vn;
vn.x = dot(i.TtoV0, normal);
vn.y = dot(i.TtoV1, normal);
~~~

然后根据这个观察空间的法线从材质球贴图上进行采样：

~~~ShaderLab
fixed4 matcapCol = tex2D(_MatcapDiffuse, i.NtoV * 0.5 + 0.5);
~~~

为了让Matcap贴图的采样结果与漫反射贴图共同生效，我们将matcapCol与漫反射贴图的采样结果乘起来，为了防止渲染结果偏暗，我们可以将采样结果乘以一个常数来调整整体亮度。

~~~
fixed4 c = tex2D(_MainTex, i.uv.xy);
fixed4 finalColor = matcapLookup * c * _Diffuse;
~~~

将上面的代码组合起来，放入基本的顶点和片元着色器当中，得到了核心Matcap的代码：

```cpp
Shader "Custom/Matcap" {
	Properties {
		_Color ("Main Color", Color) = (1,1,1,1)
		_MainTex("Albedo Tex", 2D) = "white" {}
		_BumpMap ("Normal Tex", 2D) = "bump" {}
		_BumpValue ("Normal Value", Range(0,10)) = 1
		_MatCapDiffuse ("MatCapDiffuse (RGB)", 2D) = "white" {}
        _Diffuse ("Diffuse", Float) = 1.0
	}
	
	Subshader {
		Tags { "RenderType"="Opaque" }
		
		Pass {
			Tags { "LightMode" = "Always" }
			
			CGPROGRAM
				#pragma vertex vert
				#pragma fragment frag
				#include "UnityCG.cginc"
				
				struct v2f { 
					float4 pos : SV_POSITION;
					float4	uv : TEXCOORD0;
					float3	TtoV0 : TEXCOORD1;
					float3	TtoV1 : TEXCOORD2;
				};

				uniform fixed4 _Color;
				uniform sampler2D _BumpMap;          
				uniform float4 _BumpMap_ST;
				uniform sampler2D _MatCapDiffuse;
				uniform sampler2D _MainTex;
				uniform float4 _MainTex_ST;
				uniform fixed _BumpValue;
            	uniform float _Diffuse;
            
				v2f vert (appdata_tan v)
				{
					v2f o;
					o.pos = UnityObjectToClipPos (v.vertex);
					o.uv.xy = TRANSFORM_TEX(v.texcoord,_MainTex);
					o.uv.zw = TRANSFORM_TEX(v.texcoord,_BumpMap);
					
					
					TANGENT_SPACE_ROTATION;
					o.TtoV0 = normalize(mul(rotation, UNITY_MATRIX_IT_MV[0].xyz));
					o.TtoV1 = normalize(mul(rotation, UNITY_MATRIX_IT_MV[1].xyz));
					return o;
				}			
				
				float4 frag (v2f i) : COLOR
				{
					fixed4 c = tex2D(_MainTex, i.uv.xy);
					float3 normal = UnpackNormal(tex2D(_BumpMap, i.uv.zw));
					normal.xy *= _BumpValue;
					normal.z = sqrt(1.0- saturate(dot(normal.xy ,normal.xy)));
					normal = normalize(normal);
					
					half2 vn;
					vn.x = dot(i.TtoV0, normal);
					vn.y = dot(i.TtoV1, normal);

					fixed4 matcapLookup = tex2D(_MatCapDiffuse, vn * 0.5 + 0.5);					
					fixed4 finalColor = matcapLookup * c * _Diffuse;
					return finalColor;
				}

			ENDCG
		}
	}
}
```

核心Matcap的问题很多，比如在动态光源角度和动态摄像机角度的情况下效果并不好。我们可以尝试实时生成Matcap采样图，通过实时渲染一颗球体来生成Matcap纹理，然后再将这个纹理用作Matcap运算。这么做的好处是比起直接使用PBR，虽然多渲染了一个球体，却节约了大量场景物体的光照计算，总的来说效率高得多。

Matcap最复杂的问题则是基于法线采样的平面性问题，当对接近平面的网格使用Matcap渲染时，网格法线的变化不大，导致采样的范围很小，以至于整个网格的亮度几乎不产生变化，导致失真。这个问题不会在PBR中出现，是因为PBR中除了考虑片元的法线，还考虑了光源方向和视角方向，而在Matcap中我们忽略了这两个因素。

解决这个问题我们可以使用**采样向量修正**和**曲率匹配算法**。

+ **采样向量修正**

  在PBR中我们需要使用光源来计算片元亮度，但在Matcap算法中我们完全抛弃了光源，这导致了平面法线采样的错误。

  但我们还可以获得视角方向，所以我们可以通过视角方向来反推光源方向，并通过光源方向代替法线方向来对Matcap纹理进行采样，如下图所示：

  ![采样向量修正.png](Textures\采样向量修正.png)

  借用在Phong模型中的反射向量公式，我们轻易的得到了采样向量$\vec i =\vec v - 2(\vec n\cdot\vec v)\vec n$。

+ **曲率匹配**

  曲率匹配试图解决采样向量修正引入的新问题，在采样向量修正中我们将反推光源方向代替法线方向对Matcap纹理进行了采样，这一措施在平面和小曲率面中的显示效果良好，但在大曲率面上产生错误的渲染效果，错误的原因如图：

  ![曲率匹配](Textures\曲率匹配.png)

  在大曲率表面上，修正后的采样方向远远偏离了法线方向。所以我们希望对采样方向进行一个与采样点表面曲率有关的插值，当曲率越小，采样向量越接近反推光源方向；当曲率越大，采样结果越接近法线方向。

  那我们该如何获得表面曲率呢？在ShaderLab中我们可以使用下面的语句获得表面曲率：

  ~~~ShaderLab
  curvature = saturate(length(fwidth(i.Normal)) / length(fwidth(i.Pos)));
  ~~~
  
  其中fwidth被用于在片元着色器中获得某个属性的偏移量，在此我们将相邻法线的偏移量除以相邻片元间的距离，得到了近似曲率。

更新了修正后的完整着色器如下：

~~~ShaderLab
Shader "Custom/Matcap" {
	Properties {
		_MatCapDiffuse ("Capture Map", 2D) = "white" {}
		_CurveScale ("Curve Scale", Float) = 1.0
		_BumpMap ("Normal Map", 2D) = "bump" {}
		_BumpValue ("Normal Value", Range(0,10)) = 1
		_Color ("Main Color", Color) = (1,1,1,1)
		_MainTex("Albedo Map", 2D) = "white" {}
        _Diffuse ("Diffuse", Float) = 1.0
	}
	
	Subshader {
		Tags { "RenderType"="Opaque" }
		
		Pass {
			Tags { "LightMode" = "Always" }
			
			CGPROGRAM
				#pragma vertex vert
				#pragma fragment frag
				#include "UnityCG.cginc"
				
				struct v2f { 
					float4 pos : SV_POSITION;
					float3 normal : NORMAL;
					float4 uv : TEXCOORD0;
					float3 TtoV0 : TEXCOORD1;
					float3 TtoV1 : TEXCOORD2;
					float3 viewDir : TEXCOORD3;
				};

				uniform float _CurveScale;
				uniform fixed4 _Color;
				uniform sampler2D _BumpMap;          
				uniform float4 _BumpMap_ST;
				uniform sampler2D _MatCapDiffuse;
				uniform sampler2D _MainTex;
				uniform float4 _MainTex_ST;
				uniform fixed _BumpValue;
            	uniform float _Diffuse;
            
				v2f vert (appdata_tan v)
				{
					v2f o;
					o.normal = v.normal;
					o.pos = UnityObjectToClipPos (v.vertex);
					o.uv.xy = TRANSFORM_TEX(v.texcoord,_MainTex);
					o.uv.zw = TRANSFORM_TEX(v.texcoord,_BumpMap);
					
					float4 viewPos = mul(unity_WorldToObject, _WorldSpaceCameraPos);
					o.viewDir = normalize(UnityObjectToClipPos(viewPos) - o.pos);
					
					TANGENT_SPACE_ROTATION;
					o.TtoV0 = normalize(mul(rotation, UNITY_MATRIX_IT_MV[0].xyz));
					o.TtoV1 = normalize(mul(rotation, UNITY_MATRIX_IT_MV[1].xyz));
					return o;
				}			
				
				float4 frag (v2f i) : COLOR
				{
					fixed4 c = tex2D(_MainTex, i.uv.xy);
					float3 normal = UnpackNormal(tex2D(_BumpMap, i.uv.zw));
					normal.xy *= _BumpValue;
					normal.z = sqrt(1.0- saturate(dot(normal.xy ,normal.xy)));
					normal = normalize(normal);
					
					half2 vn;
					vn.x = dot(i.TtoV0, normal);
					vn.y = dot(i.TtoV1, normal);

					half2 vm;
					float3 adjust =  i.viewDir - 2 * dot(normal, i.viewDir) * normal;
					vm.x = dot(i.TtoV0, adjust);
					vm.y = dot(i.TtoV1, adjust);

					float cur = saturate(length(fwidth(i.normal)) / length(fwidth(i.pos)) * _CurveScale);

					float2 samp = lerp(vm, vn, cur);

					fixed4 matcapLookup = tex2D(_MatCapDiffuse, samp * 0.5 + 0.5);					
					fixed4 finalColor = matcapLookup * c * _Diffuse;
					return finalColor;
				}

			ENDCG
		}
	}
}
~~~

<div STYLE="page-break-after:always;"></div>

### 屏幕后处理

#### 离屏渲染

##### 什么是帧缓冲对象FBO？

在正常的渲染流程中，片元着色器的输出会被送到双缓冲区，然后直接被显示器读取。而所谓**离屏渲染(Off-screen Rendering)**，就是将片元着色器的输出储存到非显示器的缓冲区中。

**帧缓冲对象(Frame Buffer Object,FBO)**就是GPU中渲染结果的目的地，我们的所有绘制结果，包括颜色缓冲、深度缓冲和模板缓冲都是FBO的一部分。有一个默认的FBO直接连着我们的显示器窗口区域，也就是将渲染的结果显示在显示器上。但现代的GPU通常可以跳出这个限制，创建其它的FBO，这种FBO存在的意义就是允许我们将渲染结果保存在GPU的一块储存区域，待之后使用，常用的使用方式包括：

+ 将FBO拷贝到显存中，这样GPU可以在渲染到显示器的同时立刻使用这个纹理。
+ 将FBO关联到GPU上的一个纹理对象，这样图像不再能渲染到显示器，但省下了拷贝的时间。
+ 将FBO传回到CPU，储存到内存中，CPU可以对这个纹理进行处理，比如压缩为某种格式的图片存入硬盘，对像素数据进行分析，转化为字节流发送到网络等。

##### 如何使用GrabPass抓取屏幕？

GrabPass的思路是自设FBO的第一种使用方式，即将后置缓冲区的材质复制一副到显存中，以供一个Pass采样使用：

~~~ShaderLab
SubShader
{
	Tags{"Queue"="Transparent" "RenderType"="Qpaque"}
	GrabPass("_GrabTex");
	Pass
	{
		sampler2D _GrabTex;
	}
}
~~~

使用ShaderLab的ComputeGrabScreenPos函数来通过裁剪空间顶点坐标获得在GrabTex上的采样点坐标。下面的例子中我们在渲染这个物体前将帧缓存中的信息抓取成了纹理，然后对这个物体覆盖的屏幕应用了一个扭曲噪声，实现了类似透过水面观察物体时产生波纹的效果。

~~~ShaderLab
SubShader
{
  	Tags{"Queue"="Transparent" "RenderType"="Qpaque"}
  	GrabPass("_GrabTex");
  	
  	Pass
  	{
  		#pragma vertex vert
  		#pragma fragment frag
  		#include "UnityCG.cginc"
  		
  		sampler2D _GrabTex;
  		uniform fixed3 _Color;
  		
  		a2v
  		{
  			float4 vertex : POSITION;
        }
        v2f
        {
          	float4 pos : SV_POSITION;
          	float4 scrPos : TEXCOORD0;
        }
        
        float noise(float2 p)
        {
        	//这里构造一个梯度扭曲噪声
        }
        
        v2f vert(a2v v)
        {
          	v2f o;
          	o.pos = mul(UNITY_MATRIX_MVP, v.vertex);
          	o.scrPos = ComputeGrabScreenPos(o.pos);
        }
        fixed4 frag(v2f i) : SV_Target
        {
        	fixed2 scrPos = i.scrPos.xy / i.scrPos.w;
          	fixed3 grabColor = Tex2D(_GrabTex, noise(scrPos)).rgb;
          	return fixed4(grabColor, 1);
  		}
  	}
}
~~~

GrabPass的效率相比Render Texture低，但相比其它几种抓取方式高，并且在着色器中的语法十分便利。

##### 什么是RenderTexture？

RenderTexture的实现思路是自设FBO的第二种使用方式。

在Unity中创建一个Render Texture，将其赋给一个摄像机的Render Target，这样摄像机就会将图像绘制在Render Texture上了。在Inspector中，你可以定义RenderTexture的类型，色彩储存格式，是否需要储存深度缓冲区等信息。

注意，Render Texture虽然和Texture2D很像，但有本质差别。不同于Texture是一个存在于硬盘或内存中的图片，Render Texture本质上是一个指向显存中地址的动态指针，它并不存在于CPU内存里，而在Unity中将Render Texture转化为Texture2D的过程就是将图片从GPU传输到CPU内存的过程，这个传输过程可以使用ReadPixels函数实现：

~~~C#
Texture2D uvtexRead = new Texture2D();
RenderTexture currentActiveRT = RenderTexure.active;
RenderTexture.active = uvTex;
uvtexRead.ReadPixels(new Rect(0, 0, Width, Height), 0, 0);
RenderTexture.active = currentActiveRT;
//现在uvtexRead中已经获得了来自GPU的数据，接下来就是对其的操作了，比如以png格式保存进硬盘：
using(FileStream fs = File.Open(path, FileMode.Create))
{
    byte[] bytes = uvtexRead.EncodeToPNG();
    BinaryWriter writer = new BinaryWriter(fs);
    writer.Write(bytes);
    fs.Close();
}
~~~

Render Texture除了支持ARGB32和ARGB24格式外，还支持ARGBHALF和ARGBFLOAT这样的浮点格式，这些浮点格式一般是用来保存uv或深度等非颜色信息的，并非所有GPU都支持浮点格式的纹理采样。

Render Texture不要使用new RenderTexture，因为这涉及GPU而非CPU的内存操作，而众所周知GPU的内存操作是十分浪费时间的事情。可以考虑使用RenderTexture类的GetTemporary和ReleaseTemporary函数来获取和销毁RenderTexture，GPU将维护一个池来重用大小格式一致的RenderTexture资源，RenderTexture的格式需要在申请时定义。在GetTemporary函数中调用了DiscardContents()函数，它可以取消GPU对RenderTexture的同步化开销。

~~~C#
RenderTexture.GetTemporary();//获得一块格式匹配的内存用于新的RT
RenderTexture.ReleaseTemporary();//注意这不是让GPU回收内存的语句，只是解除对内存的占用以便其它RT使用这块不用了的内存
RenderTextrue.DiscardContents();//放弃维护显存中所有现有数据，将它们直接清零
~~~

下面是Unity中使用GetTemporary申请临时RenderTexture的格式：

~~~C++
RenderTexture.GetTemporary(512, 512, 16, RenderTextrueFormat.ARGB32, RenderTextureReadWrite.sRGB, 4);
/*
	第一个参数为RT的像素宽度
	第二个参数为RT的像素高度
	第三个参数为RT深度缓冲区的大小(位数)
	第四个参数为RT颜色缓冲区的格式
	第五个参数为GPU对RT进行读写的模式：
		使用sRGB读写时，GPU会认为RT中储存的是sRGB信息，所以会在采样时对其中的数据进行伽马解码，在写RT时会对结果进行伽马加码
		使用Linear读写时，GPU不会进行任何伽马加码或解码操作，在RT中储存的不是颜色信息而是深度或法线信息时这格外重要
		使用Default时，引擎会根据渲染设置时的色彩空间配置来决定1使用哪种读写模式
	第六个参数为抗锯齿级数，级数越高抗锯齿效果越好，注意只支持1、2、4、8
*/
~~~

有的时候我们想人为的控制每一次渲染，那我们可以将摄像机disable掉，然后手动调用每一次Render。

有时候我们想用一个Shader去渲染这个Render Texture，我们可以调用像机的RenderWithShader函数，它将使用你指定的Shader去渲染场景，将场景物体上所有的Shader都替换为指定的Shader进行Draw Call，参数则按名字传递。

~~~C#
Camera.RenderWithShader(Shader shader);
Camera.RenderWithShader(Shader shader, string replacementTag);
~~~

tag的作用是限制渲染的类型，比如说某些Pass的"RenderType"标签为"Opaque"，我们只想将具有"Opaque"标签的物体用替换shader进行渲染，剩下的物体不渲染，那么我们可以将"Opaque"作为参数填入replacementTag中，支持的Tag包括：

> Opaque、Transparent、TransparentCutout、Background、Overlay
>
> TreeOpaque、TreeTransparentCutout、TreeBillBoard、Grass、GrassBillBoard

##### 如何使用Bilt指令抓取屏幕？

我们可以不在Assests下创建Render Texture，直接使用：

~~~C#
Graphics.Bilt(RenderTexture src, RenderTexture dst, Material mat);
~~~

这个函数可以从一个纹理scr上获取MainTex，将其应用到材质mat中，将渲染结果输出到dst上，这个过程是CPU调用Draw Call交给GPU实现的，所以效率比在CPU上运行高不少。

如果dst为空，且主摄像机的Render Target也为空，Unity会将显示器的后置缓冲区作为渲染目标。如果dst为空，且主摄像机的Render Target不为空，渲染目标会默认为主摄像机的Render Target。

注意，如果将src和dst设置成同一个RenderTexture可能导致不正常的渲染效果。

我们可以看到Unity的绝大部分屏幕后处理都是在OnRenderImage中使用一连串的Graphics.Bilt指令来完成一重重对图像的处理的。

OnRenderImage是Unity MonoBehaviour的生命周期函数：

~~~C#
void OnRenderImage(RenderTexture src, RenderTexture dst);
~~~

在整个渲染流程结束后，渲染结果被打印在显示器上之前，这个函数会被调用。

渲染结果会被作为src导入，将处理结果输出到dst，最终dst就会被打印到显示器上。

这个函数和Bilt配合可以很轻松的实现屏幕后处理。

#### 梯度与卷积

##### 如何使用卷积进行边缘检测？？

我们仅从图像处理的角度来理解**卷积(Convolution)**，不去考虑它在数学、物理和人工智能等领域中的其它伟大应用方式。

在图像处理中，我们在使用函数计算某像素的值时，不仅希望依据该处的纹素值进行计算，还希望参考其周围的其它纹素点，一般来说，其参考的是这个像素周围2×2或3×3的一圈相邻纹素。将相邻的纹素值按照一定的权重加起来，就得到了新的纹素的值，即函数的输出值，这个权重表就被称为**卷积核(Convolution Kernal)**，或者称为**算子**。如下图所示，左侧是原始图像及其纹素值，中间是一个3×3的算子，最后输出为右侧的目标图像。

![卷积](Textures\卷积.gif)

> 注：上面的解释是针对图像处理这一单一领域对卷积的简化理解，这种理解方式仅适用于图像处理和图形学领域，而不适用于数学、物理、人工智能等其它领域！

显然，如果相邻像素之间存在明显的颜色、亮度、纹理差异，我们会认为它们之间应当有一条边界。

相邻像素之间，关于某种属性的差，可以使用这种属性的**梯度**表示。显然，我们可以通过判断梯度的大小来决定某像素是否位于边界，梯度大于阈值则为边界，而小于阈值则不是边界。

我们可以使用卷积来计算某点处的梯度，有几种常用的边缘检测算子被先后提出来：

Roberts算子：$G_x=\begin{bmatrix}-1&0\\0&1\end{bmatrix},G_y=\begin{bmatrix}0&-1\\1&0\end{bmatrix}$，得到的边缘比较粗，对具有陡峭的低噪声的图像效果好

Prewitt算子：$G_x=\begin{bmatrix}-1&0&1\\-1&0&1\\-1&0&1\end{bmatrix},G_y=\begin{bmatrix}-1&-1&-1\\0&0&0\\1&1&1\end{bmatrix}$，对灰度渐变和噪声较多的图像效果好

Sobel算子：$G_x=\begin{bmatrix}-1&0&1\\-2&0&2\\-1&0&1\end{bmatrix},G_y=\begin{bmatrix}-1&-2&-1\\0&0&0\\1&2&1\end{bmatrix}$，对灰度渐变和噪声较多的图像效果好，定位比较准确

可见，这三种算法各包含了两个方向的算子，使用$G_x$可以计算纹素在x轴方向上的梯度，使用$G_y$可以计算纹素在y轴方向上的梯度。将它们整合起来得到整体的梯度，即$G=\sqrt{G_x^2+G_y^2}$，为了方便计算，有时我们也会使用$G=|G_x|+|G_y|$。这样我们也得到梯度的方向角公式$\theta =\arctan\frac{G_y}{G_x}$。

下面我们以Sobel算子为例对图像进行边缘检测：

~~~ShaderLab
sampler2D _MainTex;
half4 _MainTex_TexelSize;
fixed _EdgeOnly;
fixed4 _EdgeColor;

#pragma vertex vert
#pragma fragment frag
#include "UnityCG.cginc"

struct a2v
{
	float4 vertex : POSITION;
	float2 uv : TEXCOORD0;
}

struct v2f
{
	float4 pos : SV_POSITION;
	half2 uv[9] : TEXCOORD0;
}

v2f vert(av2 v)
{
	v2f o;
	o.pos = mul(UNITY_MATRIX_MVP, v.vertex);
	
	half2 uv = v.uv;
	
	o.uv[0] = uv + _MainTex_TexelSize.xy * halfs(-1,-1);
	o.uv[1] = uv + _MainTex_TexelSize.xy * halfs(0,-1);
	o.uv[2] = uv + _MainTex_TexelSize.xy * halfs(1,-1);
	o.uv[3] = uv + _MainTex_TexelSize.xy * halfs(-1,0);
	o.uv[4] = uv + _MainTex_TexelSize.xy * halfs(0,0);
	o.uv[5] = uv + _MainTex_TexelSize.xy * halfs(1,0);
	o.uv[6] = uv + _MainTex_TexelSize.xy * halfs(-1,1);
	o.uv[7] = uv + _MainTex_TexelSize.xy * halfs(0,1);
	o.uv[8] = uv + _MainTex_TexelSize.xy * halfs(1,1);
	
	return o;
}

half Sobel(v2f i)
{
	const half Gx[9]={-1,-2,-1,0,0,0,1,2,1};
	const half Gt[9]={-1,0,1,-2,0,2,-1,0,1};
	
	half edgeX = 0;
	half edgeY = 0;
	for(int it = 0; it < 9; it++)
	{
		edgeX += texColor * G_x[it];
		edgeY += texColor * G_y[it];
	}
	
	half edge = 1- abs(edgeX) - abs(edgeY);
	
	return edge;
}

fixed4 frag(v2f i) : SV_Target
{
	half edge = Sobel(i);
	
	return lerp(_EdgeColor, tex2D(_MainTex, i.uv[4]), edge);
}
~~~

##### 如何使用卷积实现高斯模糊？

卷积的另一个常见应用是**高斯模糊**，所谓高斯模糊，就是按照**高斯分布函数(又称正态分布函数)**将纹素相邻的纹素值叠加，使得每个纹素受到周围其它纹素的影响，实现画面的模糊效果。

高斯方程：
$$
G(x,y) = \frac{1}{2\pi\sigma^2}e^{\frac{x^2+y^2}{2\sigma^2}}
$$
我们设标准差σ为1，将某像素到中心像素的像素距离作为x，y，代入式子，得到高斯算子：
$$
G_{3\times3}=\begin{bmatrix}1&2&1\\2&4&2\\1&2&1\end{bmatrix}\\
G_{5\times5}=\begin{bmatrix}1&4&7&4&1\\4&16&26&16&4\\7&26&41&26&7\\4&16&26&16&4\\1&4&7&4&1\end{bmatrix}
$$
将算子进行归一化，即在矩阵前除以权重的和，使其和为1：
$$
G_{3\times3}=\frac{1}{16}\begin{bmatrix}1&2&1\\2&4&2\\1&2&1\end{bmatrix}\\
G_{5\times5}=\frac{1}{273}\begin{bmatrix}1&4&7&4&1\\4&16&26&16&4\\7&26&41&26&7\\4&16&26&16&4\\1&4&7&4&1\end{bmatrix}
$$
将上面的算子应用在原图像上就可以实现高斯模糊，并且可以通过多次使用高斯滤波来增强模糊效果。

我们可以把5x5或更大的高斯算子近似的拆解成一个列向量和一个行向量的积的形式，如：
$$
G_{5\times5}=\begin{bmatrix}0.0545&0.2442&0.4026&0.2442&0.0545\end{bmatrix}
\begin{bmatrix}0.0545\\0.2442\\0.4026\\0.2442\\0.0545\end{bmatrix}
$$
这样的拆解可以有效降低编写代码的复杂度并提升效率，因为我们**只需要对原图分别在水平和竖直方向上各应用一次高斯模糊**，就可以将采样的复杂度由O(n^2^)降低为O(2n)。为了高效实现高斯模糊，我们使用了两个Pass分别在水平和竖直方向对纹理进行操作：

~~~ShaderLab
SubShader
{
	CGINCLUDE
	
	#include "UnityCG.cginc"
	
	sampler2D _MainTex;
	half4 _MainTex_TexelSize;
	float _BlurSize;
	
	struct a2v
	{
		float4 vertex : POSITION;
		float2 uv : TEXCOORD0;
	}
		
	struct v2f
	{
		float4 pos : SV_POSITION;
		half2 uv[5] : TEXCOORD0;
	}
	
	fixed4 fragBlur(v2f i) : SV_Target
	{
		float weight[3] (0.4026, 0.2442, 0.0545);
		fixed3 sum = tex2D(_MainTex, i.uv[0]).rgb * weight[0];
		
		for(int it = 1; it < 3; it++)
		{
			sum += tex2D(_MainTex, i.uv[it*2-1]).rgb * weight[it];
			sum += tex2D(_MainTex, i.uv[it*2]).rgb * weight[it];
		}
		
		return fixed4(sum, 1);
	}
	
	v2f vertBlurVertical(a2v v)
	{
		v2f o;
		o.pos = mul(UNITY_MATRIX_MVP, v.vertex);
		half2 uv = v.uv;
		o.uv[0] = uv;
		o.uv[1] = uv + float2(0, _MainTex_TexelSize.y * 1) * _BlurSize;
		o.uv[2] = uv - float2(0, _MainTex_TexelSize.y * 1) * _BlurSize;
		o.uv[3] = uv + float2(0, _MainTex_TexelSize.y * 2) * _BlurSize;
		o.uv[4] = uv - float2(0, _MainTex_TexelSize.y * 2) * _BlurSize;
			
		return o;
	}
	
	v2f vertBlurHorizontal(a2v v)
	{
		v2f o;
		o.pos = mul(UNITY_MATRIX_MVP, v.vertex);
		half2 uv = v.uv;
		o.uv[0] = uv;
		o.uv[1] = uv + float2(_MainTex_TexelSize.y * 1, 0) * _BlurSize;
		o.uv[2] = uv - float2(_MainTex_TexelSize.y * 1, 0) * _BlurSize;
		o.uv[3] = uv + float2(_MainTex_TexelSize.y * 2, 0) * _BlurSize;
		o.uv[4] = uv - float2(_MainTex_TexelSize.y * 2, 0) * _BlurSize;
			
		return o;
	}
	
	ENDCG
	
	Pass
	{
		CGPROGRAM
		#pragma vertex vertBlurVertical
		#pragma fragment fragBlur		
		ENDCG
	}
	
	Pass
	{
		CGPROGRAM
		#pragma vertex vertBlurHorizontal
		#pragma fragment fragBlur
		ENDCG
	}
}
~~~

##### 如何借助画面模糊优化边缘检测？

借助高斯模糊，可以获得质量更高的边缘检测效果。以Canny检测算子为例：

Canny算法包括四个步骤：

+ 使用高斯算子对原图像进行一次高斯模糊，去除噪声。

+ 使用Sobel算子计算高斯模糊图中每个纹素在x、y轴上的梯度。

+ 进行非极大值抑制。

  梯度大，不代表这个纹素就是边缘，光照和渐变纹理等都可能在非边缘上制造出大梯度纹素。解决这个问题的方法是，确定该纹素沿梯度方向上是极大值点，如下图所示，通过比对领域中的点来确定某个纹素是不是极大值点：

  ![极大值抑制](Textures\极大值抑制.jpg)

  如图中，C点与g~1~、g~2~、g~3~、g~4~四个点进行比较，如果C点梯度不是五点中的最大者，则认为C点不是边缘点，将其去除。

+ 使用双阈值来进行边缘连接。

  只使用一个阈值来判断具有某梯度的纹素是否为边缘误差过大，所以Canny算法使用了两个阈值Low和High来判断纹素是否为边缘点：

  + 如果该点的梯度值小于Low，判断不是边缘点，直接舍去。
  + 如果该点的梯度值大于High，判断是边缘点。
  + 如果该点的梯度值位于Low和High之间，根据该点相邻的点中是否存在梯度值大于High的点，如果有，则认为该点也是边缘点，否则认为不是边缘点，舍去。

##### 如何实现Bloom效果？

Bloom效果就是学习真实摄像机拍摄较亮物体时的过度曝光效果，让画面中较亮的区域扩散到周围的区域中，造成一种朦胧的效果。

Bloom的实现思路为：根据一个阈值提取出图像中较亮的区域，把它们存储在一张渲染纹理中，再利用高斯模糊对渲染纹理进行模糊处理，模拟光线扩散的效果，最后再将其和原图像混合，得到最终的结果。



##### 什么是Retinex算法？



#### 后期纹理

##### 什么是后期渲染纹理？

后期渲染纹理主要分为三种：后期深度纹理、后期法线纹理和后期运动矢量纹理。它们分布储存了片元在观察空间中的深度值、法线值，和基于像机运动的运动方向。这些后期纹理常用在各种后处理，以及类似后处理的技术，如混合光线追踪等算法中。

在C#代码中，通过修改像机的DepthTextureMode属性来要求摄像机自动渲染后期纹理：

~~~C#
camera.DepthTextureMode = DepthTextureMode.Depth;
camera.DepthTextureMode = DepthTextureMode.DepthNormal;
camera.DepthTextureMode = DepthTextureMode.MotionVector;
~~~

在ShaderLab中，声明这样的属性可以访问当前摄像机渲染的后期渲染纹理：

~~~ShaderLab
uniform sampler2D _CameraDepthTexture;
uniform sampler2D _CameraDepthNormalsTexture;
uniform sampler2D _CameraMotionVectorTextrue;
~~~

+ **后处理深度纹理**

后处理深度纹理是摄像机额外渲染的一副图像，其分辨率和屏幕分辨率一致，使用R和G两个通道保存16位的浮点数信息。摄像机将片元在NDC下的z轴值映射到0~1后以灰度的形式渲染到图像上，而该图像则作为另一次渲染的输入，通过对后处理深度纹理的采样来获取深度信息。显而易见，为了渲染后处理深度纹理我们必须使用额外的Draw Call。

下图就是在Unity中获得的某场景深度纹理图，显然，离摄像机越远的地方越亮，表面深度值越大。

![深度纹理](E:/Textures/图形学/深度纹理.png)

后处理深度纹理的渲染运用了和阴影投射相同的Shader，所以只有非透明物体会被后处理深度纹理渲染，同时如果一个Shader不支持阴影阴影投射，那么就不会出现在深度纹理上。后处理深度纹理可以被用于制造景深、实现运动模糊、全局雾效和GPU粒子等效果。

后处理深度纹理受NDC影响很大，如果视锥体的大小不恰当，就会导致深度值难以区分。如果画面出现明显的偏黑，可能是因为视锥体的远裁剪平面过远，导致深度值集中在极小的范围。

由于透视除法的特性，片元的深度值是非线性的，即从近截面到远截面的深度精度分布不均匀。为了兼顾近处和远处的细节，我们需要将深度纹理获取到的深度值映射到一个线性空间中。

设裁剪空间中，顶点坐标的$z$分量为$z_{clip}$，顶点坐标的$w$分量为$w_{clip}$；设观察空间中顶点坐标的$z$分量和$w$分量为$z_{view}$和$w_{view}$，近裁剪平面距离摄像机的距离为$n$，远裁剪平面距离摄像机的距离为$f$，根据投影矩阵我们得知：
$$
\left \{ \begin{array}{c}
z_{clip}=z_{view}\frac f{n-f}+w_{view}\frac{nf}{n-f}\\
w_{clip} = (-1)\cdot z_{view}
\end{array}\right.
$$
其中，$w_{view}=1$，而转换到NDC坐标下的顶点的$z$分量为
$$
z_{ndc}=\frac{z_{clip}}{w_{clip}}=\frac{\frac{z_{view}f+nf}{f-n}}{z_{view}}
$$
又因为在深度纹理中，深度值$d=0.5z_{ndc}+0.5$，得到
$$
z_{view}=\frac1{[\frac{(f-n)d}{nf}-\frac1n]}
$$
注意到在Unity中观察空间为右手坐标系，所以$z_{view}$为负数，所以在Unity中使用这个公式需要对其取反：
$$
z_{view}=\frac1{[\frac{(n-f)d}{nf}+\frac1n]}
$$
所以我们可以通过这个函数将从深度纹理中采样得到的顶点深度值变换到观察空间中，也就是距离摄像机近裁剪平面的距离：

~~~hlsl
inline float LinearEyeDepth(float z)
{
	//_ZBufferParams的z分量为x分量除以视锥体远裁剪平面距离，即(n-f)/(n*f)
	//w分量为y分量除以视锥体远裁剪平面距离，即1/n
	return 1.0 / (_ZBufferParams.z * z + _ZBufferParams.w);
}
~~~

我们可以在Unity中用简单的宏命令来完成整个获取深度和解码的过程：

~~~ShaderLab
float depth = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv);
depth = Linear01Depth(depth);
~~~

+ **后处理法线纹理**

后处理法线纹理用两个通道储存物体表面在观察空间的法线方向。由于我们知道法线方向总是单位向量，所以我们只需要两个通道来记录一个法线，而第三个方向的值可以由这两个通道值得出。

后处理法线纹理的压缩方式和法线贴图完全不同，一般使用的压缩格式被称为**球极投影(Stereographic projection)**压缩。

先介绍球极投影的概念：以摄像机为球心做一个单位球，在观察空间中，将xoy平面(摄像机平面)称为赤道平面，点N(0,0,1)称为北极点，取这个单位球上的一点P，过点N和点P得到唯一的射线L，令L与赤道平面交于点P‘，则称点P’为点P的**球极投影点**。在压缩后处理法线纹理时，将法线方向的反方向作为点P，就可以将三维的法线向量压缩成二维的球极投影坐标，获得更大的精度：

<img src="Textures\球极投影点.png" alt="球极投影点" style="zoom:80%;" />

显然，球极投影点可以使用以下公式计算：
$$
(X,Y)=(\frac{x}{z+1},\frac{y}{z+1})
$$
通过球极投影，可以将三维向量映射到$[-1,1]^2$中，而为了让它们能编码到纹理中，还需要将取值范围映射到$[0,1]^2$。为了使编码得到的纹理效果尽可能好，根据FOV(视锥体的高宽比)的大小，还应选择对编码的浮点数额外进行一次线性映射，在高宽比为16:9时，Unity选择的缩放系数为1/1.7777。下面是对后处理法线纹理的压缩和解压函数完整代码：

~~~HLSL
inline float2 EncodeViewNormalStereo(float3 n)
{
	float kScale = 1.7777;
	float2 enc;
	enc = n.xy / (n.z+1);
	enc /= kScale;
	enc = enc * 0.5 + 0.5;
	return enc;
}

inline float3 DecodeViewNormalStereo(float4 enc4)
{
	float kScale = 1.7777;
	float3 nn = enc4.xyz * float3(2*kScale, 2*kScale, 0) + float3(-kScale, -kScale, 1);
	float g = 2.0 / dot(nn.xyz, nn.xyz);
	float3 n;
	n.xy = g*nn.xy;
	n.z = g-1;
	return n;
}
~~~

+ **后期运动矢量纹理**

后期运动矢量纹理用于实现**动态模糊(Motion Blur)**效果，这个效果的含义为给运动中的物体添加了一个在运动方向上的模糊效果，增强视觉真实感。由于视觉暂留现象和摄像机的特性，真实世界中的运动物体是存在动态模糊效果的，这时计算机渲染出的过于清晰的图像反而显得不真实。我们通过计算物体表面点在屏幕空间中的运动方向来计算像素的速度。为了计算这个运动方向，至少需要获得上一帧中该点在屏幕空间中的位置。

运动模糊分为基于像机运动方向进行模糊的**像机运动模糊**，和基于对象运动方向进行模糊的**对象运动模糊**。

对于像机运动模糊来说，我们默认场景静止而像机运动，所以只需要记录上一帧中摄像机的变换矩阵，就可以直接通过将物体的世界空间坐标与上一帧的摄像机MVP变换矩阵相乘，得到上一帧中物体表面在投影空间中的坐标。通过屏幕映射，就可以得到上一帧中物体表面在屏幕空间中的坐标。将当前物体表面在屏幕空间中的坐标减去上一帧中物体表面在屏幕空间中的坐标，就可以得到基于像机运动方向的**像素运动矢量(Motion Vector)**。将这个向量储存在一副纹理图中，这幅纹理图就被称为**基于像机的后期运动矢量纹理**。在Unity中使用MotionVector模式渲染出来的后期运动矢量纹理就是基于像机的。

对于对象运动模糊来说，我们默认像机静止而物体运动。为了计算这种情况下物体在上一帧中的位置，我们必须对每个高速运动的物体额外保存其变换矩阵。对于这些高速物体表面的每个点，通过与上一帧的物体M矩阵相乘，可以得到上一帧中物体表面在世界空间中的坐标。在此基础上，可以再左乘上一帧的摄像机VP变换矩阵，得到点在上一帧中在投影空间的坐标。与像机运动模糊类似，转换到屏幕空间后与当前屏幕空间坐标相减，得到基于对象的后期运动矢量纹理。

在使用后期运动矢量纹理进行动态模糊时，沿着从后期运动矢量纹理上采样得到的像素速度方向，在连续的若干个点上采样，并进行加权混合，得到在对应方向上的模糊效果：

~~~ShaderLab
float4 col;
float2 uv = i.uv;
for(int k = 1; k < _SampleCount; k++)
{
	float4 c = tex2D(_MainTex, uv);
	uv += _BlurSize * velocity; //velocity为在后期运动矢量纹理上采样的到的速度值
	col += c;
}
col /= _SampleCount;
return fixed4(col.rgb, 1.0);
~~~

##### 如何利用深度法线纹理实现边缘检测?



##### 如何实现非体积雾？



<div STYLE="page-break-after:always;"></div>

### 真实感渲染

#### 实时局部光照模型

##### 什么是局部光照模型？

光照模型是用来表达物体经过光照后外观特征的数学模型。

**局部光照模型**与**全局光照模型**是相对的概念，仅处理光源直接照射物体表面的光照模型被称为局部光照模型。

局部光照模型是与光栅化渲染算法相适应的，我们在局部光照模型中仅考虑一个点的亮度，而不考虑这个点与场景中其它点的几何关系。在局部光照模型中，我们定义一个点的光照仅由漫反射、高光、自发光和环境光组成。其中高光就是镜面反射的程度，自发光是物体本身发光的强度，而环境光则是一种虚拟光。

在真实物理环境中，由于被漫反射和镜面反射的光可以作为新光源照射到其它物体上，并在经过多次反射或折射后进入人眼或摄像机等光学设备，这样的光照被称为间接光，间接光的存在使不被光直射的物体表面也不会表现为全黑。而在简单的光照模型中，由于我们没有考虑光的多次反射和折射，我们必须使用环境光这一属性来避免部分物体或部分物体表面被渲染为纯黑色。使用光线追踪的全局光照模型则考虑了这样的多次反射或折射，可以模拟更真实的光照环境而不需要使用环境光。

类似的，由于我们没有使用全局光照，自发光也并不能真的照亮周围的物体表面，它只能使自己看起来更亮一点。

**光照分布函数(Distribution Function)**是一类可直接用于局部光照模型的数学工具，它们定义了某点在接受指定光照下的视觉效果，所以在局部光照模型中，我们可以简单的在片元着色器中根据光源等信息对片元套用光照分布函数进行光照计算，并得出和真实情况近似的光照效果。

##### 有哪些漫反射BRDF？

**双向反射分布函数(Bidirectional Reflectance Distribution Function，BRDF)**是一类局部光照模型。

BRDF用于定义一个表面的光照特性，准确的说，它定义了在给定入射方向、入射辐照度、法线方向和出射方向的状态下，计算出射辐照度的大小。在本章中，我们只考虑漫反射、高光、自发光和环境光四种光的成分和强度。

经典的双向分布函数包括**Lambert模型**、**半Lambert模型**、**Phong模型**、**Blinn-Phong模型**和**Cook-Torrance模型**。我们将它们分为两个部分，**漫反射BRDF**和**高光反射BRDF**：

+ **Lambert模型：**

Lambert模型是经典的漫反射模型，它本身不考虑高光等光照元素，但它对漫反射的计算策略在过去的几十年中几乎一直在被沿用。另外，它也可以被用于模拟大理石和塑料这种哑光的质感。

在漫反射中，视角的位置被认为是不重要的，因为反射是完全随机的，因此可以认为在任何反射方向上的分布都是一样的。但是，入射光线的角度很重要。漫反射符合**兰伯特定理**：反射光的强度与表面法线和光源方向之间夹角的余弦值成正比。因此，漫反射部分的计算如下：
$$
c_{diffuse}=(c_{light}* m_{diffuse})\max(0,\vec n\cdot \vec l)
$$
其中，**n**和**l**分别是表面法线和光源方向矢量，特别注意**l**是从物体表面指向光源方向的矢量，而非入射方向。m~diffuse~是材质的漫反射颜色，而c~light~是光源颜色。

+ **半Lambert模型**

由于Lambert模型是不考虑环境光的，从理论上讲，当点乘结果为负数时将其截取为0，说明该点不会受到光的直射，所以在无光直射的顶点，会渲染出纯黑的效果。如果不考虑环境光，这种效果是符合物理情境的，但并不符合人的直觉。

半Lambert模型是一个历史产物，它为了应对没有环境光的情况下黑面的问题，对Lambert模型的结果进行了一次映射，从(-1,1)区间映射到了(0,1)区间，而不再使用截取法：
$$
c_{diffuse}=(c_{light}* m_{diffuse})\frac{\vec n\cdot \vec l+1}2
$$
这个方法随着环境光的引入已经被历史抛弃了。这个模型仍存在历史价值，但我们仅会使用Lambert而非半Lambert模型对漫反射进行模拟。

+ ##### Minnaert漫反射模型

这个模型多被用于模拟丝绒的漫反射光照，在边缘处散发着“黑光”：
$$
C_{diffuse}=m_{diffuse}*((\vec n\cdot\vec l)(\vec n\cdot\vec v))^{m-1}*C_{light}
$$
其中m为模型的粗糙度，m越高模型表面越暗。

+ **Oren-Nayar模型**

这个模型是对Lambert模型的更新，在考虑微小面元遮挡的情况下，考虑了表面的粗糙度$\sigma$，对漫反射光照的优化模型。在《风之旅人》中用于实现松散沙子的漫反射效果。
$$
\begin{split}
A&=1-0.5\frac{\sigma^2}{\sigma^2+0.33}\\
B&=0.45\frac{\sigma^2}{\sigma^2+0.09}\\
\alpha&=\max(\arccos(\vec n\cdot\vec l), \arccos(\vec n\cdot\vec v))\\
\beta&=\min(\arccos(\vec n\cdot\vec l), \arccos(\vec n\cdot\vec v))
\\C_{diffuse}&=\frac\rho\pi*(\vec n\cdot\vec l)*(A+B*\sin(\alpha)*\tan(\beta)*\max(0,\vec v\cdot\vec r))*C_{light}
\end{split}
$$
其中$\rho$是漫反射强度。

##### 有哪些高光反射BRDF？

+ ##### Phong模型

Phong模型是一种典型的纯几何经验模型。由于自发光和环境光都与法线等向量无关，在一次渲染中可以被理解为常数，所以我们重点考量的是漫反射和高光反射的计算。

Phong模型沿用了Lambert的漫反射策略。

Phong模型中的高光反射是一种经验模型，也就是说，它并不完全符合真实世界中的高光反射现象。

根据反射定律，我们可以得到镜面反射方向的矢量**r**：

<img src="Textures\镜面反射方向.png" alt="镜面反射几何" style="zoom:50%;" />

我们在根据几何关系推出公式后得到：
$$
\vec r=2\vec w - \vec l=2(\vec n\cdot\vec l)\vec n-\vec l
$$

根据Phong模型，我们近似认为高光反射的强度与视角方向和反射光方向的夹角的余弦值相关：
$$
c_{specular}=(c_{light}* m_{specular})\max(0,\vec v\cdot\vec r)^{m_{gloss}}
$$
其中，m~gloss~是材质的参数光泽度，它被用于控制高光区域的范围，光泽度越高范围就越小、边缘越锐利。

最后，Phong模型就是将四个光照分量进行加和：
$$
c=c_{emissive}+c_{ambient}+c_{diffuse}+c_{specular}
$$

+ **Blinn-Phong模型**

Blinn针对高光反射的计算方式对Phong模型进行了一次改进，改进后的结果被称为Blinn-Phong模型，他仍是一种经验模型。

Blinn模型避免了计算反射方向**r**，而是计算了视角方向**v**与光源方向**l**的角平分矢量**h**：
$$
\vec h = \frac{\vec v + \vec l}{|\vec v+\vec l|}
$$
然后用**h**代替**r**进行高光计算：
$$
c_{specular}=(c_{light}* m_{specular})\max(0,\vec n\cdot\vec h)^{m_{gloss}}
$$
显然，Blinn-Phong模型和Phong模型的结果并不一样。当摄像机和光源距离模型足够远时，Blinn-Phong模型会快于Phong模型，而反之Phong模型可能更快一些。Blinn-Phong模型在部分情况下优于Phong模型，但实际上我们还是需要动态的选择。

+ ##### Schlick模型

这个模型是对Blinn-Phong模型的优化，试图简化其中与$m_{gloss}$有关的指数运算，可以实现更类似油漆的效果。
$$
c_{specular}=(m_1*(\vec n\cdot\vec l)+m_2*\frac{\vec r\cdot\vec v}{n-((n-1)*(\vec r\cdot\vec v))})c_{light}
$$
其中$m_1,m_2$是两个控制因子，$n$是高光强度。

##### 什么是基于物理的渲染(PBR)？

前面提到的光照模型都普遍将着色区域假设为一个理想的光滑平面，表面法向量可以由一个单一法向量来定义表示。事实上，着色的区域是一个有无数比入射光线覆盖范围更小的微小表面组成的粗糙区域。

<img src="Textures\理想反射与实际反射.png" alt="理想反射与实际反射" style="zoom: 80%;" />

所以，在进一步追求真实的PBR渲染中，我们需要使用一个**概率分布函数D**来计算任意方向的微小表面在着色区域中存在的概率。

在考虑物体表面的粗糙特性的情况下，除了法线分布不均匀外，还要考虑反射光被物体表面凸起遮挡的情况，后者被称为**几何衰减**。几何衰减因子十分重要，因为它保证了出射辐照度一定小于入射辐照度。总结起来，整个模型主要由3个因子组成：**法线概率分布函数D**、物体的反射系数**菲涅尔因子F**和**几何衰减因子G**。

迪斯尼公司整合了Microfacet模型中的影响因子，并提出了著名的**迪斯尼双向反射分布函数(Disney BRDF)**。迪斯尼模型在图形学渲染领域的意义不可谓不大，它优化和规范了美术的工作流，简化了美术需要考虑的信息，同时也提供给美术一个标准的操作空间，有利于美术资源的流动。迪斯尼模型的定义如下：
$$
f(\vec n,\vec l,\vec v, \vec h)=\frac{D(\vec n,\vec h)F(\vec v,\vec h)G(\vec n,\vec l,\vec v,\vec h)}{\pi(\vec n\cdot\vec l)(\vec n\cdot\vec v)}
$$
使用D、F和G描述一种高光模型的方式被称为**迪斯尼模型**(注意，迪斯尼模型只描述了高光，还要加上任意一种漫反射模型才能渲染出正确的物体)。迪斯尼模型分母中的常数$\pi$也常被约写为4、3、2或1，由于这个系数仅线性的影响光照的强度，所以各个版本没有本质区别。

**Cook-Torrance模型**是迪斯尼模型最典型的实现之一，它着重于实现具有金属光泽的物体表面效果，它在Lambert模型对漫反射的描述的基础上，再一次修正了对高光的计算模式。

在Cook-Torrance模型中，法线分布函数通常采用**Beckmann法线分布函数**，使用到参数粗糙度m，公式为：
$$
D_{Beckmann}=\frac{1}{\pi m^2(\vec n\cdot\vec h)^4}\exp({-\frac{\tan^2(\arccos(\vec n\cdot\vec h))}{m^2}})=\frac{1}{\pi m^2(\vec n\cdot\vec h)^4}\exp({\frac{(\vec n\cdot\vec h)^2-1}{m^2(\vec n\cdot\vec h)^2}})
$$
菲涅尔项一般使用前面提到过的**Schlick近似**：
$$
F_{Schlick}=F_0+(1-F_0)(1-(\vec v\cdot\vec h))^5
$$
注意这里使用的是$\vec v\cdot\vec h$而不是$\vec n\cdot\vec v$，因为我们使用的法线并不是宏观物体的表面法线，而是Microfacet模型中的微表面法线，而这个法线是由法线分布函数筛选得到的。在某些情况下，还可以使用$\vec l\cdot\vec h$来代替$\vec v\cdot\vec h$，如Unity中就使用了$\vec l\cdot\vec h$的版本。

其中$F_0$为物体的基础反射率。但注意，由于$F_0$的取值与物体导电度有关，换句话说，与物体的金属性强弱有关，为了能使用一个公式同时描述不同金属度下的菲涅尔效果，将材料的金属性参数整合到$F_0$的计算中，得到实际计算$F_0$的代码如下：

~~~ShaderLab
float3 F0 = lerp(unity_ColorSpaceDielectricSpec.rgb, Albedo, _Metalic);
~~~

其中unity_ColorSpaceDielectricSpec是一个接近(0.04, 0.04, 0.04)的向量，整个公式可以得到物体对红绿蓝三种颜色的反射率。材质金属性越强，物体的基础反射率越接近物体原本的颜色，而金属性越弱，菲涅尔效果就越接近黑色，也就是菲涅尔效果越弱。这个操作使得金属的高光带有表面颜色，而非金属基本上只带有光源颜色。

几何衰减因子一般使用**Cook-Torrance衰减函数**：
$$
G_{CookTorrance}=min(\frac{2(\vec n\cdot\vec h)(\vec n\cdot\vec v)}{\vec h\cdot\vec v},
\frac{2(\vec n\cdot\vec h)(\vec n\cdot\vec l)}{\vec h\cdot\vec l},1)
$$
除了Cook-Torrance，另一种常用的迪斯尼模型渲染方案是GGX方案，由**GGX法线分布函数**、菲涅尔项和**GGX几何衰减函数**组成，它因为具有更好的高光泛光效果而闻名业界：
$$
D_{GGX}=\frac{\alpha^2}{\pi((\vec n\cdot\vec h)^2(\alpha^2-1)+1)^2}\\
G_{GGX}=\frac{\vec n\cdot\vec v}{(\vec n\cdot\vec v)(1-k)+k}=\frac{\vec n\cdot\vec v}{lerp(\vec n\cdot\vec v, 1,k)}
$$
其中，$\alpha$用于表现表面粗糙度，而$k$根据使用直接光照还是**IBL(基于图像的光照)**使用不同的公式，这两个值都逼近二分之一：

$$
k_{direct}=\frac{(\alpha+1)^2}8,k_{IBL}=\frac{\alpha^2}2
$$
除此之外，**Smith-Joint几何衰减函数**对GGX几何衰减函数进行了一些优化，使其获得了更柔和的渐变感：
$$
G_{Smith}=\frac{\vec n\cdot\vec v}{(\vec n\cdot\vec v)(1-k)+k}\cdot\frac{\vec n\cdot\vec l}{(\vec n\cdot\vec l)(1-k)+k}=\frac{\vec n\cdot\vec v}{lerp(\vec n\cdot\vec v, 1,k)}\cdot\frac{\vec n\cdot\vec l}{lerp(\vec n\cdot\vec l, 1,k)}
$$
其中，k的取值和GGX中一致。Smith-Joint公式相比GGX几何衰减函数多了一个乘法项，在考虑光线出射时视线方向的遮挡的基础上额外考虑了光线入射时在光源方向上的遮挡。

**Unity的Standard材质使用的是GGX法线分布函数和Smith-Joint几何衰减函数**。注意，Unity的Standard材质计算了间接光照，所以Standard的暗部比只是用GGX或Cook-Torrance模型亮的多。

几乎所有具备**各项同性**的光照模型都可以被理解为一种迪斯尼模型。显然，由于迪斯尼模型相比Phong模型或Blinn-Phong模型考虑了更多参数，后两者也可以被简单的理解成是迪斯尼模型当中的一个简单特例，在取指定D、F、G值时，迪斯尼模型可以坍缩为Blinn-Phong模型，例如当D、F、G分别取：
$$
D_{Phong}=\frac{e+2}{2\pi}(\vec n\cdot\vec h)^{m_{gloss}}\\
F=c_{spec}\\
G_{Phong}=(\vec n\cdot\vec l)(\vec n\cdot\vec v)
$$
将所有常数项整合为K，就可以得到Blinn-Phong模型的表达式。当然，由于菲涅尔项只取了一个常数，所以Blinn-Phong模型并不能完美的模拟实际物理环境，只能属于广义的写实光照模型，却不属于精确意义上的PBR。

##### 如何实现各项异性光照？

**各项异性高光(Anisotropic Specular)**是某些特殊物体(如橡胶、易拉罐、毛发等)在不同方向和不同角度观察时物体表面产生不同高光效果的一种特性，这中特性可以通过区分物体在切线和副法线上的不同属性(主要是法线分布属性)的方式来实现。比如在Phong模型的高光公式中引入切线$\vec t$，使成像的亮度与入射光在切平面上的投影方向相关，得到了一种**经验模型**：**Banks高光模型**：
$$
S_{Banks}=m_{specular}*(\sqrt{1-(\vec l\cdot\vec t)^2}\sqrt{1-(\vec v\cdot\vec t)}-(\vec l\cdot\vec t)(\vec v\cdot\vec t))^{m_{gloss}}
$$
除次之外，还有**Ward模型**，它是行业内目前最流行的各项异性高光模型之一：
$$
S_{Ward}=\frac{c_{spec}}{\sqrt{(\vec n\cdot\vec l)(\vec n\cdot\vec v)}}
\frac{\vec n\cdot\vec l}{4\pi \alpha_x\alpha_y}\exp\left[ -2\frac{
(\frac{\vec h\cdot\vec t}{\alpha_x})^2+(\frac{\vec h\cdot\vec b}{\alpha_y})^2
}{1+(\vec h\cdot\vec n)}\right]
$$

Ward模型中直接用数学拟合的正态分布函数替换了菲涅尔项和几何遮蔽项，这事实上使得它和真实的物理效果相差甚远，却使得各个参数对各项异性的影响变得更直观，更受到美术工作者的喜爱。

Ward分布模型十分实用，如在Autodesk的Uber Shader中就使用了这个分布函数：

```glsl
float3 WardAniso(float3 N, float3 H, float NdotL, float NdotV, float Roughness1, float Roughness2, float3 anisotropicDir, float3 specColor)
{
	float3 binormalDirection = cross(N, anisotropicDir);
 
	float HdotN = dot(H, N);
	float dotHDirRough1 = dot(H, anisotropicDir) / Roughness1;
	float dotHBRough2 = dot(H, binormalDirection) / Roughness2;
 
	float attenuation = 1.0;
	float3 spec = attenuation * specColor
		* sqrt(max(0.0, NdotL / NdotV)) 
		* exp(-2.0 * (dotHDirRough1 * dotHDirRough1 
		+ dotHBRough2 * dotHBRough2) / (1.0 + HdotN));
 
	return spec;
}
```

> 论文原文：https://cseweb.ucsd.edu/~ravir/6998/papers/p265-ward.pdf

各项异性光照并非只有经验模型，通过在迪斯尼模型中的法线分布函数上引入切线和副法线，我们也可以将Beckmann和GGX法线分布函数写成各项异性的版本，从而得到满足**物理模型**的各项异性光照：

+ **各项异性Beckmann分布**形式如下：
  $$
  D_{Beckmann\_Aniso}=\frac1{\pi\alpha_x\alpha_y(\vec n\cdot\vec h)^4}\exp(-\frac{\frac{(\vec t\cdot\vec h)^2}{\alpha_x^2}+\frac{(\vec b\cdot\vec h)^2}{\alpha_y^2}}{(\vec n\cdot \vec h)^2})
  $$

+ **各项异性GGX分布**形式如下：
  $$
  D_{GGX\_Aniso}=\frac1{\pi\alpha_x\alpha_y}\frac1{(\frac{(\vec t\cdot\vec h)^2}{\alpha_x^2}+\frac{(\vec b\cdot\vec h)^2}{\alpha_y^2}+(\vec n\cdot\vec h)^2)^2}
  $$

其中$\vec h$是半矢量half，$\vec t$是切线，$\vec b$是副法线，$\alpha_x$和$\alpha_y$是在切线和副法线方向上不同的粗糙度。

上面提到的几种模型都提供了α~x~和α~y~两个参数来控制在两个方向上的粗糙度，当两个参数相等的时候，即坍缩为各向同性。

<img src="E:/Textures/图形学/Ward效果.png" alt="Ward效果" style="zoom:50%;" />

> 图片来自https://blog.csdn.net/u011469662/article/details/101107750

##### 如何实现毛发各项异性渲染？

毛发渲染是各项异性渲染的有一个重要应用场景，它们代表了各项异性中的一类重要分支：**Strand-Based Anisotropy**，即基于线的各项异。早期的毛发渲染使用面片+贴图的方式实现，随着硬件和渲染技术的提升，慢慢发展出了基于经验的**Kajiya-Kay模型**，以及基于物理的**Marschner模型**。

**Kajiya-Kay模型将毛发理解为微平面模型的一种特例**，我们将毛发看成直径非常小长度非常长的圆柱。Kajiya-Kay模型中的漫反射部分和高光部分，就是分别将Lambert模型和Phong模型应用到这样的微圆柱表面上得到的。

每一根头发可以用一根线段定义，对于我们所观察的头发上的一点，它的性质可以用其坐标$x_0$和其切线方向$\vec t$定义。光源方向$l$在法线平面上的投影为$l'$，我们将向量$l'$作为计算光照时使用的法线，则可以通过$t$与$l'$的叉乘得到副法线向量$b$。视线方向为$v$：

<img src="Textures\MicroCylinder.png" alt="MicroCylinder" style="zoom: 33%;" />

其中$l'$可以根据投影公式获得：
$$
l'=\frac{l-(t\cdot l)t}{||l-(t\cdot l)t||}
$$
Kajiya模型中的漫反射分量是通过沿着朝向光源方向的半圆柱表面对Lambert模型进行积分获得的，背向光源的半圆柱表面则不会被考虑。为了沿着这个半圆柱表面对Lambert模型积分，我们需要定义在Lambert模型中使用的法线$n$，这里我们注意到，朝向$l'$方向的半圆柱表面事实上可以在法平面中由$b$和$l'$定义：
$$
n=b\cos\theta+l'\sin\theta
$$
于是我们得到Kajiya模型中漫反射的表达式，其中沿圆柱体表面的半圆柱表面的面积由$hr\mathrm d\theta$定义：
$$
\begin{split}
\tau_{diffuse}&=k_d\int_0^\pi l\cdot nhr\mathrm d\theta\\
&=k_d\ hr\int_0^\pi l\cdot (b\cos\theta+l'\sin\theta)\mathrm d\theta\\
&=k_d\ hr\ l\cdot l'\int_0^\pi\sin\theta\mathrm d\theta\\
&=(K_d)l\cdot l'
\end{split}
$$
其中，$K_d$是公式中所有常数项的集合，可以用于调整整个漫反射的效果。我们将$l'$的计算公式也导入上式，得：
$$
\begin{split}
\tau_{diffuse}&=K_d\ l\cdot\frac{l-(t\cdot l)t}{||l-(t\cdot l)t||}\\
&=K_d\frac{1-(t\cdot l)^2}{\sqrt{1-(t\cdot l)^2}}\\
&=K_d\sqrt{1-(t\cdot l)^2}\\
&=K_d\sin<t,l>\end{split}
$$
可见，毛发的漫反射强度可以计为切线与光线的夹角的正弦值；也就是说，在毛发指向光源方向时，毛发显得更暗，这在现实生活中也是合理的。Kajiya提出的漫反射公式没有考虑毛发的自阴影(也就是毛发对毛发的遮蔽)，所以通常来说这个公式计算的毛发会偏亮。

毛发的高光部分是根据与Phong模型类似的思想推导得出的：由于反射光方向与入射光方向关于法线对称，而这个微圆柱上一点的法线方向是垂直于切线方向的所有方向。我们将入射方向与切线方向的夹角称为$\theta$，Kajiya认为反射光方向可以被定义为一个顶角为$\theta$的圆锥的母线方向，而其中最接近视线方向$v$的一条母线方向被标记为$v'$，我们将$v$与切线方向反方向的夹角称为$\theta'$，则$v$与$v'$的夹角为$|\theta'-\theta|$。如下图所示：

<img src="Textures\Kajiya高光.png" alt="Kajiya高光" style="zoom: 33%;" />

仿照Phong模型的高光公式，定义了Kajiya模型的高光公式为：
$$
\tau_{specular}=k_s(v\cdot v')^p=k_s\cos^p(|\theta'-\theta|)
$$
其中$k_s$是控制系数，$p$是光泽度。

于是，高光会在视线方向贴近圆锥表面时变得最强，并在其它情况下按照与Phong模型类似的方式衰减。通过对上面公式的推导得到：
$$
\begin{split}
\tau_{specular}&=k_s(\cos\theta\cos\theta'+\sin\theta\sin\theta')^p\\
&=k_s((\vec t\cdot\vec l)(\vec t\cdot\vec v)+\sin<\vec t,\vec l>\sin<\vec t,\vec v>)^p
\end{split}
$$
这个高光公式的一个简化版本为$\tau_{specular}=k_s\sin^p<\vec t,\vec h>$。

在实际应用中，**我们需要将毛发的高光分成相互错开的两层高光，这与毛发的物理性质有关**。其中靠近发根的一层高光是有颜色的，而靠近发梢的一层高光是无色的。其具体的物理原理在稍后介绍Marschner模型时再详解。为了使这两层高光能表现出锯齿状的边缘来显示毛发的不均匀性质，我们会对它们的切线沿着法线方向进行一次随机的偏移。正向的移动会使高光偏向v正方向(通常是发根)，负向的移动会使高光偏向v负方向(通常是发梢)。这里我们可以使用纹理或者噪声来实现对切线的扰动。

不同于基于经验公式的Kajiya-Kay模型，**Marschner模型是从毛发的物理结构出发的基于物理的着色方案**。

真实世界的毛发是一种纤维，每条纤维由几层组成：中心的发髓(Medulla)、内圈的皮质(Cortex)和表层的角质层(Cuticle)构成，如下图所示：

<img src="Textures\毛发结构.jpg" alt="毛发结构" style="zoom: 33%;" />

其中，角质层放大后，可见不平滑的表面，角质层的法线变化提供了独特的高光和漫反射效应，毛发表面的鳞片状结构使毛发表面的法线存在轻微但系统的倾斜，总体来说**其表面法线向发根方向倾斜约3°**。此外，光线照射毛发后，还会产生透射和折射，**毛发表面的折射率约为1.55**，设定透射率为$\sigma_a$。毛发微表面的坑洼具有和鳞片类似的性质，且具有较统一的指向性，由根部指向尾部，在图形学中我们可以用切线和各项异性属性来衡量这一现象。观察在毛发中的光照现象，我们将毛发简化为轴向光照模型(沿毛发生长的方向)和径向光照模型(毛发切面)两个角度：

<img src="Textures\毛发光照模型.png" alt="毛发光照模型" style="zoom: 33%;" /><img src="Textures\毛发横截面光照.png" alt="毛发横截面光照" style="zoom: 33%;" />

按照人眼看到的光线的光路，我们将光照在毛发上产生的光学现象抽象成以下3个分量：

+ **反射(R)**：表面的反射，产生靠近发梢的主高光，受毛发切线和各项异性影响。
+ **传输-传输(TT)**：光线照射并穿透毛囊，然后从另一边照射出去，这是光线在一定发量中的散射过程。产生了部分的透射效果。
+ **传输-反射-传输(TRT)**：光线进入毛囊，从内表面边界反射出来，然后再照射出来。产生的是靠近发根的次高光。

事实上，在Kajiya模型中我们使用的两层高光，就是分别由R通路和TRT通路产生的。由于折射的原因，两者会在轴向上朝不同方向错开，使得TRT光路更靠近发根，R光路更靠近发梢；其中TRT这条光路由于被毛发内部部分吸收，所以会产生有色的一层高光。

类似在计算反射时使用的BRDF，我们需要一个函数来统一的描述在给定入射方向和观察方向下，观察到的光强度占入射光的比例，由于光线照射到毛发时发生的一系列光学反应被称为散射，所以这个函数被称为**双向散射分布函数BSDF**。

为了得到BSDF，我们分别计算轴向光照模型$M(\theta_l,\theta_v)$与径向光照模型$N(\phi_l,\phi_v,n)$。在下图中，L与V方向分别是光源方向和观察方向，$\theta_l$和$\theta_v$分别是光源方向和观察方向与它们在径面上的投影的夹角；$\phi_l$与$\phi_v$则分别是光源方向和观察方向在径面上的投影与副法线方向的夹角；$n$为毛发的折射率。

<img src="Textures\Marschner参数图.png" alt="Marschner参数图" style="zoom:50%;" />

由此，我们得到BSDF的表达式：
$$
BSDF(\theta_l,\theta_v,\phi_l,\phi_v,n)=M_RN_R+M_{TT}N_{TT}+M_{TRT}N_{TRT}
$$
由于折射率一般是常量，所以在实时渲染中我们关心的只有四个输入，即$\theta_i、\theta_o、\phi_i、\phi_o$。其中，对于分量$M_R$$、$$M_{TT}$和$M_{TRT}$来说，其关心的只有$\theta_l$和$\theta_v$；而对分量$N_R$、$N_{TT}$和$N_{TRT}$来说，其关心的只有$\phi_l$和$\phi_v$。所以，一种常见的方法是将这六个函数以查找表的形式储存，我们可以使用$\sin\theta_l$和$\sin\theta_v$分别作为u和v来对$M_R$$、$$M_{TT}$和$M_{TRT}$对应的三张LUT进行采样，使用$\sin\phi_i$和$\sin\phi_v$分别作为u和v来对$N_R$、$N_{TT}$和$N_{TRT}$对应的三张LUT进行采样，得到这六个分量在不同视角下的值。理论上我们有六个分量，所以需要六张LUT(Look up table)贴图，但由于通常来说，径向的散射$N_R$、$N_{TT}$、$N_{TRT}$以及轴向反射$M_R$可以忽略介质吸收，三个颜色通道的吸收率基本一致，所以可以使用单通道进行储存，**也就是说这4个分量可以共用一张贴图，总共只需要三张LUT贴图。**如果将$M_{TT}$和$M_{TRT}$近似的认为相等，则总计只需要两张LUT贴图。



>  注：Marschner模型的原论文链接http://graphics.stanford.edu/papers/hair/hair-sg03final.pdf

##### 如何实现次表面散射？

次表面散射(Sub-Surface-Scattering)简称3S，是一种光线穿过透明/半透明表面时发生散射的照明现象，常用于皮肤、玉石、牛奶、树叶或蜡烛等半透明又有体积感的物体的渲染。

次表面散射光从表面进入物体经过内部散射，然后又通过物体表面的其他顶点出射，这表明在考虑次表面散射时，物体内部的任何一点的光照度取决于体内其他点的光照度和材质本身的透光率。抛开材质本身的性质不说，这一特性使得次表面散射的光照方程变成一个复杂的微分方程，求出此方程的准确解是十分困难的，另一方面，材质本身可能具有复杂的各向异性和不均匀密度等性质，因此计算这样的积分变得非常困难。

<img src="Textures\SSS.jpg" alt="SSS" style="zoom:80%;" />

根据光线传输方程，光线经过很多次散射后，散射的出射光线衰减度与入射方向和观察方向无关，只与散射光线出射点位置以及散射距离有关。距离入射点的径向距离越远，散射光线亮度越低；距离入射点的径向距离越近，散射光线亮度越高。光散射时散射强度与和光源的距离可以用以下公式计算：
$$
\phi(r)=\frac{e^{-\sigma_tr}}{4\pi r^2}+\frac{\sigma_s}{2\pi r^2}\int_0^\infin\frac{\arctan^2u}{u-\alpha\arctan u}\sin(r\sigma_tu)\mathrm{d}u
$$
这个公式无法求得解析解，所以对这个公式试验出了很多个近似公式进行拟合，其中最有名的是Normalized Diffusion公式：
$$
\phi(r)=\frac{e^{r/d}+e^{-r/3d}}{8\pi dr}
$$
其中d是控制函数形状的参数，如果给定漫反射强度$A=\max(0,n\cdot l)$以及入射点和出射点的距离$l_d$，可得：
$$
d=\frac{l_d}{3.5+100(A-0.33)^4}
$$
BSSRDF是实现次表面散射最常规的解法，其定义如下，其中$F_t(\eta,w)$是入射与出射时的菲涅尔折射项系数，$C_{ss}$是介质颜色：
$$
S(p_i,p_o,w_i,w_o)=\frac1\pi F_t(\eta_o,w_o)\phi(p_o-p_i)F_t(\eta_i,w_i)C_{ss}
$$
除了BSSRDF外，实时局部次表面散射还有三个流派：**切线空间次表面散射**、**屏幕空间次表面散射**和**预积分LUT次表面散射**。

+ 切线空间次表面散射(TSSSS)

这是最直接的实现思路：既然散射光线的亮度只与光线入射点和入射点的光照强度有关，我们只要遍历一点附近的所有表面，对这些相邻点贡献的散射强度进行积分，并且接收到距离近的点散射光亮度大，接收到距离远的点散射光亮度小。入射点的光照强度则可以用入射点处的漫反射强度近似。

接下来，我们尝试根据光的散射分布函数在切线空间对周围的表面进行卷积。由于散射强度关于散射距离的分布函数也可以用高斯分布近似，所以我们可以**在切线空间用高斯卷积核卷积实现次表面散射**。

整个切线空间次表面散射的流程如下：1、计算漫反射，并**将漫反射结果渲染为辐照度贴图**；2、对**漫反射结果进行多次高斯卷积**，将每次**卷积的结果都加权叠加到次表面散射贴图上**；3、在网格上对次表面散射贴图和辐照度贴图采样，**次表面散射贴图和辐照度贴图的差就是单纯次表面散射的强度**；4、计算高光并按照菲涅尔系数叠加在网格上。

这个根据漫反射来计算次表面散射的方法可以用这样的公式描述：
$$
\sum_{i=1}^NG_i(C_{\mathrm{diffuse}})*(1-w_i)+C_{\mathrm{diffuse}}*w_i
$$
一般$N\le6$，更多次的混合几乎不会再贡献任何光照了。其中$G_i$表示第$i$次高斯模糊，每次高斯卷积使用的卷积核是不同大小的，次数越大卷积范围越广。$w_i$代表第$i$次混合的权重，对不同的物体会使用不同的权值。我们通常使用一张表来储存不同物体在第$i$次卷积中使用的系数$G_i$和$w_i$，以人皮肤的介质系数表为例：

| 卷积层数 | 平均散射距离(mm^2) | 红光混合权值$w_r$ | 绿光混合权值$w_g$ | 蓝光混合权值$w_b$ |
| -------- | ------------------ | ----------------- | ----------------- | ----------------- |
| 1        | 0.0064             | 0.233             | 0.455             | 0.649             |
| 2        | 0.0484             | 0.100             | 0.336             | 0.344             |
| 3        | 0.1870             | 0.118             | 0.198             | 0.000             |
| 4        | 0.5670             | 0.113             | 0.007             | 0.007             |
| 5        | 1.9900             | 0.358             | 0.004             | 0.000             |
| 6        | 7.4100             | 0.078             | 0.000             | 0.000             |

根据模型的大小以及uv的分布，可以**根据散射距离计算出不同的卷积核**。另外，我们也注意到不同**物体对不同色光的散射强度是不同的**，在上表中我们注意到人皮肤对红光的散射距离最远，这也是人皮肤的次表面散射通常呈现红色的原因。在混合时，我们需要分布对RGB三个分量进行加权混合。

另外我们注意到，由于次表面散射需要光线入射和出射物体，所以在使用次表面散射时需要考虑入射和出射时的折射对能量的损耗。根据菲涅尔定律，入射光能量会被分给折射光和反射光。简单来说，高光反射越强的区域，次表面散射自然就会越弱。

这个算法存在两个缺陷：1、对uv的排布有很特殊的要求，这可能会导致在展uv时不能使用最优的方案，使内存消耗增加；2、不符合常规渲染管线的渲染流程，不能使用诸如背面剔除的常见优化策略。

+ 屏幕空间次表面散射(SSSSS)

这是一种使用后处理实现的次表面散射效果，也是最常用的次表面散射算法。与在切线空间的次表面散射类似，在屏幕空间中做多次高斯卷积，将每次的结果都加权叠加到原画面上：

<img src="Textures\SSSSS.jpg" alt="SSSSS" style="zoom:80%;" />

屏幕空间次表面散射使用的介质系数表和切线空间的是一致的，但我们注意到，由于卷积距离这个系数是基于切线空间统计的，所以在切线空间中我们可以对所有纹素使用同一个卷积参数，但在屏幕空间中我们需要根据片元距离屏幕的距离(也就是深度)来估测卷积的范围。同时，因为在屏幕空间中进行模糊会将物体的轮廓与背景混在一起，所以加入了模板缓冲来避免物体轮廓的变形。下面是部分实现代码示例，着重实现了模糊和混合部分，模板缓冲和高光效果等部分就略过了：

~~~ShaderLab
//_uv是屏幕空间的uv输入，_offset是默认情况下每次采样跳跃的距离和方向
fixed3 SSBlur(float2 _uv, float2 _offset)
{
	//高斯模糊
	float w[6] = {0.006,0.061,0.242,0.242,0.061,0.006}; //高斯卷积的卷积核
	float o[6] = {-1.0,-0.6667,-0.3333,0.3333,0.6667,1.0}; //卷积时使用的偏移倍数
	
	fixed4 colorM = tex2D(_MainTex, _uv);
	
	float depthM = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, _uv);
	depthM = Linear01(depthM);
	
	float3 colorBlur = colorM.rgb * 0.382; //高斯卷积核的中心
	
	//根据深度改变采样距离，离得越远每次的offset理应越小
	float2 offset = float2(_offset, _offset) / depth; 
	
	//因为要进行多次高斯模糊，这里使用的是一个简化的算法，只在一个方向上进行了7个像素的混合，而不是在7*7的矩形内进行混合
	for(int k = 0; k < 6; k++)
	{
		float2 uv = _uv + o[k] * _OffsetUV.xy * _MainTex_TexelSize.xy * offset * k;
		
		//在相邻纹素采样
		float3 color = tex2D(_MainTex, uv).rgb;
		float depth = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, uv);
		depth = Linear01(depthM);
		
		//距离越远越无关，由于在另外两个方向的距离已经在卷积核中考虑了，所以此处只考虑深度
		float t = abs(depth - depthM); 
		color = lerp(color, colorM, t);
		
		//高斯混合
        colorBlur += w[k] * color;
	}
	
	return colorBlur;
}

//这个函数用于混合六份高斯模糊后的贴图
fixed3 SSScater(float2 _uv, float2 _blurDir)
{
	//储存每层高斯模糊使用的_offset参数
	float scaterDis[6] = {0.0064, 0.0484, 0.187, 0.567, 1.99, 7.41};
	//存储每层高斯模糊的混合系数
	float3 mixWeight[6] = {
		float3(0.233, 0.455, 0.649),
		float3(0.100, 0.336, 0.344),
		float3(0.118, 0.198, 0.000),
		float3(0.113, 0.007, 0.007),
		float3(0.358, 0.004, 0.000),
		float3(0.078, 0.000, 0.000)
	};
	
	float3 colorMix = float3(0,0,0);
	
	float2 blurDir = Normalize(_blurDir);
	for(int k = 0; k < 6; k++)
	{
		float c = SSBlur(_uv, blurDir * scaterDis[k]);
		
		colorMix += c * mixWeight[k];
	}
	
	return colorMix;
}
~~~

+ 预积分次表面散射(PISSS)

  预积分次表面散射技术基于这样的结论：肉眼可见的次表面散射只会出现在表面曲率足够大，且处于明暗交界线位置的区域。换而言之，次表面散射只与两个量有关，一是表面的曲率，而是法线和光线的点积。表面曲率越大，次表面散射在纹理空间的扩散范围就越大，而在比较平坦的位置则不容易显现出次表面散射的效果。

  预积分次表面散射技术在计算光照前会对模型通过预积分计算表面曲率，将表面曲率保存在纹理当中，这样我们就可以在运行时采样获得表面曲率。注意除了考虑几何形状本身的曲率外，还要考虑由法线贴图提供的凹凸细节当中暗藏的曲率：
  $$
  \frac1r=\frac{\Delta N}{\Delta p}
  $$
  根据曲率和漫反射强度，我们可以在一张**LUT(Look Up Table)纹理**上通过参数进行采样，获得次表面散射的强度，如图所示：

  <img src="Textures\SSSLUT.png" alt="SSSLUT" style="zoom:67%;" />

  这个算法的优点是完全避免了高斯模糊和积分这样消耗性能的操作。

#### 实时全局光照技术

##### 什么是体积阴影算法？

**体积阴影算法(Shadow Volme)**是一种利用模板缓冲的阴影计算技术。相比深度阴影算法，体积阴影更能规避锯齿阴影，能实现半透明物体的阴影投射，但更难规避硬阴影效果。

体积阴影的实现需要构建**阴影体**，阴影体是一个实实在在的网格，由三角形构成。我们需要根据光源的位置和产生阴影的物体的形状去生成这个网格。阴影体起始于产生阴影的物体(之后记为SC)面向光源的面，而终止于接受光源的物体(之后记位SR)。

##### 什么是深度阴影算法？

**深度阴影算法(Shadow Map)**是最简单的阴影算法，其核心思路是在有向平行光的光源坐标空间下渲染一张深度阴影纹理，以此为依据判断哪些片元在光照过程中被其它片元遮挡了。这个算法易于实现，是很多引擎提供的基本算法。点光源也可以使用深度阴影映射算法，但点光源需要渲染CubeMap，而有向平行光只需要渲染一般的2D纹理。

**一切阴影算法的任务都是针对任意一个片元，鉴定它是否处于阴影中**。伪代码如下：

~~~C#
bool ShadowAlgorithm(fragment frag)
{
	return IsInShadow(frag.worldPos);
}
~~~

对平行光而言，鉴定片元是否处于阴影中，可以很自然的理解为等同于鉴定片元是否被光源可见。换句话说，需要从光源的方向上渲染一张深度纹理，根据深度阴影纹理来判断片元是否处于阴影中。

深度纹理是经典的后期纹理【见屏幕后处理-后期纹理】，所以我们跳过渲染深度纹理的部分，将重心放在解决如何渲染和如何采样这两个问题。

+ 如何渲染

  对于点光源来说，使用cubemap的渲染算法，只需要按照场景中的物体到光源的最远距离来设置渲染边界即可。对于平行光来说，由于其没有透视效果，需要**使用正交投影**的变换矩阵来渲染深度阴影纹理。我们不可能渲染一幅无限大的纹理，所以在渲染平行光深度纹理时要对光源视锥体的大小和坐标进行确认。为了确保摄像机视锥体中的所有物体都被包含到光源视锥体中，我们需要想办法确定一个合适的光源视锥体的范围。我们将这个问题简化为给定场景的AABB和光源方向，求光源视锥体：

  <img src="Textures\光源视锥体.jpg" alt="光源视锥体" style="zoom:50%;" />

  注意，这里我们不是让光源视锥体包含摄像机视锥体，而是让光源视锥体包含场景AABB，这是因为摄像机视角下的阴影不一定是由摄像机视角下的物体产生的。

  我们使用下面的伪代码来模拟计算光源视锥体的过程：
  
  ~~~~C#
  private void FitToScene() {
      List<Vector3> aabbBounds = GetSceneAABBCorners(sceneObject);
   
      float xmin = float.MaxValue, xmax = float.MinValue;
      float ymin = float.MaxValue, ymax = float.MinValue;
      float zmin = float.MaxValue, zmax = float.MinValue;
      foreach (Vector3 cornerPoints in aabbBounds) 
      {        
          //将AABB的顶点转移到光源坐标空间下
          Vector3 pointInLightSpace = transform.worldToLocalMatrix.MultiplyPoint(cornerPoints);
          
          xmin = Mathf.Min(xmin, pointInLightSpace.x);
          xmax = Mathf.Max(xmax, pointInLightSpace.x);
          ymin = Mathf.Min(ymin, pointInLightSpace.y);
          ymax = Mathf.Max(ymax, pointInLightSpace.y);
          zmin = Mathf.Min(zmin, pointInLightSpace.z);
          zmax = Mathf.Max(zmax, pointInLightSpace.z);
      }
   
      float xsize = (xmax - xmin)/2;
      float ysize = (ymax - ymin)/2;
      float zsize = (zmax - zmin)/2;
   
      //设置光源处的摄像机
      lightCamera.transform.localPosition = new Vector3((xmin+xmax)/2, (ymin+ymax)/2, 0);
      lightCamera.orthographicSize = Mathf.Max(xsize, ysize);
      lightCamera.nearClipPlane = zmin;
      lightCamera. farClipPlane = zmax;
  }
  ~~~~

+ 如何采样

  我们获得了一幅在光源空间下的深度阴影纹理，加下来讨论如何利用这幅纹理判断摄像机观察空间下的片元是否处于阴影中。

  对于场景中的一点，我们可以将它转换到光源空间中，获取其在光源空间下的屏幕坐标，这样就可以在光源空间下的深度阴影纹理上采样。如果这个片元暴露在光源中，抛弃浮点数误差，则其在光源空间下的坐标深度应该等于在深度纹理上采样的深度。

  但事实上直接对深度进行比较仍无法完美的实现的阴影效果，问题出在图像分辨率误差上：

  <img src="Textures\ShadowAcne.png" alt="ShadowAcne" style="zoom:80%;" />

  如图所示，黄色阶梯状的线段表示了深度阴影纹理的值，因为纹理的分辨率不足，所以实际上多个片元会被采样到同一个深度上。在这种情况下，如果直接判断片元在光源空间下的深度是否大于等于深度阴影纹理中的深度，则会有部分片元被认为处于阴影中，形成明暗相交的条纹。这种因为自己遮挡自己而产生的阴影条纹被称为**Self Shadow**，这种现象被称为**Shadow Acne(阴影渗漏)**。

  为了避免Shadow Acne，我们在计算片元在光源空间下的深度时需要先将片元坐标朝光源方向上拉近一点：

  <img src="Textures\ShadowBias.png" alt="ShadowBias" style="zoom: 80%;" />

  我们需要计算一个合理的Shadow Bias值，如果过小则无法解决Shadow Acne问题，如果过大就或导致阴影彻底偏离正确位置。

  目前常用的Shadow Bias计算方法是**基于物体坡度的Shadow Bias**，当在光源坐标空间中的坡度越大，深度阴影纹理上的一个像素对应的物体上的一个面的深度变化就会越大，就需要更大的Shadow Bias来解决问题。坡度可以根据光源坐标空间中的法线计算得到：slope = 1- dot(normal, -light)，英伟达提供的bias公式为bias = factor * slope + constrantBias。

在Unity中，产生阴影的物体需要使用一个额外的Pass来在深度阴影纹理上渲染自己的深度。我们将这个Pass的LightMode标签设为ShadowCaster，则Unity会调用这个Pass来渲染深度阴影纹理。当开启一个灯光的阴影投射后，Unity可以在物体材质的Fallback队列中自动搜索到一个标记了ShadowCaster的通路，所以我们没有必要手动写一个Pass来实现阴影投射。与产生阴影相关的内置属性可以直接在物体所挂的Mesh Renderer组件中修改。

对于多光源的情况，每个光源都可以产生一个深度阴影纹理。在着色器中对这些深度阴影纹理逐个进行处理，可以得到若干个屏幕空间的阴影纹理，将这些屏幕空间下的阴影纹理混合就可得到一个总的**屏幕空间阴影纹理**，它表示了在屏幕空间中某个片元接受到的来自所有光源产生的阴影强度的总和。但这种屏幕空间阴影纹理技术并不被所有硬件兼容，在部分硬件中仍需要每个物体对每个光源运行一个pass来实现在光源空间中的阴影计算，也可以在引擎中通过设置手动关闭屏幕空间阴影。在Unity中，可以使用一组三个宏来跨平台地接受阴影信息：

~~~ShaderLab
struct v2f
{
	float4 pos : SV_POSITION;
	float3 worldNormal : TEXCOORD0;
	float3 worldPos : TEXCOORD1;
	SHADOW_COORDS(2) //这个宏定义了阴影纹理坐标,且使用TEXCOORD2储存
	//LIGHTING_COORDS(2,3) 这个宏定义了光源空间坐标TEXCOORD2,和屏幕阴影坐标TEXCOORD3
}

...

v2f vert(a2v v)
{
	v2f o;
	...
	//这个宏变换了阴影纹理坐标
	//如果硬件允许使用屏幕空间阴影纹理则会变换为屏幕空间的uv坐标
	//如果不支持使用屏幕空间阴影纹理，则会变换为在深度阴影纹理中采样所需的uv坐标
	TRANSFER_SHADOW(o); 
	//使用LIGHTING_COORDS时,这里改用TRANSFER_VERTEX_TO_FRAGMENT
	return o; 
}

fixed4 frag(v2f i) : SV_Target
{
	...
	fixed shadow = SHADOW_ATTENUATION(i); //这个宏自动在屏幕空间阴影纹理或阴影纹理贴图上采样得到阴影强度
	//使用LIGHTING_COORDS时,这里改用LIGHT_ATTENUATION(i);
	...
}
~~~

深度阴影算法产生的阴影俗称“**硬阴影**”。因为光的衍射，影子的边缘应该是渐变的，这种阴影被称为"**软阴影**"，但由于深度阴影算法没有考虑这一情况，所以产生的阴影具有非常明显生硬的边缘。减弱硬阴影的算法有很多，有在屏幕空间的软阴影算法，如屏幕空间阴影高斯模糊；也有在光源空间的软阴影算法，如**百分比渐进滤波(PCF)**算法，其思路是：在片元被变换到光源空间后，不只与深度阴影纹理中对应uv处的深度值比较，还比较了深度阴影纹理周边纹素中的深度值。一般来说，PCF比较深度阴影纹理中相邻3x3的纹素，通过判断一个片元在3x3的纹素中被其中的多少遮挡，可以得到9个等级的阴影强度。相应的，如果提升采样和比较的距离，可以得到更多阶的阴影：

<img src="Textures\PCF.png" alt="PCF" style="zoom: 33%;" />

深度阴影算法的最大缺陷是阴影边缘锯齿问题，这是深度阴影纹理分辨率不足导致的。虽然屏幕空间高斯模糊可以一定程度上缓解这个问题，但在小物体阴影中仍然表现非常糟糕，这里的小物体指物体相对于场景较小，所以在深度阴影纹理中只能占据很少的像素数。为了缓解深度阴影的锯齿，可以渲染多张不同细节程度的深度阴影纹理，这个算法称为**CSM(层叠式阴影纹理，Cascaded Shadow Map)**，可以理解为深度阴影纹理的mipmap：

![CSM shadow quality](Textures\CSM shadow quality.png)

+ 将摄像机视锥体分成若干个子视锥体
+ 对每个子视锥体中的物体构建一个光源视锥体，然后渲染深度阴影纹理
+ 在片元中根据观察空间深度判断应该采样哪一幅深度阴影纹理
+ 对两个不同分辨率的交界处进行混合处理

##### 什么是深度透射算法？

深度阴影纹理除了可以实现阴影外还可用于实现透射效果，常用于玉石、蜡烛、皮肤等并非完全透明但存在一定透射的物体：

<img src="Textures\基于阴影纹理的透射.jpg" alt="基于阴影纹理的透射" style="zoom: 67%;" />

在计算阴影时会将当前点到灯光的距离和深度阴影纹理中对应点的深度做对比，如果前者大于后者则表示被遮挡，此处我们只需求出这个具体的差值记为光在物体内部传播的厚度。这里我们注意到，在存在多重遮挡的情况下，这个差值事实上远大于物体在光源方向的厚度。幸运的是，在多重遮挡的情况下此处本就应当产生阴影，额外计算的厚度并不会导致透射计算的误差过大。

在透射运算中，光线在物体内部传输的衰减程度与距离呈指数关系，我们可以根据皮肤的散射率计算这个值：

| 卷积层数 | 散射距离(mm^2)，用于生成不同的卷积系数 | 红光混合权值$w_r$ | 绿光混合权值$w_g$ | 蓝光混合权值$w_b$ |
| -------- | -------------------------------------- | ----------------- | ----------------- | ----------------- |
| 1        | 0.0064                                 | 0.233             | 0.455             | 0.649             |
| 2        | 0.0484                                 | 0.100             | 0.336             | 0.344             |
| 3        | 0.1870                                 | 0.118             | 0.198             | 0.000             |
| 4        | 0.5670                                 | 0.113             | 0.007             | 0.007             |
| 5        | 1.9900                                 | 0.358             | 0.004             | 0.000             |
| 6        | 7.4100                                 | 0.078             | 0.000             | 0.000             |

~~~ShaderLab
float3 transmit(float s) {
  return float3(0.233, 0.455, 0.649) * exp(-s * s / 0.0064) +
         float3(0.1,   0.336, 0.344) * exp(-s * s / 0.0484) +
         float3(0.118, 0.198, 0.0)   * exp(-s * s / 0.187)  +
         float3(0.113, 0.007, 0.007) * exp(-s * s / 0.567)  +
         float3(0.358, 0.004, 0.0)   * exp(-s * s / 1.99)   +
         float3(0.078, 0.0,   0.0)   * exp(-s * s / 7.41);
}

//透射颜色与透射距离、光源强度，以及透射光和法线的夹角有关
float transmittance = transmit(s) * _LightColor0.rgb * max(0.3 + dot(-normal, lightDir), 0) * albedo.rgb;
~~~

##### 什么是实时光线追踪算法?

光线追踪是一个历史悠久的仿真算法，最早可以追溯到20世纪六十年代。虽然近期光线追踪算法有重新大热的趋势，但光线追踪并不是替代光栅化算法的全新算法，相反，光栅化算法是为了解决光线追踪算法性能缺陷而开发出来的。GPU加速光线追踪算法也是一个持续数十年的热门话题，目前最火热的实时光线追踪是基于微软的**DXR(DirectX Raytracing，一种基于BVH的光追算法)**和英伟达的**RTX(一种基于DXR的硬件架构)**的光追方案，经过实测，实际效率也很难达到其所吹嘘的高度，一般需要配合**DLSS(深度学习超级采样)**来降低采样次数。纯粹的光线追踪目前很难在保持质量的基础上将运行速率提高足矣实时渲染的程度。

**目前的光线追踪算法想要实现实时渲染，仍然需要与光栅化算法混合使用，或者说当下主流引擎仍仅将光追看作一种后处理方案**。在这种结构下，通常将光线追踪特性渲染到一个**离屏表面(Off-Screen Surface)**上，再与光栅化的场景混合到一起。这么做有两个优点：一是光线追踪的精度与画面精度分离，可以通过降低光追精度来加速；二是可以沿用光栅化管线的既有功能，在光追难以实现和计算量大的特性上使用光栅化管线实现。



##### 什么是蒙特卡洛法光线追踪？

**蒙特卡洛积分**是图形学里常用的数值方法。其核心思想是，对于一个复杂的积分，如果难以解出解析解，就随机产生若干次变量，得到若干个特殊解，当随机次数足够大时，就可以根据特殊解以及随机变量的概率密度函数来逼近解析解。

先跳过数学部分直接给出蒙特卡洛法的公式：
$$
\int_{-\infin}^\infin g(x)dx\approx\frac1N\sum_{k=1}^N\frac{g(x_k)}{f(x_k)}
$$
其中$f(x_k)$为随机量$x$在$x_k$的概率密度函数。

我们可以从期望和方差的角度验证蒙特卡洛积分的正确性：
$$
E[\frac1N\sum_{k=1}^N\frac{g(x_k)}{f(x_k)}]=\frac1N\sum_{k=1}^N\int\frac{g(x)}{f(x)}\cdot f(x)dx=\int g(x)dx
$$

$$
\sigma^2[\frac1N\sum_{k=1}^N\frac{g(x_k)}{f(x_k)}]=\frac1N[\int\frac{g(x)^2}{f(x)}dx-E]
$$

回顾一下在光线追踪中得到的渲染方程：



##### 什么是屏幕空间全局光照(SSGI)？

屏幕空间全局光照实际上可以分为**屏幕空间反射(SSR, Screen Space Reflection)**和**屏幕空间环境光遮蔽(SSAO, Screen Space Ambient Occlusion)**等子技术，它们都运用了类似光线追踪的实现思路，但在性能上相比光追要可靠的多。但总的来说，相比预处理的全局光照算法依然慢的离谱。所以**屏幕空间光照一般使用在具有大量动态物体的场景中**。

+ 屏幕空间反射



+ 屏幕空间环境光遮蔽(SSAO)

  所谓**环境光遮蔽(Ambient Occlusion, AO)**，可以理解为烘焙光照贴图的一种相似处理。它根据环境光参数和环境几何信息，来计算场景中任意一点的光照强度系数。其中，环境光遮蔽值描述了表面上任意一点接收到的环境光被周围几何体所遮蔽的百分比，这使得渲染的结果更有层次感。

  **屏幕空间环境光遮蔽(SSAO)**就是在屏幕空间中计算环境光遮蔽的算法，可以理解为一种后处理效果。这个效果利用了屏幕空间中的若干后期纹理，包括：屏幕空间坐标纹理、屏幕空间法线纹理、屏幕空间

  首先，对屏幕空间中的每个片元，获得其屏幕空间坐标P和屏幕空间法线N。

  然后，在屏幕空间法线方向上一定范围的半球体中，随机取若干个点进行采样，得到它们的屏幕空间坐标$P_i$：

  <img src="Textures\SSAO.jpg" alt="SSAO" style="zoom:50%;" />

  紧接着，对$P_i$在屏幕空间深度纹理上采样，类似深度阴影算法，通过对比就可以得知采样点$P_i$是否在屏幕空间中被遮挡。根据该点的方向和距离计算这个被遮挡的采样点对目标点P的遮挡的影响。一般来说，距离P越远的采样点，和偏离法线的角度越大的采样点，其对总遮挡的影响越小。

  最后将各个采样点被遮挡的情况加权平均，来作为点P被遮蔽的程度。

  这里我们使用的是法线方向的半球体，而不是球体，因为通常来说法线背面方向上都是物体内部，且几乎不影响该点的遮蔽性，这将浪费一半的性能在不需要的计算上。

  如果采样次数过多，就会产生性能瓶颈。但如果采样次数过少，则会因为随机性产生噪点。一般采样次数在10~20次就足矣产生可堪使用的环境光遮蔽，并且可以对远距离的片元进行更少次数的采样来实现LOD。

  通常来说，在使用SSAO之后需要对SSAO部分进行一次模糊再应用在原图像中。除此之外，还需要进行时域平滑来避免因为随机采样导致的闪动。另一种处理时域问题的方案是，预先生成采样点的随机序列，然后对屏幕空间中的每个片元都应用相同的一组采样点，这样也可以避免随时间产生的闪动。

  可以通过对法线进行轻微偏移的办法来增加整体的随机性，这么做可以一定程度上减弱噪点，但仍然需要进行额外的模糊才能达到均匀的阴影效果。

#### 预处理全局光照技术

##### 什么是烘焙式全局光照？

烘焙光照是为场景中静止不动的物体和光源预先针对间接照明进行计算，将计算结果储存在**光照贴图(Lighting Map)**中，然后在运行时使用这些间接光照信息进行渲染的技术。其中，生成光照贴图的过程被称为烘焙(baking)。

一个经典的烘焙算法被称为**辐照度法**。

##### 什么是预计算实时全局光照？

**预计算实时全局光照(Precomputed Realtime Global Illumination, PRGI)**，是为了弥补在烘培光照中光源的数量、类型、位置等属性不可变的缺陷产生的技术。注意，PRGI只能实现光源属性可变的光照环境，仍无法实现动态物体的实时全局光照渲染。

PRGI在计算物体表面间接照明效果的基础上，还需要处理沿不同方向入射的光线的出射方向的问题。

PRGI也能实现较为柔和的阴影，但是除非场景很小，否则这些阴影的效果一般比使用烘培式全局光照产生的阴影要粗糙。

##### 什么是环境法线算法(Bent Normal)？

对于不管什么反射模型的渲染方程来说，几何模型的表面法向量是必需的一个信息元素。**环境法线算法**是在运行前进行的对法线的一种修正，它主要有两个作用：1、代替原始Normal进行烘焙光照，增强烘焙环境贴图时的效果；2、代替原始Normal进行光照计算，可以在静态物体上得到类似于**环境光遮蔽(AO)**的自遮挡阴影效果。

在环境法线的修正中，我们希望让新向量朝向一个不被其它几何体遮挡的平均方向，也就是光线传入的主要方向。由于在任何一种反射模型当中都存在这样的性质：入射方向和法线方向越接近，反射的强度就越大，所以经过修正后的法线也可以保证物体在遮挡最小的方向上表现的最亮，从而实现环境光遮蔽的效果。

整个修正的过程可以以积分形式表示为：
$$
N_b(x)=\frac1\pi\int_\Omega w*V(x,w)\mathrm{d}w
$$
其中$V(x,w)$表示点$x$在$w$方向上的可见性，也就是被遮挡的程度。整个公式就是对球面上的向量按照遮挡程度进行加权平均的过程：

<img src="Textures\Bent Normal.png" alt="Bent Normal" style="zoom:67%;" />

这个积分可以使用蒙特卡洛法离散化来获得一个可行的Bent Normal公式：

$$
N_b(x)\approx\frac1\pi\frac1N\sum_{k=1}^N\frac{w*V(x,w)}{\frac1{4\pi}}=\frac4N\sum_{k=1}^Nw*V(x,w)
$$
为了离线烘焙这个新的Bent Normal，就需要得到物体表面在若干个方向上的可见性程度。为了一批处理尽可能多的表面，我们可以使用类似深度阴影纹理的思路来一次性计算场景中所有片元各个方向上的可见度：

+ 首先，生成待烘焙物体或场景的AABB盒，方便确定渲染深度纹理的视锥体。
+ 生成足够多均匀朝向不同方向的正交摄像机，将它们的视锥体恰好移动到包含物体或场景的AABB盒。
+ 使用一个摄像机渲染一副深度纹理，在物体表面使用PCF采样获得软阴影强度s，则$V(x,w)=1-s$，计算出$w*V(x,w)$，将结果用Blend One One混合叠加到环境法线纹理输出上。
+ 重复上一步直到所有摄像机渲染完毕，最后遍历环境法线纹理的每个像素，乘以系数$\frac4N$，得到最终的Bent Normal纹理。

**使用像机一次性渲染场景中的所有表面，还可以避免传统使用蒙特卡洛法加速的技术中常出现的噪点问题**。

##### 什么是基于图像的光照(IBL+SH)?

使用烘焙光照贴图能大幅度提升场景渲染的真实性，但缺点是光照贴图无法应用于非静态物体上，这导致运动的物体和场景显得很不协调。而**IBL**就是为了弥补烘培光照和预计算实时全局光照(PRGI)不能处理动态物体的缺陷产生的技术。

传统的Cubemap技术广义上也属于IBL的范畴。但在实现传统的Cubemap实现环境光照时，需要对每个材质挂载一个专门的Cubemap。换句话说，CPU需要对位于不同位置的物体更新、调整Cubemap，这将导致一定程度上的混乱。

光照探针技术相当于是一组封装后的Cubemap，它们在烘焙阶段或预计算阶段，对空间中的一些采样点(这些采样点就被称为光照探针)采集Cubemap，然后在运行环境下渲染运动物体时，CPU会从离运动物体最近的若干个光照探针中取出Cubemap，按照物体距离Cubemap的距离对这几个Cubemap进行插值，然后填充到着色器的纹理缓冲区中。**光照探针系统有效的实现了CPU和GPU在Cubemap上的解耦**：CPU只需要维护一组固定的光照探针组，而GPU只需要处理与物体相关的一个或几个Cubemap。

光照探针通过渲染Cubemap来捕获环境景象，这意味着它需要对场景进行六次渲染，每次渲染立方体的一个面。虽然传统的反射用光照探针通常被用于预处理全局光照，但**目前的引擎已经提供了实时渲染光照探针的选项**，这个方案比较消耗性能，可以通过限制反射用光照探针实时刷新的频率来降低其性能消耗。

和Cubemap遇到的限制类似，光照探针不适合描述光线穿过大型物体的情况，也不适用于平坦物体或者凹多边形物体表面的光照。对于前面提到的大型、平坦、凹多边形三种情况，可以使用**光探针代理体技术(Light Probe Proxy Volume, LPPV)**进行处理。

光照探针也可以应用LOD技术，其中对光照探针进行降采样(模糊处理)时使用的就是著名的球谐函数。



 

**使用球谐函数的IBL也被称为球谐光照(又称SH)**。

概括的说，球谐光照是**把真实环境中连续的光照方程通过采样进行离散化**(储存到贴图中)，**通过球形坐标系分解简化反射方程**，**通过求解积分(球谐变换)获得球谐系数**，最后**在运行时根据球谐系数重构光照方程**。总结来说就是：**采样、分解、变换、重构**。

通过对真实模型进行简化，我们通过对一点的法线方向所对应的半球空间中的光照进行积分，得到该点应得的光照强度，也就是在半球空间中对光线方向函数$V$的亮度函数$L$进行积分：
$$
L(x,\omega_o)=L_e(x,\omega_o)+\int_sf_r(x,\omega_i\rightarrow\omega_o)L(x',\omega_i)G(x,x')V(x,x')d\omega_i
$$
上式中，$L(x,\omega_o)$表示在位置x上，出射方向为$\omega_o$的光线的辐射度；$L_e(x,\omega_o)$表示物体在位置x上，自发光方向为$\omega_o$的光线的辐射度；$f_r(x,\omega_i\rightarrow\omega_o)$表示在位置$x$出的BRDF，其中入射光为$\omega_i$，出射光为$\omega_o$；$L(x',\omega_i)$表示从其他物体上的位置点$x'$处，沿着方向$\omega_i$照射来的光线的辐射度；$G(x,x')$表示$x$与$x'$的几何衰减；$V(x,x')$表示$x$与$x'$的遮挡损耗。

整个公式表达了，某个待考察的半球空间的出射辐射度等于入射辐射度值得立体角微分在半球球面上得积分。而入射辐射度则利用辐射度传输算法或光线跟踪算法产生。在现阶段硬件中，很难高效求解该式的积分，所以需要使用蒙特卡洛积分法进行近似地模拟。

在蒙特卡洛法光线追踪中我们已经提到了蒙特卡洛积分：
$$
\int_{-\infin}^\infin g(x)dx\approx\frac1N\sum_{k=1}^N\frac{g(x_k)}{f(x_k)}
$$
在球面上均匀采样，得到使用的概率分布函数的方程$\int_0^{4\pi}f(x)dx=1$，解得$f(x)=\frac1{4\pi}$。

可以得到原始积分方程近似模拟的算式为：
$$
\int_0^{4\pi}g(x)dx\approx\frac{4\pi}N\sum_{i=1}^Ng(x_i)
$$


##### 什么是PhotonMap算法？



#### 标准着色器

##### Standard着色器的结构如何？

这里我们讨论的是Unity的Standard.shader文件。

它由两个SubShader和一个Fallback组成，依次完成高性能、中等性能和低性能的渲染任务。其中我们只关注第一个SubShader，即最复杂但对性能要求也最高的SubShader，接下来我们称该SubShader为标准着色器。

标准着色器由五个Pass组成：基于前向渲染途径的ForwardBase通路和FORWARD_DELTA(等同于ForwardAdd)通路，与阴影投射相关的ShadowCaster通路，基于延迟渲染途径的延迟通路，以及用于处理与光照探针相关的镜面光照的META通路。

标准着色器使用的光照模型是基于迪斯尼模型的，使用**GGX法线分布函数和Smith-Joint几何衰减函数**的物理光照。

从美术人员的角度，标准着色器使用**金属度/粗糙度工作流**，这说明**标准着色器使用Metallic和Smoothness两个参数来控制光照效果**，与使用Specular和Glossiness来控制光照效果的**高光强度/光泽度工作流**相对。其中，**Metallic表示菲涅尔公式中的$F_0$，决定了物体表面反射能量的强度**，通常来说，金属物体的金属性在0.9以上，非金属集中在0.2以下。而**Smoothness表示微表面法线与宏观表面法线方向一致的微表面在所有微表面中所占的比例，比例越大物体越光滑**。

标准着色器中含有大量多编译指令，在接下来的分析中会尽量跳过这些编译指令，对尽可能多的效果进行实现。

##### 如何实现标准着色器的ForwardBase通路？

根据我们对前向渲染路径的了解，这个通路中我们需要逐片元处理环境光、主有向平行光和光照贴图，并处理所有逐顶点光照和球谐光照。在这个通路中，定义了标准和简化两个版本的顶点和片元着色器，这里我们以

##### 如何实现标准着色器的ForwardAdd通路？



##### 如何实现标准着色器的ShadowCaster通路？



##### 如何实现标准着色器的延迟渲染通路？



##### 如何实现标准着色器的Meta通路？

Unity主要使用辐照度算法视线全局光照，由于辐照度算法主要针对间接光产生的漫反射，而难以计算间接光产生的高光反射，所以间接光的高光反射难以烘焙进光照贴图中。Meta通路就是专门用于补偿间接高光的通路。

Meta通路需要自发光贴图和反照率贴图来计算反光，以此使物体对周围物体产生光照影响。

<div STYLE="page-break-after:always;"></div>

### 风格化渲染

#### 卡通渲染

##### 如何实现日式卡通渲染?

卡通渲染主要分为美式卡通渲染和日式卡通渲染。其中，日式卡通的特点是具有明显的明暗交界和大范围的纯色色块。一般来说，在谈到卡通渲染或"三渲二"时指代的都是日式卡通渲染。

日式卡通渲染的主要特征有：描边、光照离散化、对明暗分界线增加饱和度。

日式卡通渲染的着色基于Blinn-Phong，这是因为日式卡通渲染不需要多少光照细节。

在描边方案的选择上有多种方案，如法线外扩、法线外移、模板法线外扩、菲涅尔描边、Sobel卷积描边、深度/法线纹理卷积描边等【见屏幕后处理】。

漫反射部分我们需要在兰伯特光照的基础上实现没有过渡的涂色感，这一步称为离散化。离散化的手段一般被分为**Cel Shading**和**Tone Based Shading**两种。

在Cel Shading模式中，在漫反射的处理中，**通过一个阈值将整个模型的漫反射区分为亮部和暗部**，在漫反射强度大于阈值时将其截断为亮部，在漫反射强度小于阈值时将其截断为暗部。这个步骤用step函数(或Ramp函数)实现：

~~~shaderlab
float step(float a, float x)
{
	return x >= a ? 1 : 0;
}
~~~

如果想要实现更平滑的截断效果(在色块与色块间更平滑的过渡)，可以使用smoothstep效果：

~~~glsl
float smoothstep(float a, float b, float x)
{
    float t = saturate((x-a)/(b-a));
    return t*t*(3.0-(2.0*t));
}
~~~

或者使用基于**坡道贴图(RampMap)**的漫反射算法，使用一个坡道贴图来实现由夹角到光照强度的映射，换句话说就是用对一副贴图的采样来代替颜色变换函数：

~~~hlsl
float difLight = dot(s.Normal, lightDir);
float hLambert = difLight * 0.5 + 0.5;
float3 ramp = tex2D(_RampTex, float2(hLambert, hLambert)).rgb;
		    
float4 col;
col.rgb = s.Albedo * _LightColor0.rgb * (ramp);
col.a = s.Alpha;
return col;
~~~

整个漫反射处理过程可以整理为下图：

![日式卡通漫反射](Textures\日式卡通漫反射.png)

高光反射在Blinn-Phong的基础上使用和漫反射一样的截断或者平滑截断处理。

在明暗分界线上增加饱和度，明暗分界线可以通过判断漫反射强度与漫反射分界阈值的差来获取，在获取到这个差值后，根据距离的大小，另距离分界线越近的像素提高越多的饱和度。饱和度算法见【色彩学基础-色彩运算-颜色修正】。

使用Ramp图实现的一个卡通渲染如下：

~~~ShaderLab
Shader "Unlit/3R2"
{
    Properties
    {
        _Color ("Main Color", Color) = (1,1,1,1)
        _ShadowColor ("Shadow Color", Color) = (0.5,0.5,0.5,1)
        _Specular ("Specular", Float) = 20
        _SpecularGloss("Specular Gloss", Float) = 20
        _OutlineThickness ("Outline Thickness", Float) = 1

        _MainTex ("Diffuse Texture", 2D) = "white" {}
        _NormalMap("Normal Texture", 2D) = "white" {}
        _SpecularMap("Specular Texture", 2D) = "white" {}
        _DiffuseRamp("Diffuse Ramp", 2D) = "white" {}
        _DiffuseRampPower("Diffuse Ramp Power", Range(0,1)) = 0.3
        _SpecularRamp("Specular Ramp", 2D) = "white" {}
        _SpecularRampPower("Diffuse Ramp", Range(0,1)) = 0.3
        
    }
    SubShader
    {
        Tags { "RenderType"="Opaque" "LigetMode"="ForwardBase"}
        Cull Off
        LOD 100

        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "AutoLight.cginc"
            #include "Lighting.cginc"
            #include "UnityCG.cginc"

            uniform float4 _Color;
            uniform float4 _ShadowColor;
            uniform float4 _Specular;
            uniform float _SpecularGloss;
            uniform float _OutlineThickness;

            sampler2D _MainTex;
            float4 _MainTex_ST;
            sampler2D _NormalMap;
            float4 _NormalMap_ST;
            sampler2D _SpecularMap;
            float4 _SpecularMap_ST;
            sampler2D _DiffuseRamp;
            float4 _DiffuseRamp_ST;
            float _DiffuseRampPower;
            sampler2D _SpecularRamp;
            float4 _SpecularRamp_ST;
            float _SpecularRampPower;

            struct a2v
            {
                float4 vertex : POSITION;
                float2 uv : TEXCOORD0;
                float4 normal : NORMAL;
                float4 tangent : TANGENT;
            };

            struct v2f
            {
                float4 uv0 : TEXCOORD0;
                float4 uv1 : TEXCOORD1;
                float3 viewDir : TEXCOORD2;
                float3 lightDir : TEXCOORD3;
                float4 vertex : SV_POSITION;
                SHADOW_COORDS(4)
            };

            v2f vert (a2v v)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(v.vertex);
                o.uv0.xy = TRANSFORM_TEX(v.uv, _MainTex);
                o.uv0.zw = TRANSFORM_TEX(v.uv, _NormalMap);
                o.uv1.xy = TRANSFORM_TEX(v.uv, _SpecularMap);
                o.uv1.zw = float2(0, 0);

                float3 binormal = normalize(cross(normalize(v.normal), normalize(v.tangent)) * v.tangent.w);
                float3x3 rotation = float3x3(v.tangent.xyz, binormal, v.normal.xyz);

                o.viewDir = mul(rotation, ObjSpaceViewDir(v.vertex)).xyz;
                o.lightDir = mul(rotation, ObjSpaceLightDir(v.vertex)).xyz;
                TRANSFER_SHADOW(o);
                return o;
            }

            float4 frag(v2f i) : SV_Target
            {
                float4 o;

                float4 diffuseColor = tex2D(_MainTex, i.uv0.xy);
                float3 normal = UnpackNormal(tex2D(_NormalMap, i.uv0.zw));
                float3 specular = tex2D(_SpecularMap, i.uv1.xy);

                float3 viewDir = i.viewDir;
                float3 lightDir = i.lightDir;
                float3 halfDir = normalize(lightDir + viewDir);

                float shadow = SHADOW_ATTENUATION(i);

                float NdotL = dot(normal, lightDir);
                float NdotV = dot(normal, viewDir);
                float NdotH = dot(normal, halfDir);

                float diffuseSampler = clamp((NdotL + 1.0) * 0.5, 0.02, 0.98);
                float diffuseRampColor = tex2D(_DiffuseRamp, float2(diffuseSampler, 0.5)) * _DiffuseRampPower;        
                o = lerp(diffuseColor * _ShadowColor, diffuseColor, diffuseRampColor);
                o *= (1.0 + diffuseSampler * diffuseSampler);
                o *= float4(_Color * _LightColor0.rgb, 1);

                float3 specularColor = specular * pow(saturate(NdotH), _SpecularGloss);
                float specularSampler = (specularColor.r + specularColor.g + specularColor.b) / 3 * _Specular;
                float4 specularRampColor = tex2D(_SpecularRamp, float2(specularSampler, 0.5));
                o += float4(_Color.rgb * _LightColor0.rgb * specularRampColor,1);

                float4 shadowColor = o * _ShadowColor;
                float attenuation = saturate(2.0 * shadow - 1.0);
                o = lerp(shadowColor, o, attenuation);

                return o;
            }
            ENDCG
        }
    }
}
~~~

##### 如何实现美式卡通渲染？

美式卡通的特点是色彩连续、存在渐变色。



##### 如何实现卡通次表面散射？

首先我们需要有次表面散射的基本知识【见实时局部光照-次表面散射】，接下来我们将次表面散射改造为适合卡通渲染的情况。



#### 艺术化渲染

##### 如何实现像素风格渲染？



##### 如何实现素描风格渲染?



##### 如何实现水墨风格渲染?



##### 如何实现油画风格渲染？

油画风格渲染本质上是一种特殊的模糊滤镜，换句话说是一种后处理效果。

为了实现油画渲染，需要解决颜料感和笔触感。所谓颜料感就是需要相近颜色形成一定色块，这些色块的边界清晰，存在一定的颜色变化。所谓笔触感就是色块具有一定的方向性，朝某个角度延展。

为了实现颜料感，就必然需要对一定区域的色彩取均值。油画风格的均值计算方法与高斯模糊的思路不同：高斯模糊需要对所有相邻的像素取均值，而油画风格渲染中只需要对相邻像素中色彩相近的部分取均值。这种特殊的均值求法有很多种实现思路，其中一个著名的思路是象限法：关注一个像素点，将该点周围边长为2n+1个像素的正方形纳入考虑范围，然后将这个正方形分成四个象限区域，分别命名为NE、NW、SW、SE。在这四个区域中挑选较接近的一个象限，对该区域的像素取均值，作为本像素的输出结果。

##### 风格迁移算法如何应用于艺术化渲染?

风格迁移算法是起源于2001年的一种将一副图片的风格迁移到另一副图片上的算法，其核心思路是将图片A切割成若干个小块A‘，根据图片B上的光影强度，用A‘拼接出类似B的图像，然后对A’的边缘进行智能的混合，从而实现用A的风格渲染B的效果。我们可以通过将这种技术应用到实时渲染中来实现艺术化渲染。



#### 2D光照

##### 如何实现2D阴影绘制？



<div STYLE="page-break-after:always;"></div>

## 计算机动画

### 骨骼动画

#### 姿势

##### 骨骼动画是如何定义的？

**本质上，骨骼动画是一种逐顶点动画的实现方案，且其核心精神是一种对逐顶点动画的压缩算法**。

通常来说，一个网格在进行动画时，无论是使用关键帧动画、程序化动画还是物理动画，相邻顶点的运动总是具有一定的趋同性。比如手臂上的顶点在播放动画时总会使用相同的一套变换矩阵。换句话说，我们没有必要在记录顶点的动画信息时总是将整个网格的所有顶点的运动都记录下来，只需要记录若干个被共用的变换矩阵，然后让顶点记录它所使用的变换矩阵的索引即可。

在实际应用时，为了使关节交接处的顶点变化更平滑，通常使用权重组代替单一的索引。这种让顶点使用权重数组来引用变换矩阵，并通过插值实现顶点运动的动画技术被称为**矩阵调色板蒙皮技术(Matrix Palette Skinning)**。通常来说，影响同一顶点的关节数不会超过4个，这可以有效防止顶点数组的长度变得过于夸张。

在计算机图形学领域，这些被共用的变换矩阵被形象化的称为**"骨骼(Bones)"**，因为骨骼在运动时会带动其上的皮肤，而变换矩阵的值在变化时会带动使用它的所有顶点运动。根据为每个顶点设置针对不同变换矩阵的权值的过程，则被形象化的称为**“蒙皮(Skin)”**。所有使用骨骼+蒙皮体系实现的动画，都可以被称为骨骼动画，常见的如骨骼关键帧动画、骨骼物理动画等。

对骨骼的储存，使用了被称为**骨骼层次**的树型结构。这是因为，对于一个复杂网格的一小部分(如人形网格的一个手臂)，它的运动通常不是完全独立的，而是相对于一个**父关节(Parent Joint)**进行的(如手臂的运动总是相对于肘关节)。

这颗骨骼层次树使用和场景树一样的父子空间结构实现对变换矩阵的储存：每个关节的变换矩阵是基于父空间，也就是父关节的模型空间定义的。在计算模型空间到世界空间的变换矩阵时，只需要从当前节点逆向遍历其所有父节点，依次左乘每个节点的变换矩阵，就可以得到一个骨骼对应的变换矩阵。在局部坐标系下构建的变换矩阵被称为**本地变换矩阵**，到世界空间的变换矩阵被称为**组合变换矩阵**。

在关键帧骨骼动画中，每一个关键帧被称为一个**姿势(Pose)**，CPU通过在姿势之间插值来获得每一帧的组合变换矩阵，然后实现对网格的变换和渲染。

关键帧之间的插值是关键帧骨骼动画的重要课题，因为变换矩阵中的每一项并不能正交分解，所以就不能被独立插值，这就导致了直接对变换矩阵进行线性插值一定会产生错误的结果。所以，我们必须将变换矩阵正交分解成若干个可以独立插值的项，然后分别进行插值。通常来说，是将矩阵拆分成平移、旋转和缩放项进行分别插值。其中，旋转矩阵是不能直接进行线性插值的，所以必须在四元数形式下进行插值后【见数学知识-几何变换-旋转插值】再变换回矩阵格式。

考虑到**球状三次样条插值**性能略差，可以结合物体距离摄像机的距离，在计算较远物体的动画时使用线性插值，在播放较近物体的动画时使用样条插值，这可以被理解为一种对动画进行的LOD。

鉴于此，在一个可以制作蒙皮骨骼动画的3D软件中，会**使用本地坐标向量+本地旋转四元数+本地缩放的格式来储存关键帧**，换句话说就是引擎中储存Transform的格式。这是因为只有对本地变换矩阵进行插值才能得到正确的骨骼动画插值效果。而在CPU向GPU填充变换矩阵时，则会直接填充生成的组合变换矩阵。

##### 什么是蒙皮解算？

在对骨骼进行定义后，就需要根据变换矩阵计算处顶点的新坐标，可以在CPU中实现对顶点数组的修改，也可以在GPU顶点着色器中实现顶点的坐标运算。这个步骤被称为**蒙皮解算**。

在CPU中实现，则我们需要修改顶点数组，将顶点坐标修正到正确的模型空间坐标中，如伪代码所示：

~~~C#
void AdjustSkinnedMesh(Mesh mesh, Joint joint)
{
	Vector3[] vertices = mesh.vertices;
    Vector3[] normals = mesh.normal;
    for(int i = 0; i < vertices.Length; i++)
    {
        Vector3 animedVertex = new Vector3(0,0,0);
        Vector3 animedNormal = new Vector3(0,0,0);
        float[] weights = new float[4];
        int[] jointID = new int[4];
        
        weights[0] = mesh.jointWeight.x;
        weights[1] = mesh.jointWeight.y;
        weights[2] = mesh.jointWeight.z;
        weights[3] = mesh.jointWeight.w;
        
        jointID[0] = mesh.jointID.x;
        jointID[1] = mesh.jointID.y;
        jointID[2] = mesh.jointID.z;
        jointID[3] = mesh.jointID.w;
        
        for(int j = 0; j < 4; j++)
        {
            float weight = weights[j];//储存第i个顶点的第j组权重值
            int jid = jointID[j];//储存第i个顶点的第j组骨骼ID
            //这里使用二维数组仅做展示用,实际上一般是Vector4
            
            Matrix matrix = joint[jid].matrix;
            Matrix inverseMatrix = joint[jid].inverseMatrix;
            animedVertex += matrix.Multiply(vertices[i]) * weight;  
            animedNormal += inverseMatrix.Multiply(normals[i]) * weight;
        }
        vertices[i] = animedVertex;
        normals[i] = animedNormal.normalized;
    }
    mesh.vertices = vertices;
    mesh.normal = normals;
}
~~~

在GPU中进行顶点坐标变换可以提升约5倍的蒙皮结算速度，尤其是在视锥体中存在大量不同步运动的骨骼动画对象时(即这些对象在同一时间播放不同的动画时)能极大的提高效率。注意，在这种情况下CPU仍要计算所有骨骼的组合变换矩阵，只是省去了对所有顶点应用这个矩阵的过程。这么做的缺陷是，需要对动画物体和非动画物体实现两套Shader，并且需要针对性的修改渲染管线，这大大增加了工作流的复杂度：

~~~ShaderLab
......

uniform float4x4 joints[60];
uniform float4x4 inverseJoints[60];
......

struct a2v
{
	float4 vertex : POSITION;
	float3 normal : NORMAL;
	float2 uv : TEXCOORD0;
	float4 weights : TEXCOORD1;
	float4 boneID : TEXCOORD2;
	......
};

g2f vert(v2f v)
{
	g2f o = (g2f)0;
	
	float4 weights = v.weights;
	float4 boneID = v.boneID;

	float4 pos = float4(0,0,0,0);
	float3 normal = float3(0,0,0);
	
	pos += mul(joints[boneID.x], v.vertex) * weights.x;
	pos += mul(joints[boneID.y], v.vertex) * weights.y;
	pos += mul(joints[boneID.z], v.vertex) * weights.z;
	pos += mul(joints[boneID.w], v.vertex) * weights.w;
	
	normal += mul(inverseJoints[boneID.x], v.normal) * weights.x;
	normal += mul(inverseJoints[boneID.y], v.normal) * weights.y;
	normal += mul(inverseJoints[boneID.z], v.normal) * weights.z;
	normal += mul(inverseJoints[boneID.w], v.normal) * weights.w;
	normal = normalize(normal);
	
	......
	
	o.pos = pos;
	o.normal = normal;	
	......
	
	return o;
}

......
~~~

##### 如何实现动画混合？

动画混合是将两个或更多姿势结合的方法，其本质上和关键帧之间的插值区别不大，但要额外考虑到动画对齐。





##### 什么是动画重定向？

骨骼动画中的动画信息可以被转移到使用相同或相近骨骼的网格中，这个技术被称为**动画重定向**。



#### 混合

##### 什么是IK骨骼动画？

**IK(Inverse Kinematics，反向动力学)骨骼动画**是一种运行时动画计算技术，它是一种逆向生成技术，试图通过给定肢体末端的姿态逆向推断出整个肢体、或者说父骨骼节点的姿态。这个概念与**FK(正向动力学)骨骼动画**相对应，而FK骨骼动画中骨骼由根节点从上自下驱动。如在手部动画时使用FK动画时，先转动腰部，然后转动大臂，齐次是小臂，最后才是腕部。但如果需要实现一个坐着的角色手扶着桌面缓慢起身的动画，如果使用FK就需要不断调整手部的位置以使其能保持在桌面上，这将浪费大量关键帧。而使用IK动画，则可以先设定手部的姿态，在角色起身的过程中手部保持不动，由程序自动计算大臂和小臂的合适位置。

IK骨骼动画的使用需要使用**约束(Constraint)**。约束定义了每个关节可以扭转、拉伸和弯曲的角度范围，如人的膝盖只能向后弯曲并进行及其有限的扭转，而腰部和颈部关节却能向各个方向弯曲，并进行很大限度的扭转。通过设定约束，可以使程序在计算父节点姿态时获得更自然的结果。



##### 如何实现程序化骨骼绑定？



##### 什么是Motion Matching?



#### 物理动画

##### 什么是基于物理的角色动画?



#### 2D骨骼动画

##### 什么是2D关节动画？



##### 如何实现自由变形？



<div STYLE="page-break-after:always;"></div>

### 流体渲染

#### 水体渲染



#### 体积云雾



#### 烟雾火焰



### 引擎物理

#### 刚体物理

##### 如何实现碰撞检测？



#### 粒子物理

##### 什么是PBD？

**PBD(Position Based Dynamic)**是与**FBD(Force Base Dynamic)**相对的概念。



<div STYLE="page-break-after:always;"></div>

### 粒子系统

#### 朴素粒子系统

##### 粒子系统是什么？

粒子系统的概念于1983年在Reeves.V.T的论文《Particle System A Technique for Modeling a Class of Fuzzy Objects》中被首次提及。从此被广泛应用于计算机图形学中各种模糊物体的模拟，比如火焰、爆炸、烟、水流、火花、落叶、云、雾、雪、尘、流星尾迹等景物，以及如发光轨迹等抽象视觉效果。这些物体模型很难用具体的形状、大小来描述，但我们可以通过粒子系统的思想去描述组成这些物体的每个元素和它的变化。

粒子系统具有三个特征：

+ 群体性：粒子系统由大量可见元素构成。
+ 统一性：每个元素具有相同的表现规律。

+ 随机性：每个元素表现出不同的具体特性。

##### 粒子系统有怎样的结构？

一个粒子系统由粒子、发射器、力场构成。发射器发射粒子，粒子在力场中动态的修改属性。

发射器模板需要定义发射器形状(如四边形、圆环、网格等)，发射方向，粒子的更新方式和粒子的初始属性。发射器间隔固定时间产生新的粒子，赋予新粒子初始属性；然后遍历粒子列表，删除(隐藏)超出生存期的粒子，根据粒子的属性更新剩下的粒子；最后将所有粒子整合为一次Draw Call进行渲染。

粒子一般是一个带有纹理的四边形(Billboard)，有时是简单的几何体。我们可以认为粒子实际上是一个很小的网格模型，而纹理赋予了它特殊的外表。粒子通过一些属性来处理行为，这些属性包括位置，方向，缩放，颜色，uv偏移等。一个粒子类的例子如下：

```c
typedef struct//structrue for particle
{
  bool active;//active or not
  float life;//last time
  float fade;//describe the decreasing of life
  float r,g,b;//color
  float x,y,z;//the position
  float vx,vy,vz;//velocity
  float ax,ay,az;//acceleration
}particles;
```

力场作用于粒子，对其运动产生影响。一般力场包括恒力场(如重力)、吸引力场、排斥力场、漩涡力场等。有些引擎中力场和粒子系统是分离的，一个力场可以影响多个粒子系统，一个粒子系统也可以被多个力场影响。

粒子系统的概念十分简单，但其优化难度高，实现高性能大数量的粒子依然存在难度。

##### 使用什么数据结构储存粒子？

为了每一帧去更新粒子系统，有一种直观但不高效的实现如下：

+ 定义一个数组管理所有粒子
+ 遍历数组更新所有粒子的状态
+ 创建一个足够容纳粒子系统中最大粒子数目的顶点缓存
+ 将所有处于活动状态的粒子赋值到这个巨大的顶点缓存中去
+ 将顶点缓存中的粒子全部绘制出来

在创建顶点缓存之后，需要将所有需要的顶点都从数组复制到顶点缓存里面，数目非常的大，在等待CPU拷贝时GPU处于空闲状态，GPU的利用率过低。同时，我们还必须考虑到粒子的生成与销毁的管理开销，对一个包含成千上万元素的数组来说，删除元素可能是毁灭性的。大量的申请和释放内存操作也是性能瓶颈之一。

首先我们引入**池(Pool)**的概念来优化数组的性能。池在程序优化中是十分常见的一个概念，我们可以将暂时不显示的顶点保存在池中来避免回收它的内存。首先我们申请足够存放可能出现的最大数量顶点的内存，假设某粒子系统最多出现15000个顶点。

设数组首指针为BP\*，数组尾指针为TP\*，我们额外声明一个指针作为池指针PP\*，它初始情况下也指向数组尾。在实例化这个数组后，粒子系统仅维护该数组且不再进行任何内存申请和释放。

PP\*与TP\*之间存放着存活的粒子，而BP\*与PP\*之间是暂时不渲染渲染而被保存在池中的粒子。

| BP   |      |      |      |      |      | PP   |      |      |      |      |      | TP       |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | -------- |
| 0    | 0    | 0    | 0    | 0    | 0    | p6   | p5   | p4   | p3   | p2   | p1   | 数组边界 |

每当需要生成新的粒子，只需要让PP向前移动一位，并对该位上的粒子进行初始化。

| BP   |      |      |      |      | PP   |      |      |      |      |      |      | TP       |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | -------- |
| 0    | 0    | 0    | 0    | 0    | p7   | p6   | p5   | p4   | p3   | p2   | p1   | 数组边界 |

而每当有粒子死去，我们把PP指向的粒子移动到该粒子的位置，然后将PB向后移动一位。如图我们删除了p3。

| BP   |      |      |      |      |        | PP   |      |      |      |      |      | TP       |
| ---- | ---- | ---- | ---- | ---- | ------ | ---- | ---- | ---- | ---- | ---- | ---- | -------- |
| 0    | 0    | 0    | 0    | 0    | ~~p7~~ | p6   | p5   | p4   | p7   | p2   | p1   | 数组边界 |

对这些操作进行合法性检查后代码如下：

```c
particles *BP, *PP, *TP;

void initArray(){
    BP = new particles[15000];
 	TP = AB + 15000;
    PP = TP;
}

particles* initParticle(particles* p){}

particles* newParticle(){
    if(PP > BP) PP--;
    return PP;
}

void dieParticle(particles* p){
    p* = PP*;
    if(PP < TP) PP++;
}
```

池的另一个优势是，在更新粒子状态时不需要遍历全部的15000个粒子，而仅需要遍历从PP到TP的这部分有效粒子：

```c
void updateParticle(particles* p){}

void updatePool(){
    for(particles* p = PP; p < TP; p++)
    {
        updateParticle(p);
    }
}
```

同时，我们还会注意到，从PP开始的数组恰好就是我们打算用于渲染的顶点缓冲，将PP作为图形API的输入可以直接省去拷贝过程。

##### 什么是Billboard？

广告牌技术试图实现让多边形始终朝向摄像机的方向，这个技术在粒子系统中十分常用。鉴于GPU的并行特性，在顶点着色器中实现广告牌效果比在CPU中实现具有好得多的性能。

广告牌相关的代码可能出现在CPU端或GPU端，在常见的CPU粒子环境中，广告牌出现在CPU端。除了粒子系统外，还有一些情况也需要使用广告牌效果，一般来说他们也在CPU端实现。

广告牌技术的本质就是构造旋转矩阵。借用【空间变换】的知识，我们只需要找到能够表示多边形状态的三个正交的基向量和一个原点，将它们看作观察空间的子坐标系，对其进行空间变换，就能得到多边形需要的旋转矩阵。

在构造旋转矩阵前，我们需要确定旋转的目标方向。在【几何变换】中我们提到，仅凭单个向量是无法确定一个物体的旋转的。幸好，在实际开发中我们一般希望广告牌指向上的方向永远是(0,1,0)，而法线方向(前方向)随视角变化。只不过，正因如此，广告牌的上方向并不总是与视角方向正交，我们必须对它进行变换才能获得一组正交基向量。首先我们根据表面法线和指向上的方向来计算出目标方向的右方向：
$$
right = up\times normal
$$
然后我们对右方向归一化，再右法线方向和右方向得出正交的上方向：
$$
up'=normal\times normalize(right)
$$
在顶点着色器中，我们在模型空间下进行计算，我们通过空间变换获得模型空间下的视角坐标，记为viewer，然后计算正交基向量，代码基于Unity Shader/CG。

```ShaderLab
float3 viewer = mul(_World2Object, floa4(_WorldSpaceCameraPos,1));
float3 center = float3(0,0,0);
float3 normalDir = viewer - center;
normalDir = Normalize(normalDir);
//防止y与法线方向平行
float3 upDir = abs(normalDir.y) > 0.999 ? float3(0,0,1) : float3(0,1,0);
float3 rightDir = normalize(cross(upDir,normalDir));
upDir = normalize(cross(normalDir, rightDir));
float3 centerOffs = v.vertex.xyz - center;
float3 localPos = center + rightDir * centerOffs.x + upDir * centerOffs.y + normalDir * centerOffs.z;
o.pos = mul(UNITY_MATRIX_MVP, float4(localPos,1));
```

##### 如何使用着色器模拟力场?



##### 如何实现UV粒子动画？



#### GPU辅助粒子系统

##### 如何处理静态的粒子运动？

这里的静态粒子运动，指的是单个粒子的状态和其它粒子无关，且单个粒子的状态和上一时刻的状态无关。即，粒子的状态是一个与时间有关的函数，对于一个给定的时间，粒子状态确定，此所谓“静态粒子”。

显然，我们需要将需要的数据作为顶点属性，通过顶点数组传送到着色器中。下面以Unity风格的方式我们将单个粒子的初始坐标、初速度、和加速度(加速度恒定)作为顶点属性输入着色器，并在顶点着色器中对动态进行了计算：

~~~C#
using UnityEngine.Rendering;

Mesh GenerateVertex(Vector3[] pos, Vector3[] vel, Vector3[] acc, Color[] colors)
    {
        if (pos.Length != vel.Length || pos.Length != acc.Length || pos.Length != colors.Length) 
        {
            throw new System.ArgumentException(); 
        }
    
        Mesh target = new Mesh();

        int vertexCount = pos.Length;

        target.vertices = pos;

        int[] idx = new int[vertexCount];
        for (int i = 0; i < vertexCount; i++)
        {
            idx[i] = i;
        }
        //将索引的拓扑模式设置为顶点，这是为了防止数据被几何着色器拒绝访问
        target.SetIndices(idx, MeshTopology.Points, 0);

        target.SetUVs(1, vel);
        target.SetUVs(2, acc);
        target.SetColors(colors);

        return target;
    }
~~~

在上面的函数中，我们使用Unity的Mesh组件实现了对顶点属性的传输。为了保证能正确的接收这些顶点，我们需要在着色器文件中定义好a2v结构体：

~~~ShaderLab
struct a2v
{
	float4 vertex : POSITION;              
    float4 vel : TEXCOORD1;
    float4 acc : TEXCOORD2;
    fixed4 color : COLOR;
};
~~~

接下来是在顶点着色器中计算粒子的坐标，使用$P_1=P_0+vt+\frac12at^2$这个经典的公式：

~~~ShaderLab
o.vertex = UnityObjectToClipPos(v.vertex + v.vel * _Time.y + 0.5 * v.acc * _Time.y * _Time.y);
~~~

##### 如何使用几何着色器生成粒子？

我们将在这一步里，根据得到的顶点坐标，在几何着色器中生成一个Billboard来渲染指定的图像。

一个Billboard是一个Quad，由4个顶点，即2个三角形组成。我们需要先获得这4个顶点的坐标。首先，一个Billboard应该始终面向摄像机，也就是说这个Quad的法线方向应该始终为视角方向，我们在世界坐标下计算它：

~~~ShaderLab
float3 normal = normalize(_WorldSpaceCameraPos - mul(unity_ObjectToWorld, vertex));
~~~

然后我们将Quad的上方向设为(0,1,0)，并根据它计算出在摄像机视角中朝右的方向向量：

~~~ShaderLab
float3 up = float3(0,1,0);
float3 right = normalize(cross(normal, up));
~~~

这样，我们可以重新得到一个与right和normal方向垂直的up方向：

~~~ShaderLab
up = normalize(cross(right,viewDir));
~~~

这样，我们就得到了一个始终正对摄像机的Quad的四个顶点：

~~~ShaderLab
float size = _ParticleSize * 0.5;

vertex00 = UnityWorldToClipPos(vertex - size * right - size * up);
vertex01 = UnityWorldToClipPos(vertex - size * right + size * up);
vertex10 = UnityWorldToClipPos(vertex + size * right - size * up);
vertex11 = UnityWorldToClipPos(vertex + size * right + size * up);
~~~

在这里，我们将Quad顶点的运算放在了顶点着色器而非几何着色器里，并通过v2g结构体传输给几何着色器。这么做是因为，顶点着色器在流水线中是高度并行的，而几何着色器并不是。这提醒我们，尽可能将复杂的元素放在顶点着色器中实现，而非在几何着色器中实现。整个Pass的绘制过程如下，为了让粒子运动可逆，这里我们定义了一个\_TimeLine属性来代替\_Time.y，这样可以实现在外部修改粒子的播放速度或者实现倒放：

~~~ShaderLab
Pass
	{
		CGPROGRAM
        #pragma vertex vert
        #pragma fragment frag
        #pragma geometry geom
        #include "UnityCG.cginc"
          
        uniform sampler2D _MainTex;
        uniform float4 _MainTex_ST;
        uniform fixed4 _Color;
        uniform float _TimeLine;
        uniform float _ParticleSize;
            
        struct a2v
        {
            float4 vertex : POSITION;              
            float4 vel : TEXCOORD1;
            float4 acc : TEXCOORD2;
        };

        struct v2g
        {
            float4 vertex00 : TEXCOORD0;
            float4 vertex01 : TEXCOORD1;
            float4 vertex10 : TEXCOORD2;
            float4 vertex11 : TEXCOORD3;
        };

        struct g2f
        {
            float2 uv : TEXCOORD0;
            float4 vertex : SV_POSITION;
        };

        v2g vert (a2v v)
        {
            v2g o = (v2g)0;
            float4 vertex = v.vertex + v.vel * _TimeLine + 0.5 * v.acc * _TimeLine * _TimeLine;
            float3 viewDir = normalize(_WorldSpaceCameraPos - mul(unity_ObjectToWorld, vertex));
            float3 up = float3(0,1,0);
            float3 right = normalize(cross(viewDir, up));
            up = normalize(cross(right,viewDir));
            float size = _ParticleSize * 0.5;

            o.vertex00 = UnityWorldToClipPos(vertex - size * right - size * up);
            o.vertex01 = UnityWorldToClipPos(vertex - size * right + size * up);
            o.vertex10 = UnityWorldToClipPos(vertex + size * right - size * up);
            o.vertex11 = UnityWorldToClipPos(vertex + size * right + size * up);
            return o;
        }

        [maxvertexcount(6)]
        void geom (point v2g input[1], inout TriangleStream<g2f> outstream)
        {
            g2f o = (g2f)0;
            o.vertex = input[0].vertex00;
            o.uv = float2(0,0);
            outstream.Append(o);
            o.vertex = input[0].vertex01;
            o.uv = float2(0,1);
            outstream.Append(o);
            o.vertex = input[0].vertex10;
            o.uv = float2(1,0);
            outstream.Append(o);
            outstream.RestartStrip();

            o.vertex = input[0].vertex11;
            o.uv = float2(1,1);
            outstream.Append(o);
            o.vertex = input[0].vertex10;
            o.uv = float2(1,0);
            outstream.Append(o);
            o.vertex = input[0].vertex01;
            o.uv = float2(0,1);
            outstream.Append(o);
            outstream.RestartStrip();
        }

        fixed4 frag (g2f i) : SV_Target
        {
            fixed4 col = tex2D(_MainTex, i.uv) * _Color;
            clip(col.a - 0.5);
            return col;
        }
        ENDCG
    }
~~~

##### 实践：如何使用几何着色器生成多边形草地?

和生成粒子的思路一致，将草根的坐标作为顶点属性输入着色器，然后用几何着色器实现草的生成和渲染。

这里摘录了嘉栋大佬的着色器实现([github链接](https://github.com/chenjd/Realistic-Real-Time-Grass-Rendering-With-Unity))：

~~~ShaderLab
Properties{

		_MainTex("Albedo (RGB)", 2D) = "white" {}
		_AlphaTex("Alpha (A)", 2D) = "white" {}
		_Height("Grass Height", float) = 3
		_Width("Grass Width", range(0, 0.1)) = 0.05

	}
	SubShader{
		Cull off
			Tags{ "Queue" = "AlphaTest" "RenderType" = "TransparentCutout" "IgnoreProjector" = "True" }


		Pass
		{

			Cull OFF
			Tags{ "LightMode" = "ForwardBase" }
			AlphaToMask On


			CGPROGRAM

			#include "UnityCG.cginc" 
			#pragma vertex vert
			#pragma fragment frag
			#pragma geometry geom
			#include "UnityLightingCommon.cginc" // 用来处理光照的一些效果

			#pragma target 4.0

			sampler2D _MainTex;
			sampler2D _AlphaTex;

			float _Height;//草的高度
			float _Width;//草的宽度
			struct v2g
			{
				float4 pos : SV_POSITION;
				float3 norm : NORMAL;
				float2 uv : TEXCOORD0;
			};

			struct g2f
			{
				float4 pos : SV_POSITION;
				float3 norm : NORMAL;
				float2 uv : TEXCOORD0;
			};


			static const float oscillateDelta = 0.05;


		v2g vert(appdata_full v)
		{
			v2g o;
			o.pos = v.vertex;
			o.norm = v.normal;
			o.uv = v.texcoord;

			return o;
		}

		g2f createGSOut() {
			g2f output;

			output.pos = float4(0, 0, 0, 0);
			output.norm = float3(0, 0, 0);
			output.uv= float2(0, 0);

			return output;
		}


		[maxvertexcount(30)]
		void geom(point v2g points[1], inout TriangleStream<g2f> triStream)
		{
		 
			float4 root = points[0].pos;

			const int vertexCount = 12;

			float random = sin(UNITY_HALF_PI * frac(root.x) + UNITY_HALF_PI * frac(root.z));


			_Width = _Width + (random / 50);
			_Height = _Height +(random / 5);


			g2f v[vertexCount] = {
				createGSOut(), createGSOut(), createGSOut(), createGSOut(),
				createGSOut(), createGSOut(), createGSOut(), createGSOut(),
				createGSOut(), createGSOut(), createGSOut(), createGSOut()
			};

			//处理纹理坐标
			float currentV = 0;
			float offsetV = 1.f /((vertexCount / 2) - 1);

			//处理当前的高度
			float currentHeightOffset = 0;
			float currentVertexHeight = 0;

			//风的影响系数
			float windCoEff = 0;

			for (int i = 0; i < vertexCount; i++)
			{
				v[i].norm = float3(0, 0, 1);

				if (fmod(i , 2) == 0)
				{ 
					v[i].pos = float4(root.x - _Width , root.y + currentVertexHeight, root.z, 1);
					v[i].uv = float2(0, currentV);
				}
				else
				{ 
					v[i].pos = float4(root.x + _Width , root.y + currentVertexHeight, root.z, 1);
					v[i].uv = float2(1, currentV);

					currentV += offsetV;
					currentVertexHeight = currentV * _Height;
				}

				float2 wind = float2(sin(_Time.x * UNITY_PI * 5), sin(_Time.x * UNITY_PI * 5));
				wind.x += (sin(_Time.x + root.x / 25) + sin((_Time.x + root.x / 15) + 50)) * 0.5;
				wind.y += cos(_Time.x + root.z / 80);
				wind *= lerp(0.7, 1.0, 1.0 - random);

				float oscillationStrength = 2.5f;
				float sinSkewCoeff = random;
				float lerpCoeff = (sin(oscillationStrength * _Time.x + sinSkewCoeff) + 1.0) / 2;
				float2 leftWindBound = wind * (1.0 - oscillateDelta);
				float2 rightWindBound = wind * (1.0 + oscillateDelta);

				wind = lerp(leftWindBound, rightWindBound, lerpCoeff);

				float randomAngle = lerp(-UNITY_PI, UNITY_PI, random);
				float randomMagnitude = lerp(0, 1., random);
				float2 randomWindDir = float2(sin(randomAngle), cos(randomAngle));
				wind += randomWindDir * randomMagnitude;

				float windForce = length(wind);

				v[i].pos.xz += wind.xy * windCoEff;
				v[i].pos.y -= windForce * windCoEff * 0.8;

				v[i].pos = UnityObjectToClipPos(v[i].pos);

				if (fmod(i, 2) == 1) {

					windCoEff += offsetV;
				}

			}

			for (int p = 0; p < (vertexCount - 2); p++) {
				triStream.Append(v[p]);
				triStream.Append(v[p + 2]);
				triStream.Append(v[p + 1]);
			}
		}


		half4 frag(g2f IN) : COLOR
		{
			fixed4 color = tex2D(_MainTex, IN.uv);
			fixed4 alpha = tex2D(_AlphaTex, IN.uv);

			half3 worldNormal = UnityObjectToWorldNormal(IN.norm);

			//ads
			fixed3 light;

			//ambient
			fixed3 ambient = ShadeSH9(half4(worldNormal, 1));

			//diffuse
			fixed3 diffuseLight = saturate(dot(worldNormal, UnityWorldSpaceLightDir(IN.pos))) * _LightColor0;

			//specular Blinn-Phong 
			fixed3 halfVector = normalize(UnityWorldSpaceLightDir(IN.pos) + WorldSpaceViewDir(IN.pos));
			fixed3 specularLight = pow(saturate(dot(worldNormal, halfVector)), 15) * _LightColor0;

			light = ambient + diffuseLight + specularLight;

			return float4(color.rgb * light, alpha.g);

		}
		ENDCG
	}
}
~~~

##### 如何使用GPU进行动态粒子运算？

这里的动态粒子运算，指的是一个粒子在下一时刻的状态，与上一时刻自己或其它粒子的状态有关。即粒子状态的计算需要之前时刻的状态作为参数，这就是“动态粒子”。这也是**物理动画**中常用的概念。

此处我们尝试使用Compute Shader来对粒子状态进行更新，然后将状态通过Structured Buffer传入着色器。





<div STYLE="page-break-after:always;"></div>

## 渲染优化

### 着色器优化

#### 多细节层次LOD

##### 如何对网格实现LOD？

网格的LOD一般与摄像机到物体的距离有关。



如果存在一个巨大的网格，自动LOD可能不能满足需求，此时需要手动通过脚本实现LOD。

##### 如何对贴图实现LOD？

对纹理贴图进行的LOD俗称**MipMap**，译为**多级渐远纹理技术**，又被称为MIPS。它是将原纹理提前用滤波处理来得到更小的版本，这样在实时处理中如果物体远离摄像机，就可以对较小的版本采样而不是现场运算。这是一个典型的用空间换时间的算法，使用Mipmap一般会额外占用33%的内存空间用来存放这些较小的版本。

![Mipmap](E:/Textures/图形学/Mipmap.gif)

在CG语言中，可以使用tex2Dlod(sampler2D texture, float4(coord2, 0, lod))的格式，来在指定的LOD等级采集mipmap。在某些算法中不能使用硬件自动计算mipmap level的方式来计算采样等级，而是需要手动运算，例如在IBL中，需要基于粗糙度来计算采样使用的lod等级。而**tex2D默认情况下由GPU自动判断采集哪个等级的Mipmap**。

很容易想到，模型表面离摄像机越近时就应采样更细致的、更大的mipmap图，也就是更大的Mipmap Level。例如假定对一个铺满屏幕的墙壁采样时，Mipmap Level为0，而当摄像机远离时，该墙壁只占屏幕空间的1/4面积，此时使用Mip Level为1，也就是长和宽为原材质1/2的纹理进行采样。

那么GPU是如何判断纹理在屏幕上所占的比例的呢？可以使用导数运算ddx和ddy【见流水线-片元着色器】，计算在屏幕空间中一个物体所生成的相邻片元的uv的差。显然，**相邻片元的uv差值越大，说明相邻片元在物体上相距越远，也就说明这个物体占屏幕空间的比例越小，对应的就需要使用更高的Mipmap等级进行采样**。我们可以用这样的代码来理解tex2D对mipmap采样的逻辑(并不是tex2D真正的源码)：

~~~ShaderLab
tex2D(sampler2D tex, float2 uv)
{
	float dx = ddx(uv);
	float dy = ddx(uv);
	
	//texSize是纹理的大小
	float px = dx / texSize.x;
	float py = dy / texSize.y;
	float lod = 0.5 * log2(max(dot(px,px), dot(py,py)));
	float4 sample = float4(uv, 0, lod);
	tex2Dlod(tex, sample);
}
~~~

注意，因为在顶点着色器中是不支持使用导数的，所以在顶点着色器中只支持tex2Dlod，而不能使用tex2D。

##### 如何对着色器实现LOD？

着色器的LOD是与摄像机的位置无关的，在Unity中，需要通过手动设置Shader.maximumLOD属性来调整LOD的值。

#### 渲染管线优化

##### 什么是GPU实例化？

**GPU示例化(GPU Instance)**是通过CPU和GPU的协作来有效减少Draw Call的技术。

> 在Unity中，可以使用#pragma multi_compile_instancing来启用具有GPU实例化功能的着色器。

从思想上，GPU实例化就是CPU将一个模型和它们不同的参数一起传给GPU，而由GPU进行多次的渲染，从而实现在绘制使用同一个网格的多个物体时降低Draw Call次数的效果。

原本每绘制一个物体就要做一次Draw Call，使用GPU实例化后只需要进行一次。Direct 3D和OpenGL等流水线都已经实现了这个功能，而渲染引擎则在此基础上进行了封装，使得在各个平台上都能使用同一套代码进行GPU实例化。

显然，由于我们研究的是使用同一网格的多个物体，所以这些物体可以共享同一个顶点数组。除此之外，我们还需要另一个缓冲区来描述这些模型在世界坐标下的位置信息，来实现对这批物体的统一渲染。直观的，我们使用一个矩阵缓冲区来存放这些物体的世界空间变换矩阵(MVP中的M)。相比批处理，对每个模型的复制只是多出了16个浮点数大小的矩阵，却减少了一整个顶点数组的消耗，总的来说空间优势明显。

CPU将变换矩阵数组与顶点数组一起传输到GPU，需要GPU读取顶点数组，然后进行若干次几何阶段运算，每次都读取一个变换矩阵将顶点变换到齐次裁剪空间。由于GPU的流处理结构，光栅化后的片元会同步输出到像素阶段。

这个流程需要GPU流水线的特殊支持，所以并非所有图形硬件都能使用GPU实例化。

##### 什么是单程立体渲染?

单程立体渲染是VR领域的概念，由于VR设备需要同时渲染左右眼两个不同角度的画面，使用传统的渲染方案使需要消耗两倍的渲染资源。单程立体渲染尝试将左右眼的图像打包渲染到一张可渲染纹理中，降低总体的Draw Call数。



<div STYLE="page-break-after:always;"></div>

### GPU辅助计算

#### Compute Shader



#### CUDA



## 图形学综合技术范例

### 毛发渲染领域

##### 基于Shell思想的毛发渲染

这个思想的核心是，在渲染完原本的模型表面后，再沿着法线方向对网格进行外扩，在外扩方向上重复渲染多次“外壳”，重复若干次，使得在视觉效果上就像这些“外壳”密切连接在一起，形成了沿着网格法线方向伸展的无数条“毛发”。

我们将这些“外壳”称为Shell，每一个Shell称为一个Layer，由若干个Layer组成毛发。毛发的长度记为FurLength。接下来我们试图通过编写一个Pass，这个Pass每次渲染一个Shell，然后在CPU中通过修改Layer，并多次调用它来实现若干个Shell的渲染。为了能在CPU中修改Layer，我们定义了ShellLevel，这是一个$(0,1]$的浮点数，表示了当前渲染的Shell的位置位于这一组Shell的哪个部分。

实现这个效果需要考虑三个部分的核心问题：

+ 顶点偏移的位置

  顶点的偏移可以归结为三个不同的成分：沿法线方向的生长量、整体基于力的位移量和基于惯性运动的惯性量。
  
  生长量主要由当前的ShellValue和FurLength决定，用于表达当前Shell距离原模型表面的距离。
  
  位移量由一个向量常量决定，主要用于表现风力或重力对毛发的影响，是当前Shell朝某个方向的系统性的位移。考虑到毛发的硬度，越接近原模型表面的Shell位移距离越近，越远离元模型表面的Shell位移距离越远。我们可以通过设计一张硬度纹理来在模型表面生成软硬不一的毛发效果。由于风力对不同位置毛发的作用不同，也可以使用一个力场贴图或者噪声图来实现不同毛发受风力的不同作用，在此示例中没有这么做。
  
  惯性量是在物体运动时，毛发的尖端相比毛发的根部运动的更慢的效应。换句话说，更上层的Shell的运动受到其在上一帧时的位置的影响。为了保留Shell在上一帧时的位置，需要由CPU保存上一帧物体的模型-世界矩阵，以此在着色器中计算出上一帧的位置，并根据ShellValue和毛发硬度对上一帧和这一帧的Shell位置进行插值。

+ 毛发的透明度

  为了在不同高度、不同位置产生疏密不同的毛发，需要使用一个或多个噪声纹理来修改毛发的透明度。在此称该噪声纹理为Slice纹理。
  
  使用同一个Slice纹理对所有Shell进行透明度修正，将ShellValue-Slice.a作为片元的透明度，可以模拟出长度不同的毛发参差不齐的效果。如果使用多个Slice纹理对不同的Shell进行修正，可以产生毛发卷曲或者毛发粗细不均的效果。
  
  为了正常的进行混合，渲染Shell的Pass应该开启深度测试、关闭深度写入、开启透明度混合。

+ 毛发的光照

  在此使用了Kajiya-Kay模型进行光照渲染，使用其它毛发各项异性模型皆可。
  
  为了使用Kajiya-Kay模型，我们需要获得毛发的切向量。此处有两种毛发切向量的获取思路：一是在毛发弯曲程度基本可忽略的情况下，用模型法线直接近似毛发切线，下面的示例使用了这一方法；二是在计算Shell的位置时，计算更上一层的Shell的位置，根据两个Shell的坐标的插值获得切向量。
  
  使用Kajiya-Kay模型模拟自阴影的效果，通过让更接近根部的Shell、也就是ShellLevel较低的Shell变暗来模拟出自阴影的感觉，这个模拟方式避免了深度阴影纹理的渲染。

CPU核心代码:

~~~C#
private void Update()
{
    MaterialPropertyBlock block = new MaterialPropertyBlock();
    FurMaterial.SetMatrix("_LastFrameM", lastFrameM);       
    for (int i = 1; i <= LayerCount; i++)
    {
        float shellLevel = (float)i / LayerCount;
        block.SetFloat("_ShellLevel", shellLevel);
        for(int j = 0; j < Mesh.subMeshCount; j++)
            Graphics.DrawMesh(Mesh, transform.localToWorldMatrix, FurMaterial, 0, Camera.current, j, block);          
    }
    lastFrameM = transform.localToWorldMatrix;
}
~~~

GPU核心代码:

~~~ShaderLab
Shader "Unlit/Fur"
{
    Properties
    {
        _DiffuseMap("Diffuse", 2D) = "white" {}
        _FurMask("Fur Mask", 2D) = "white" {}
        _SliceMask("Slice Mask", 2D) = "white" {}
        _HardnessMask("Hardness Mask", 2D) = "white" {}
        _Hardness("Hardness", Range(0,1)) = 1
        _FurLength("Fur Length", Range(0.0001,0.003)) = 1.0
        _ShellLevel("Shell Level", Range(0,1)) = 0.1
        _GlobalForce("Global Force", Vector) = (0, -1, 0, 0)
        _Inertance("Inertance", Range(0,1)) = 0.5
        _SubSpecularValue("SubSpecularValue", Float) = 0.2
        _DiffuseColor("DiffuseColor", Color) = (1,1,1,1)
        _SubSpecularColor("SubSpecularColor", Color) = (1,1,1,1)
        _SpecularPower("Specular Power", Range(0,1)) = 0.5
        _Gloss("Gloss", Float) = 5
    }
        SubShader
        {
            Tags { "RenderType" = "Transparent""Queue" = "Transparent"  }
            LOD 100

            Pass
            {
                Tags {"RenderType" = "Transparent" "Queue" = "Transparent"}
                Cull Off
                Blend SrcAlpha OneMinusSrcAlpha
                ZWrite Off

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"
            #include "Lighting.cginc"    
            #include "Noise.cginc"

            struct a2v
            {
                float4 vertex : POSITION;
                float3 normal : NORMAL;
                float2 uv : TEXCOORD0;
                float3 tangent : TANGENT;
            };

            struct v2f
            {
                float4 vertex : SV_POSITION;
                float4 uv0 : TEXCOORD0;
                float2 uv1 : TEXCOOED1;
                float3 tangent : TEXCOORD2;
                float3 viewDir : TEXCOORD3;
                float3 lightDir : TEXCOORD4;
                float3 worldPos : TEXCOORD5;
                float3 worldNormal : TEXCOORD6;
                float3 debug : TEXCOORD7;
            };
            
            //顶点相关属性
            uniform sampler2D _FurMask; //毛发长度贴图
            uniform float4 _FurMask_ST;
            uniform float _FurLength; //毛发长度系数
            uniform sampler2D _HardnessMask; //毛发硬度贴图
            uniform float4 _HardnessMask_ST;
            uniform float _ShellLevel; //Shell层数 0~1
            uniform float3 _GlobalForce; //力场
            uniform float4x4 _LastFrameM; //上一帧的M矩阵
            uniform float _Inertance; //惯性强度系数

            //片元相关属性
            uniform sampler2D _DiffuseMap; //漫反射
            uniform float4 _DiffuseMap_ST;          
            uniform sampler2D _SliceMask; //Slice纹理
            uniform float4 _SliceMask_ST;
            uniform float _SubSpecularValue; //次高光距离
            uniform float3 _DiffuseColor; //漫反射颜色
            uniform float3 _SubSpecularColor; //次高光颜色
            uniform float _SpecularPower;
            uniform float _Gloss; //高光强度
            uniform float _FurDensity, _FurShading, _RimPower, _FurThinness, _ShadowStrength, _Hardness, _NoiseStrength, _NoiseSize, _NoiseSpeed;

            v2f vert (a2v v)
            {
                v2f o;

                float epsilon = 0.1;

                float3 origin = mul(unity_ObjectToWorld, v.vertex);

                float2 uv_furMask = TRANSFORM_TEX(v.uv, _FurMask);
                float furlength = tex2Dlod(_FurMask, float4(uv_furMask, 0, 0)).r;
                float shellDis = furlength * _ShellLevel * _FurLength;
                float shellDisD = furlength * (_ShellLevel + epsilon) * _FurLength;

                float2 uv_hardMask = TRANSFORM_TEX(v.uv, _HardnessMask);
                float hardness = tex2Dlod(_HardnessMask, float4(uv_hardMask, 0, 0)).r * _Hardness;

                float3 vertexW = origin;
                float3 vertexLW = mul(_LastFrameM, v.vertex);

                float t = _ShellLevel * furlength * (1 - hardness);
                float tD = (_ShellLevel + epsilon) * furlength * (1 - hardness);

                float3 worldNormal = normalize(mul(v.normal, unity_WorldToObject));
                float3 meshTangent = normalize(mul(UNITY_MATRIX_M, v.tangent));

                float3 vertexWD = vertexW + _GlobalForce * tD;
                float3 vertexLWD = vertexLW + _GlobalForce * tD;
                vertexW += _GlobalForce * t;
                vertexLW += _GlobalForce * t;

                vertexW += smoothNoise3((vertexW + _GlobalForce * _Time.y * _NoiseSpeed) * _NoiseSize) * _NoiseStrength;
                vertexLW += smoothNoise3((vertexLW + _GlobalForce * _Time.y * _NoiseSpeed) * _NoiseSize) * _NoiseStrength;
                vertexWD += smoothNoise3((vertexWD + _GlobalForce * _Time.y * _NoiseSpeed) * _NoiseSize) * _NoiseStrength;
                vertexLWD += smoothNoise3((vertexLWD + _GlobalForce * _Time.y * _NoiseSpeed) * _NoiseSize) * _NoiseStrength;
            
                vertexW += worldNormal * shellDis;
                vertexLW += worldNormal * shellDis;
                vertexWD += worldNormal * shellDisD;
                vertexLWD += worldNormal * shellDisD;             
         
                o.worldNormal = UnityObjectToWorldNormal(v.normal);

                o.worldPos = vertexW;
                o.vertex = UnityWorldToClipPos(vertexW);      
                o.uv0.xy = TRANSFORM_TEX(v.uv, _DiffuseMap);
                o.uv0.zw = TRANSFORM_TEX(v.uv, _SliceMask);
                o.uv1.xy = uv_furMask;
                o.viewDir = WorldSpaceViewDir(float4(vertexW, 1));
                o.lightDir = WorldSpaceLightDir(float4(vertexW, 1));
                o.tangent = furlength <= 0.2 ? worldNormal : normalize(vertexWD - vertexW);
                return o;
            }

            float4 frag(v2f i) : SV_Target
            {
                float3 worldNormal = normalize(i.worldNormal);
                float3 worldTangent = normalize(i.tangent);
                float3 worldLight = normalize(_WorldSpaceLightPos0.xyz);
                float3 worldView = normalize(_WorldSpaceCameraPos.xyz - i.worldPos.xyz);
                float3 worldHalf = normalize(worldView + worldLight);

                float furlength = tex2D(_FurMask, i.uv1.xy).r;
                if (furlength <= 0.2) discard;

                float3 diffuseColor = tex2D(_DiffuseMap, i.uv0.xy);
                float sliceAlpha = tex2D(_SliceMask, i.uv0.zw).r;
                sliceAlpha  = clamp(sliceAlpha - (_ShellLevel * _ShellLevel) * _FurDensity, 0, 1);

                diffuseColor -= (pow(1 - _ShellLevel, 3)) * _FurShading;


                float rim = 1.0 - saturate(dot(worldView, worldNormal));
                diffuseColor += float3(1, 1, 1) * pow(rim, _RimPower) * 0.4;
                float3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * diffuseColor;

                float cosTL = dot(worldTangent, worldLight);
                float sinTL = sqrt(1 - cosTL * cosTL);
                float3 diffuse = saturate(sinTL) * diffuseColor * _DiffuseColor * _LightColor0.rgb;
                diffuse = lerp(float3(1,1,1) * (1 - _ShadowStrength), float3(1, 1, 1), diffuse);

                float cosTV = dot(worldTangent, worldView);
                float sinTV = sqrt(1 - cosTV * cosTV);
                
                float value = saturate(cosTL) * saturate(cosTV) + saturate(sinTL) * saturate(sinTV);
                float3 specular = saturate(pow(value, _Gloss)) * _LightColor0.rgb * _SpecularPower  * sqrt(_ShellLevel);

                float4 col = float4(diffuse + specular, sliceAlpha);

                return float4(col.rgb, sliceAlpha);
            }

            ENDCG
        }
    }
}
~~~

##### 基于Hermit曲线的毛发渲染d